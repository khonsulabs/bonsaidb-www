<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title> - BonsaiDb Updates</title>
	<link href="https://bonsaidb.io/blog/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://bonsaidb.io/blog/"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2022-07-01T00:00:00+00:00</updated>
	<id>https://bonsaidb.io/blog/atom.xml</id>
	<entry xml:lang="en">
		<title>Optimizing BonsaiDb: Promising progress on new storage layer (Part 2 of ?)</title>
		<published>2022-07-01T00:00:00+00:00</published>
		<updated>2022-07-01T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/optimizing-bonsaidb-p2/" type="text/html"/>
		<id>https://bonsaidb.io/blog/optimizing-bonsaidb-p2/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;At the end of the &lt;a href=&quot;&#x2F;blog&#x2F;optimizing-bonsaidb-p1&quot;&gt;last post in this ongoing
series&lt;&#x2F;a&gt;, I introduced an overview of
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;sediment&quot;&gt;Sediment&lt;&#x2F;a&gt;, the new storage format I was beginning development on that
I hoped would replace the append-only file format that &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt; uses.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s been a month, and as promised, I wanted to share my progress. Sediment is
still very early in development. All results in this post should be taken with
healthy skepticism, as there could be bugs that I have not discovered yet that
force me to change some fundamental aspect of the library. I hope I&#x27;m beyond
those hurdles, but there are some fun and complex problems I have yet to tackle.&lt;&#x2F;p&gt;
&lt;p&gt;This post is going to be an exploration of Sediment&#x27;s design. At the end, I will
show some of the early benchmarking results -- including running the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;tree&#x2F;main&#x2F;benchmarks&quot;&gt;Nebari
benchmark suite&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#The%20Goals%20of%20Sediment&quot; name=&quot;The%20Goals%20of%20Sediment&quot;&gt;
    The Goals of Sediment
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The primary speed issues I had identified with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt; stemmed from two
design decisions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Nebari did not preallocate disk space. Allocating a larger chunk of space and
overwriting it is faster than appending multiple times to the end of a file
due to extra filesystem metadata syncing being required each time the file&#x27;s
length changes.&lt;&#x2F;li&gt;
&lt;li&gt;Nebari required two &lt;code&gt;fsync()&lt;&#x2F;code&gt; operations for ACID. My early benchmarking was
flawed due to using temporary files in &lt;code&gt;&#x2F;tmp&lt;&#x2F;code&gt; for benchmarking against. On
Linux these days, &lt;code&gt;&#x2F;tmp&lt;&#x2F;code&gt; is often powered by the &lt;code&gt;tmpfs&lt;&#x2F;code&gt; filesystem. This
filesystem does not actually persist data to disk, which means &lt;code&gt;fsync()&lt;&#x2F;code&gt; was
almost instantaneous. The two-&lt;code&gt;fsync()&lt;&#x2F;code&gt; commit strategy is simply not viable
if the goal is to be competitive with performance.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;One of the other problems an append-only file format has is the requirement of
compaction. As a former heavy user of &lt;a href=&quot;https:&#x2F;&#x2F;couchdb.apache.org&#x2F;&quot;&gt;CouchDB&lt;&#x2F;a&gt; (the original design
inspiration for BonsaiDb), I remember struggling to compact databases when I was
running low on disk space. The problem with compacting a database is that it
requires rewriting the database to a new file -- an operation that requires free
disk space. For large databases, that could be a challenge.&lt;&#x2F;p&gt;
&lt;p&gt;I had goals for BonsaiDb to make compaction less painful, but with the prospect
of replacing the append-only format, my third goal would be to allow reusing
disk space.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#The%20API%20of%20Sediment&quot; name=&quot;The%20API%20of%20Sediment&quot;&gt;
    The API of Sediment
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;During the development of &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;, I had abstracted nearly every read
and write operation into something I called a &amp;quot;chunk&amp;quot;. A chunk was the data
stored with 8 extra bytes: 4 bytes for a CRC32 to verify the data, and 4 bytes
for the length of the data. A chunk&#x27;s ID was simply it&#x27;s location in the file.&lt;&#x2F;p&gt;
&lt;p&gt;My goal of Sediment&#x27;s design would be to focus on this primary concern: store
some arbitrary bytes and be given a unique identifier. That unique ID should be
able to retrieve or delete the data.&lt;&#x2F;p&gt;
&lt;p&gt;The only other feature Nebari needed is a way to store the &amp;quot;root&amp;quot; of the B+Tree
and retrieve it again when opening a database.&lt;&#x2F;p&gt;
&lt;p&gt;These considerations yielded this API:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Store some data
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span&gt; session = db.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;new_session&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; grain_id: GrainId = session.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;hello world&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;)?;
&lt;&#x2F;span&gt;&lt;span&gt;session.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;commit&lt;&#x2F;span&gt;&lt;span&gt;()?;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Retrieve the data
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; data = db.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;get&lt;&#x2F;span&gt;&lt;span&gt;(grain_id)?.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span&gt;assert_eq!(data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;as_bytes&lt;&#x2F;span&gt;&lt;span&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;hello world&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;);
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Archive the data (marks it for deletion)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span&gt; session = db.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;new_session&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span&gt;session.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;archive&lt;&#x2F;span&gt;&lt;span&gt;(grain_id)?;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; archiving_commit: BatchId = session.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;commit&lt;&#x2F;span&gt;&lt;span&gt;()?;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Free the archived data. Until this is complete, `grain_id` can still 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; retrieve the stored data.
&lt;&#x2F;span&gt;&lt;span&gt;db.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;checkpoint_to&lt;&#x2F;span&gt;&lt;span&gt;(archiving_commit)?;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;A &lt;code&gt;GrainId&lt;&#x2F;code&gt; is an opaque 64-bit identifier for stored data, and a
&lt;code&gt;BatchId&lt;&#x2F;code&gt; is an identifier of a specific commit. Rather than offering an API to
free data immediately, Sediment uses an internal log that can enable
replication. Why did I design it this way?&lt;&#x2F;p&gt;
&lt;p&gt;There is one key distinction between Sediment and nearly every format I&#x27;ve
compared it against: Sediment does not use &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Write-ahead_logging&quot;&gt;write-ahead logging (WAL)&lt;&#x2F;a&gt; or
journal file. Sediment only requires a single file for its storage, just like
Nebari did. My decision to do this was in an effort to minimize IO.&lt;&#x2F;p&gt;
&lt;p&gt;There are many numerous benefits for WAL, but one downside is that data written
to the database is often written multiple times. First, the new data is written
to the log. At some future point, the database engine will checkpoint the log,
and those changes will be applied to the primary data store. This is an extra
copy that can be avoided by Sediment, as there&#x27;s no real benefit to keeping a
separate log for this format.&lt;&#x2F;p&gt;
&lt;p&gt;The log that Sediment uses simply tracks &lt;code&gt;GrainId&lt;&#x2F;code&gt; allocations and archivals.
When the log is checkpointed, each archived grain in the log entries that are
being removed will be free. This simple approach provides enough information to
implement Sediment&#x27;s consistency guarantees and could be used to replicate the
database.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#The%20Format%20of%20Sediment&quot; name=&quot;The%20Format%20of%20Sediment&quot;&gt;
    The Format of Sediment
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The basic design behind Sediment is to divide the file into blocks of data that
are pre-allocated and can be relocated to defragment the file. Basins have a
list of Strata, and Strata have a list of &amp;quot;Grain Maps&amp;quot;.&lt;&#x2F;p&gt;
&lt;p&gt;A &lt;code&gt;GrainId&lt;&#x2F;code&gt; is a 64-bit identifier that uses 8 bits to identify which Basin the
grain is located within. The next 8 bits identify the Stratum. The remaining 48
bits identify the index of the grain within the Stratum.&lt;&#x2F;p&gt;
&lt;p&gt;Each Stratum has configurable properties:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Grain Length: How many bytes is each grain allocated?&lt;&#x2F;li&gt;
&lt;li&gt;Pages per Map: Grain Maps are divided into pages. To support very large
databases, each map can have multiple pages.&lt;&#x2F;li&gt;
&lt;li&gt;Grain Map Count: How many grain maps are allocated?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;One challenge this format has is keeping track of which regions of the file are
currently allocated for Basins, Strata, and Grains. These structures have been
designed to minimize the amount of data required to load enough state for the
database to be validated and the file&#x27;s allocations to be fully tracked.&lt;&#x2F;p&gt;
&lt;p&gt;The commit log is stored separately from the Basins, Strata, and Grains. The
file&#x27;s header points to the most recent page of commit log entries. If a
database is never checkpointed, there will be extra overhead on loading the
database to scan the commit log to understand which pages of the file have been
allocated and which are free. For most practical use cases, there will rarely be
more than a handful of pages of commit log entries.&lt;&#x2F;p&gt;
&lt;p&gt;Each header structure is stored twice within the file, and the &amp;quot;inactive&amp;quot; copy
is overwritten when saved. This copy-on-write behavior combined with the commit
log allows Sediment to verify that all writes performed during the last commit
are completely intact when reopening the database. If a CRC or data
inconsistency is found, the failed commit will be reverted before the database
is opened.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#The%20Architecture%20of%20Sediment&quot; name=&quot;The%20Architecture%20of%20Sediment&quot;&gt;
    The Architecture of Sediment
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The two main types in understanding how Sediment works are the &lt;code&gt;Atlas&lt;&#x2F;code&gt; and the
&lt;code&gt;Committer&lt;&#x2F;code&gt;. The &lt;code&gt;Atlas&lt;&#x2F;code&gt; is responsible for keeping track of the in-memory state
of the disk structures. The &lt;code&gt;Committer&lt;&#x2F;code&gt; is responsible for persisting changes to
those structures to disk.&lt;&#x2F;p&gt;
&lt;p&gt;This current design does not allow multiple processes to share access to a
Sediment database, as there are in-memory contracts between the &lt;code&gt;Atlas&lt;&#x2F;code&gt; and
&lt;code&gt;Committer&lt;&#x2F;code&gt;. One example is how file allocations are tracked. The &lt;code&gt;Atlas&lt;&#x2F;code&gt; is
responsible for allocating disk space for each grain written. Conversely, the
&lt;code&gt;Committer&lt;&#x2F;code&gt; will update the &lt;code&gt;Atlas&lt;&#x2F;code&gt; when disk space can be reused.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;code&gt;Atlas&lt;&#x2F;code&gt; is the least optimized part of Sediment. The problem of determining
how to allocate a grain is not simple, and my initial implementation is a brute
force algorithm. It&#x27;s been &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;sediment&#x2F;issues&#x2F;4&quot;&gt;on my radar&lt;&#x2F;a&gt;, and when
benchmarking Nebari&#x27;s insert performance, grain allocation took nearly 70% of
CPU time. Additionally, the &lt;code&gt;Atlas&lt;&#x2F;code&gt; currently is behind a single mutex. This
might be good enough, but more complex concurrency management may be something
to investigate as well.&lt;&#x2F;p&gt;
&lt;p&gt;Why is grain allocation tricky? The storage format of grains allows allocating
multiple sequential grains as a single &lt;code&gt;GrainId&lt;&#x2F;code&gt;. For example, if 64-byte value
is being written, it could be stored in four 16-byte grains or it could be
stored in 64 1-byte grains. The only limit is that only 255 consecutive grains
can be allocated for a single value. This gives an incredible amount of
flexibility, but it also has led to many hours of theorycrafting the best
allocation algorithms.&lt;&#x2F;p&gt;
&lt;p&gt;To write data, a &amp;quot;session&amp;quot; is created. The session can be used to write data,
archive data, checkpoint the database, or update the embedded header. If the
session is dropped, all reserved grains are available to be reused immediately.&lt;&#x2F;p&gt;
&lt;p&gt;When the session is committed, the changes are batched with any other pending
changes from other sessions waiting to be committed. The session&#x27;s thread checks
if another commit is already happening. If no other commit is happening, the
session&#x27;s thread becomes the commit thread and commits the currently batched
changes. Otherwise, the thread will wait for a signal that its batch has been
committed or for another opportunity to become the commit thread.&lt;&#x2F;p&gt;
&lt;p&gt;This design was intended to allow significant read and write concurrency, and
I&#x27;m happy to report that the benchmarks show promising concurrency results.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#Benchmarking%20Sediment&quot; name=&quot;Benchmarking%20Sediment&quot;&gt;
    Benchmarking Sediment
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Sediment isn&#x27;t a general purpose database, nor is it a key-value store. I am not
aware of any other storage API to benchmark against that can offer a true
apples-to-apples comparison. Still, I wanted to have some meaningful
comparisons, so I initially benchmarked single-threaded insert performance
against SQLite. However, SQLite doesn&#x27;t offer any multi-threaded concurrency
options that allow it to batch transactions. I identified &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&quot;&gt;RocksDB&lt;&#x2F;a&gt; as
a good candidate, as it has a &lt;a href=&quot;https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;rocksdb&quot;&gt;great crate&lt;&#x2F;a&gt; and supports batching
multithreaded transactions. Here are the results of the single-threaded benchmark:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Backend&lt;&#x2F;th&gt;&lt;th&gt;avg&lt;&#x2F;th&gt;&lt;th&gt;min&lt;&#x2F;th&gt;&lt;th&gt;max&lt;&#x2F;th&gt;&lt;th&gt;stddev&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;rocksdb&lt;&#x2F;td&gt;&lt;td&gt;2.591ms&lt;&#x2F;td&gt;&lt;td&gt;2.386ms&lt;&#x2F;td&gt;&lt;td&gt;2.797ms&lt;&#x2F;td&gt;&lt;td&gt;54.69us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;sediment&lt;&#x2F;td&gt;&lt;td&gt;2.765ms&lt;&#x2F;td&gt;&lt;td&gt;2.581ms&lt;&#x2F;td&gt;&lt;td&gt;6.020ms&lt;&#x2F;td&gt;&lt;td&gt;332.7us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;sqlite&lt;&#x2F;td&gt;&lt;td&gt;4.347ms&lt;&#x2F;td&gt;&lt;td&gt;4.148ms&lt;&#x2F;td&gt;&lt;td&gt;6.944ms&lt;&#x2F;td&gt;&lt;td&gt;273.2us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;sediment&#x2F;blob&#x2F;8b43f044a8daad85dbdf2218c2a7a7ced6bc9759&#x2F;benchmarks&#x2F;benches&#x2F;inserts.rs&quot;&gt;batch insert benchmark&lt;&#x2F;a&gt; measures a variable number of threads
performing a series of batch commits. Here are the average times to commit each
batch insert operation with ACID guarantees on my AMD Ryzen 3700X with 8 cores
and a Samsung SSD 970 EVO Plus:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Backend&lt;&#x2F;th&gt;&lt;th&gt;16 threads&lt;&#x2F;th&gt;&lt;th&gt;32 threads&lt;&#x2F;th&gt;&lt;th&gt;64 threads&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;rocksdb&lt;&#x2F;td&gt;&lt;td&gt;6.044ms&lt;&#x2F;td&gt;&lt;td&gt;9.194ms&lt;&#x2F;td&gt;&lt;td&gt;17.83ms&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;sediment&lt;&#x2F;td&gt;&lt;td&gt;6.362ms&lt;&#x2F;td&gt;&lt;td&gt;7.281ms&lt;&#x2F;td&gt;&lt;td&gt;8.247ms&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;sqlite&lt;&#x2F;td&gt;&lt;td&gt;24.93ms&lt;&#x2F;td&gt;&lt;td&gt;46.60ms&lt;&#x2F;td&gt;&lt;td&gt;163.2ms&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;I was very happy to see that the performance of Sediment scales very nicely even
with 64 threads concurrently writing to the database -- in spite of my naive
grain allocation algorithm. The next step was to see how much overhead Nebari
would add.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#Benchmarking%20Nebari%20on%20Sediment&quot; name=&quot;Benchmarking%20Nebari%20on%20Sediment&quot;&gt;
    Benchmarking Nebari on Sediment
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;tree&#x2F;main&#x2F;benchmarks&quot;&gt;Nebari benchmarks&lt;&#x2F;a&gt; are single-threaded and limited to
single trees, which made it much easier to quickly incorporate Sediment. There
are three main operations that are benchmarked: insert, get, and scan. Here are
the insert benchmark&#x27;s results, which tests the speed of ACID-compliant
insertion of a blob of a specified size:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Backend&lt;&#x2F;th&gt;&lt;th&gt;1 KB&lt;&#x2F;th&gt;&lt;th&gt;1 MB&lt;&#x2F;th&gt;&lt;th&gt;64 MB&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;nebari (sediment)&lt;&#x2F;td&gt;&lt;td&gt;2.823 ms&lt;&#x2F;td&gt;&lt;td&gt;4.124 ms&lt;&#x2F;td&gt;&lt;td&gt;183 ms&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;nebari (v0.5.3)&lt;&#x2F;td&gt;&lt;td&gt;2.678 ms&lt;&#x2F;td&gt;&lt;td&gt;6.959 ms&lt;&#x2F;td&gt;&lt;td&gt;284.9 ms&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;sqlite&lt;&#x2F;td&gt;&lt;td&gt;4.182 ms&lt;&#x2F;td&gt;&lt;td&gt;7.2 ms&lt;&#x2F;td&gt;&lt;td&gt;159 ms&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;For 1 KB inserts, the average performance is pretty similar. As mentioned
earlier, when profiling this benchmark, nearly 70% of the CPU time is spent in
grain allocation, which has been &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;sediment&#x2F;issues&#x2F;4&quot;&gt;on my refactor list since being
written&lt;&#x2F;a&gt;. For larger inserts, however, Sediment is
much more efficient. How about the performance of retrieving a single row out of
various data set sizes (random ids):&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Backend&lt;&#x2F;th&gt;&lt;th&gt;1k rows&lt;&#x2F;th&gt;&lt;th&gt;10k rows&lt;&#x2F;th&gt;&lt;th&gt;1m rows&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;nebari (sediment)&lt;&#x2F;td&gt;&lt;td&gt;749.6 ns&lt;&#x2F;td&gt;&lt;td&gt;908.7 ns&lt;&#x2F;td&gt;&lt;td&gt;9.067 us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;nebari (v0.5.3)&lt;&#x2F;td&gt;&lt;td&gt;766.1 ns&lt;&#x2F;td&gt;&lt;td&gt;913.2 ns&lt;&#x2F;td&gt;&lt;td&gt;9.413 us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;sqlite&lt;&#x2F;td&gt;&lt;td&gt;6.944 us&lt;&#x2F;td&gt;&lt;td&gt;7.471 us&lt;&#x2F;td&gt;&lt;td&gt;8.671 us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;As I expected, there isn&#x27;t really much variation in performance. What about
scanning a range of keys (random IDs):&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Backend&lt;&#x2F;th&gt;&lt;th&gt;32 of 1k rows&lt;&#x2F;th&gt;&lt;th&gt;100 of 10k rows&lt;&#x2F;th&gt;&lt;th&gt;1k of 1m rows&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;nebari (sediment)&lt;&#x2F;td&gt;&lt;td&gt;2.747 us&lt;&#x2F;td&gt;&lt;td&gt;72.35 us&lt;&#x2F;td&gt;&lt;td&gt;982.4 us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;nebari (v0.5.3)&lt;&#x2F;td&gt;&lt;td&gt;2.684 us&lt;&#x2F;td&gt;&lt;td&gt;90.02 us&lt;&#x2F;td&gt;&lt;td&gt;1.156 ms&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;sqlite&lt;&#x2F;td&gt;&lt;td&gt;16.67 us&lt;&#x2F;td&gt;&lt;td&gt;44.75 us&lt;&#x2F;td&gt;&lt;td&gt;414.4 us&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;This was one of the most surprising results to me. There is a ~15% speed
improvement on this &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;issues&#x2F;11&quot;&gt;fairly unoptimized operation&lt;&#x2F;a&gt; on the larger
data sets. My best guess as to why this operation is faster is that data stored
in Sediment is packed closer together, which makes it more likely that the
kernel page cache already has needed data in the cache.&lt;&#x2F;p&gt;
&lt;p&gt;Overall, these benchmarks have shown that replacing the append-only format with
Sediment has been a success so far.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#Next%20Steps&quot; name=&quot;Next%20Steps&quot;&gt;
    Next Steps
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Nebari&#x27;s multi-tree type, &lt;code&gt;Roots&lt;&#x2F;code&gt;, is not currently compatible with Sediment. To
finish realizing my goal of a single &lt;code&gt;fsync()&lt;&#x2F;code&gt; ACID transaction, I need to
update the &lt;code&gt;Roots&lt;&#x2F;code&gt; type to support multi-tree transactions within Sediment.
Until earlier this week, my vision was to refactor &lt;code&gt;Roots&lt;&#x2F;code&gt; to use a single file
for all trees by using a B+Tree of B+Trees internally.&lt;&#x2F;p&gt;
&lt;p&gt;However, with the results of the scan benchmark, I&#x27;m believe I&#x27;m seeing some
benefits of keeping &amp;quot;related data&amp;quot; together. I&#x27;m currently debating whether I
want to proceed with the single-file approach or whether I&#x27;m going to update the
current multi-file approach with a new transaction log that can be synchronized
in parallel with the affected trees.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve started maintaining a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;khonsulabs&#x2F;projects&#x2F;10&quot;&gt;project on GitHub&lt;&#x2F;a&gt; tracking the
issues I want to complete before I release the next BonsaiDb update. There are
also a lot of areas left to explore:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Sediment currently is optimized for 4 KB pages. ZFS, for example, has a 128 KB
record size by default. Should Sediment offer customizable page sizes, or is
the 4 KB page size fine even with filesystems that allocate in chunks larger
than 4 KB? I&#x27;ve seen some mixed advice, so I need to do some extensive
benchmarking on various filesystems to try to understand the impacts of the
page size.&lt;&#x2F;li&gt;
&lt;li&gt;The current pre-allocation strategy is fairly aggressive with a ~1:16 ratio.
For example, let&#x27;s say you&#x27;re writing a 1 MB value to Sediment. The smallest
grain size that can be used must be able to fit &lt;code&gt;ceil(1 MB &#x2F; 255)&lt;&#x2F;code&gt;. This means
that the smallest grain length that would be allocated is 4,096 bytes. Each
grain map page contains 4,084 grains which means a minimum grain map
allocation length would be 4 KB * 4,084, which is ~16 MB. If your database is
10 GB, allocating 16 MB is a no-brainer, but if your database is only 1 MB, a
16 MB allocation is very aggressive. I want to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;sediment&#x2F;issues&#x2F;13&quot;&gt;add another
way&lt;&#x2F;a&gt; to store large values that would not require such an
agressive preallocation strategy.&lt;&#x2F;li&gt;
&lt;li&gt;I&#x27;ve tried out using &lt;code&gt;tokio_uring&lt;&#x2F;code&gt; to add &lt;code&gt;io_uring&lt;&#x2F;code&gt; support on Linux, and my
initial benchmarking results are that there is not much noticable difference
compared against using &lt;code&gt;std::fs&lt;&#x2F;code&gt;. I need to spend some time profiling to
understand if I&#x27;m doing something silly. I&#x27;ve read that using &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; with
&lt;code&gt;io_uring&lt;&#x2F;code&gt; can be very beneficial, but the &lt;code&gt;tokio_uring&lt;&#x2F;code&gt; crate does not
support specifying additional open flags yet.&lt;&#x2F;li&gt;
&lt;li&gt;The CEO behind &lt;a href=&quot;https:&#x2F;&#x2F;hibernatingrhinos.com&#x2F;&quot;&gt;Hibernating Rhinos&lt;&#x2F;a&gt;, the company
developing RavenDB, &lt;a href=&quot;https:&#x2F;&#x2F;ayende.com&#x2F;blog&#x2F;197377-C&#x2F;re-bonsaidb-performance-update-a-deep-dive-on-file-synchronization&quot;&gt;wrote a response to my last post&lt;&#x2F;a&gt;. They&#x27;ve
had great success with direct IO even without &lt;code&gt;io_uring&lt;&#x2F;code&gt;. The basic idea is
that by specifying a few flags, you can avoid the kernel&#x27;s page caching if you
always write full pages of data that are page-aligned. While I have not tested
what sorts of gains Sediment can gain by using direct IO, the data structures
in Sediment have been designed with this potential optimization in mind.&lt;&#x2F;li&gt;
&lt;li&gt;This new format supports reusing data as well as moving the stored data
around. The approaches for doing various defragmenting operations are
currently just theoretical models in my head that need to be explored and
implemented.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;While this new format is much more complex than the current append-only format,
I&#x27;ve had enough promising results that I&#x27;m proceeding with this new format. My
next goal is to get the BonsaiDb Commerce Benchmark running atop Sediment --
that is the benchmark where the current format results in BonsaiDb to be roughly
2.5x slower than PostgreSQL. I&#x27;m optimistic that BonsaiDb can compete based on
these early results!&lt;&#x2F;p&gt;
&lt;p&gt;Assuming that the remaining development goes smoothly, I am currently hoping to
have BonsaiDb v0.5 out sometime in August with support for the new format. I am
committed to having a painless way to migrate data from the old format to the
new format, so all current users or people considering trying BonsaiDb need not
worry!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-7&quot;&gt;


&lt;a href=&quot;#Getting%20Started%20with%20BonsaiDb&quot; name=&quot;Getting%20Started%20with%20BonsaiDb&quot;&gt;
    Getting Started with BonsaiDb
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of
examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;&quot;&gt;user&#x27;s
guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good
documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We would love to hear from you if you have questions or feedback. We have &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an issue&lt;&#x2F;a&gt; with any questions or feedback.&lt;&#x2F;p&gt;
&lt;p&gt;We dream big with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, and we believe that it can simplify writing and deploying complex, data-driven applications in Rust. We would love &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;labels&#x2F;good%20first%20issue&quot;&gt;additional contributors&lt;&#x2F;a&gt; who have similar passions and ambitions.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear about it. Nothing makes us happier than hearing people are building things with our crates!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>SQLite on macOS: Not ACID compliant with the bundled version</title>
		<published>2022-06-14T00:00:00+00:00</published>
		<updated>2022-06-14T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/acid-on-apple/" type="text/html"/>
		<id>https://bonsaidb.io/blog/acid-on-apple/</id>
		<content type="html">&lt;p&gt;I&#x27;m building &lt;a href=&quot;&#x2F;about&quot;&gt;a database&lt;&#x2F;a&gt;, and I consider SQLite a &amp;quot;gold standard&amp;quot; to
compare my database against. While benchmarking new code recently, I noticed
Apple&#x27;s bundled version of SQLite is not ACID compliant.&lt;&#x2F;p&gt;
&lt;p&gt;I do not consider myself an expert on these topics. If there are any errors in
my analysis, please reach out to me, and I will correct them immediately. I&#x27;m
learning by doing, and as evidenced by &lt;a href=&quot;&#x2F;blog&#x2F;durable-writes&quot;&gt;my last post on this
topic&lt;&#x2F;a&gt;, I have made my own fair share of mistakes in
trying to implement a fast, ACID-compliant database.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Confusion%20about%20SQLite%20and%20F_BARRIERFSYNC&quot; name=&quot;Confusion%20about%20SQLite%20and%20F_BARRIERFSYNC&quot;&gt;
    Confusion about SQLite and F_BARRIERFSYNC
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;On February 17, 2022, &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;numist&#x2F;status&#x2F;1494392674014531593&quot;&gt;Scott Perry wrote this in a conversation on
Twitter:&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;There&#x27;s a third sync operation that lets you have your performance and write
ordering too: F_BARRIERFSYNC. SQLite already uses it on Darwin, and it&#x27;s part
of the best practices guide for I&#x2F;O reduction.
&lt;a href=&quot;https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;xcode&#x2F;reducing-disk-writes&quot;&gt;https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;xcode&#x2F;reducing-disk-writes&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Some people (myself included) interpretted the statement &amp;quot;SQLite already uses it
on Darwin&amp;quot; to mean that it&#x27;s the default behavior. My post will show that this
is not the case. By default, the bundled version of SQLite distributed in macOS
12.4 (21F79) relies on &lt;code&gt;fsync()&lt;&#x2F;code&gt; for synchronization.&lt;&#x2F;p&gt;
&lt;p&gt;From my investigation, Apple&#x27;s version of SQLite instead replaces &lt;code&gt;PRAGMA fullfsync = on&lt;&#x2F;code&gt;&#x27;s implementation to use &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;SQLite users who are expecting &lt;code&gt;PRAGMA fullfsync&lt;&#x2F;code&gt; to provide durability
guarantees in the event of power failures or kernel panics can &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;numist&#x2F;status&#x2F;1536859148897226753&quot;&gt;override xSync
via a custom VFS&lt;&#x2F;a&gt; or build SQLite from source.&lt;&#x2F;p&gt;
&lt;p&gt;To understand why, let&#x27;s review how to ensure persistent writes on Apple&#x27;s
operating systems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#Recap%3A%20How%20to%20guarantee%20data%20is%20saved%20to%20disk%20on%20Mac&amp;#x2F;iOS%3F&quot; name=&quot;Recap%3A%20How%20to%20guarantee%20data%20is%20saved%20to%20disk%20on%20Mac&amp;#x2F;iOS%3F&quot;&gt;
    Recap: How to guarantee data is saved to disk on Mac&amp;#x2F;iOS?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;There are two APIs we need to cover: &lt;code&gt;fsync()&lt;&#x2F;code&gt; and &lt;code&gt;fcntl()&lt;&#x2F;code&gt;. On Linux,
&lt;code&gt;fsync()&lt;&#x2F;code&gt; is the system call that is tells the kernel to fully synchronize its
file state with the disk. It is debatable whether the original POSIX
specification intends for this level of durability guarantees of &lt;code&gt;fsync()&lt;&#x2F;code&gt;, but
on Linux it tries its best to guarantee all bits changed have been synchronized
to the disk including issuing a flush of any affected volatile write caches.&lt;&#x2F;p&gt;
&lt;p&gt;However, on macOS, the man page for &lt;code&gt;fsync()&lt;&#x2F;code&gt; reads:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that while fsync() will flush all data from the host to the drive (i.e.
the &amp;quot;permanent storage device&amp;quot;), the drive itself may not physically write the
data to the platters for quite some time and it may be written in an
out-of-order sequence.&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, if the drive loses power or the OS crashes, the application may
find that only some or none of their data was written.  The disk drive may
also re-order the data so that later writes may be present, while earlier
writes are not.&lt;&#x2F;p&gt;
&lt;p&gt;This is not a theoretical edge case.  This scenario is easily reproduced with
real world workloads and drive power failures.&lt;&#x2F;p&gt;
&lt;p&gt;For applications that require tighter guarantees about the integrity of their
data, Mac OS X provides the F_FULLFSYNC fcntl.  The F_FULLFSYNC fcntl asks the
drive to flush all buffered data to permanent storage.  Applications, such as
databases, that require a strict ordering of writes should use F_FULLFSYNC to
ensure that their data is written in the order they expect.  Please see
fcntl(2) for more detail.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Apple&#x27;s documentation clearly states that for any guarantees about data loss due
to power loss or kernel panic, you must use the &lt;code&gt;fcntl()&lt;&#x2F;code&gt; API with the
&lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; command.&lt;&#x2F;p&gt;
&lt;p&gt;Back in February of this year, this topic circulated fairly widely, and &lt;a href=&quot;https:&#x2F;&#x2F;mjtsai.com&#x2F;blog&#x2F;2022&#x2F;02&#x2F;17&#x2F;apple-ssd-benchmarks-and-f_fullsync&#x2F;&quot;&gt;this
post from Michael Tsai&lt;&#x2F;a&gt; has a summary of the findings. In short, it was
noted that &lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; is incredibly slow in its current implementation. It
was noted that Apple points users to another &lt;code&gt;fcntl()&lt;&#x2F;code&gt; command in its &lt;a href=&quot;https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;xcode&#x2F;reducing-disk-writes&quot;&gt;&amp;quot;Reducing
Disk Writes&amp;quot;&lt;&#x2F;a&gt; article:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some apps require a write barrier to ensure data persistence before subsequent
operations can proceed. Most apps can use the fcntl(&lt;em&gt;:&lt;&#x2F;em&gt;:) F_BARRIERFSYNC for
this.&lt;&#x2F;p&gt;
&lt;p&gt;Only use F_FULLFSYNC when your app requires a strong expectation of data
persistence. Note that F_FULLFSYNC represents a best-effort guarantee that iOS
writes data to the disk, but data can still be lost in the case of sudden
power loss.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; issues an IO barrier such that all subsequent IO operations
must wait for all current writes to succeed. The &lt;code&gt;fcntl()&lt;&#x2F;code&gt; call returns after
issuing the barrier, but before the data is synchronized. This is why using
&lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; doesn&#x27;t fulfill the durability requirement of ACID: the changes
are confirmed before the data is fully synchronized.&lt;&#x2F;p&gt;
&lt;p&gt;I should note that while &lt;code&gt;fcntl()&lt;&#x2F;code&gt; is an API that is available on Linux,
&lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; and &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; are specific to Apple OSes. Linux has no need
for these options as &lt;code&gt;fsync()&lt;&#x2F;code&gt; provides the guarantees needed.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Does%20SQLite%20use%20F_BARRIERFSYNC%20on%20Apple%20OSes%3F&quot; name=&quot;Does%20SQLite%20use%20F_BARRIERFSYNC%20on%20Apple%20OSes%3F&quot;&gt;
    Does SQLite use F_BARRIERFSYNC on Apple OSes?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;When starting my new low-level storage layer (&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;sediment&quot;&gt;Sediment&lt;&#x2F;a&gt;), I added
support to optionally use &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; instead of &lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; on macOS.
By default, &lt;code&gt;F_FULLSYNC&lt;&#x2F;code&gt; would still be used as I wanted the user to explicitly
opt-out of ACID if they needed the extra performance on Apple hardware. This was
based on the idea that SQLite was using this same approach to achieve its very
fast speed on macOS.&lt;&#x2F;p&gt;
&lt;p&gt;Yesterday, I created a simple benchmark to see where Sediment&#x27;s performance was
currently at. I&#x27;m not ready to share numbers, and that&#x27;s not the point of this
post. The summary, however, is that Sediment was faster than SQLite on Linux,
but slower than SQLite on my M1 Macbook Air.&lt;&#x2F;p&gt;
&lt;p&gt;That puzzled me, because if both SQLite and Sediment are using the same
synchronization primitives, how could the performance difference be inverted
between by switching operating systems? I decided to investigate how SQLite
utilized &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;My first stop was the documentation. SQLite has a pragma to &lt;a href=&quot;http:&#x2F;&#x2F;www3.sqlite.org&#x2F;pragma.html#pragma_fullfsync&quot;&gt;enable
&lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, but I could not find any documentation talking
about &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt;. The documentation for &lt;code&gt;PRAGMA fullfsync&lt;&#x2F;code&gt; states that the
default value is off.&lt;&#x2F;p&gt;
&lt;p&gt;My next stop was the SQLite source code: &lt;code&gt;full_fsync()&lt;&#x2F;code&gt; is defined in
&lt;a href=&quot;https:&#x2F;&#x2F;sqlite.org&#x2F;src&#x2F;file?name=src&#x2F;os_unix.c&amp;amp;ci=b1be2259&quot;&gt;os_unix.c&lt;&#x2F;a&gt;. Its responsibility is to perform a full fsync based
on the available and configured options. This section is what is relevant for
Apple OSes:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-c &quot;&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;#elif&lt;&#x2F;span&gt;&lt;span&gt; HAVE_FULLFSYNC
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span&gt;( fullSync ){
&lt;&#x2F;span&gt;&lt;span&gt;    rc = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;osFcntl&lt;&#x2F;span&gt;&lt;span&gt;(fd, F_FULLFSYNC, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;span&gt;  }&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;else&lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    rc = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;* If the FULLFSYNC failed, fall back to attempting an fsync().
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  ** It shouldn&amp;#39;t be possible for fullfsync to fail on the local 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  ** file system (on OSX), so failure indicates that FULLFSYNC
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  ** isn&amp;#39;t supported for this file system. So, attempt an fsync 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  ** and (for now) ignore the overhead of a superfluous fcntl call.  
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  ** It&amp;#39;d be better to detect fullfsync support once and avoid 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  ** the fcntl call every time sync is called.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  *&#x2F;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span&gt;( rc ) rc = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fsync&lt;&#x2F;span&gt;&lt;span&gt;(fd);
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;#elif defined&lt;&#x2F;span&gt;&lt;span&gt;(__APPLE__)
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;* fdatasync() on HFS+ doesn&amp;#39;t yet flush the file size if it changed correctly
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  ** so currently we default to the macro that redefines fdatasync to fsync
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;  *&#x2F;
&lt;&#x2F;span&gt;&lt;span&gt;  rc = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fsync&lt;&#x2F;span&gt;&lt;span&gt;(fd);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The SQLite source code shows the implementation for calling &lt;code&gt;fcntl()&lt;&#x2F;code&gt; with
&lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt;, but has no mention of &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s one last thing to check: maybe Apple ships a custom build of SQLite that
utilizes &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt;. The best way to verify is to use dtrace to log out
the system calls the process makes.&lt;&#x2F;p&gt;
&lt;p&gt;I disabled System Integrity Protection so that I could trace the &lt;code&gt;sqlite3&lt;&#x2F;code&gt;
executable that ships with macOS 12.4 (21F79):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;~ &lt;&#x2F;span&gt;&lt;span&gt;% sudo dtruss&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt; -t&lt;&#x2F;span&gt;&lt;span&gt; fcntl sqlite3 test.sqlite
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;SYSCALL&lt;&#x2F;span&gt;&lt;span&gt;(args)    = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;return
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;SQLite&lt;&#x2F;span&gt;&lt;span&gt; version 3.37.0 2021-12-09 01:34:53
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;Enter &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;.help&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; for usage hints.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x3F, 0x6BDBD9C0)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x4, 0x32, 0x16BDBDDA8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;sqlite&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; insert into test (a) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;values&lt;&#x2F;span&gt;&lt;span&gt; (1);
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBD0B8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBD0B8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBD0B8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBCBA8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDE98)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDE98)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDE98)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDF08)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x4, 0x5F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x4, 0x3F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x5, 0x5F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDEE8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDEE8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x5, 0x5F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDE88)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDE88)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16BDBDEB8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This log shows all of the &lt;code&gt;fcntl()&lt;&#x2F;code&gt; calls issued by SQLite to perform the
&lt;code&gt;insert into test (a) values (1);&lt;&#x2F;code&gt; statement. The second argument is the
command. We can see SQLite is using commands 0x3F, 0x5A, and 0x5F. In decimal,
those are 63, 90, and 95 respectively. Looking in &lt;code&gt;fcntl.h&lt;&#x2F;code&gt;, we see these values:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-c &quot;&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;#define &lt;&#x2F;span&gt;&lt;span&gt;F_FULLFSYNC             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;51      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;* fsync + ask the drive to flush to the media *&#x2F;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;#define &lt;&#x2F;span&gt;&lt;span&gt;F_GETPROTECTIONCLASS    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;63      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;* Get the protection class of a file from the EA, returns int *&#x2F;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;#define &lt;&#x2F;span&gt;&lt;span&gt;F_BARRIERFSYNC          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;85      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;* fsync + issue barrier to drive *&#x2F;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The commands for 90 and 95 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apple&#x2F;darwin-xnu&#x2F;blob&#x2F;2ff845c2e033bd0ff64b5b6aa6063a1f8f65aa32&#x2F;bsd&#x2F;sys&#x2F;fcntl.h#L360-L370&quot;&gt;are private&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-c &quot;&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;#define &lt;&#x2F;span&gt;&lt;span&gt;F_OFD_SETLK             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;90      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;* Acquire or release open file description lock *&#x2F;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;#define &lt;&#x2F;span&gt;&lt;span&gt;F_SETCONFINED           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;95      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;* &amp;quot;confine&amp;quot; OFD to process *&#x2F;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We did not see any &lt;code&gt;fcntl()&lt;&#x2F;code&gt; calls with the command argument being 85 (0x55).
Let&#x27;s try enabling &lt;code&gt;PRAGMA fullfsync&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;sqlite&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; pragma fullfsync=on;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;sqlite&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; insert into test (a) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;values&lt;&#x2F;span&gt;&lt;span&gt; (1);
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DEF8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DEF8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DEF8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DF68)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x4, 0x5F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x4, 0x3F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DF48)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DF48)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x4, 0x55, 0x0)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x5, 0x5F, 0x1)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x4, 0x55, 0x0)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x55, 0x0)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DEE8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DEE8)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;fcntl&lt;&#x2F;span&gt;&lt;span&gt;(0x3, 0x5A, 0x16DD9DF18)   = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt; 0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;As expected, we have a new &lt;code&gt;fcntl()&lt;&#x2F;code&gt; command: 0x55. Unexpectedly, however,
instead of enabling &lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; (0x33) as we would expect from reading the
publicly available SQLite code, we see 0x55 instead which is &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To summarize, Apple&#x27;s SQLite doesn&#x27;t use &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; or &lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; by
default, and it replaces &lt;code&gt;fcntl(.., F_FULLFSYNC, ..)&lt;&#x2F;code&gt; with &lt;code&gt;fcntl(.., F_BARRIERFSYNC, ..)&lt;&#x2F;code&gt; when &lt;code&gt;PRAGMA fullfsync&lt;&#x2F;code&gt; is enabled.&lt;&#x2F;p&gt;
&lt;p&gt;This behavior was &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;numist&#x2F;status&#x2F;1536830264214638593&quot;&gt;confirmed by Scott
Perry&lt;&#x2F;a&gt; as I was editing
this post.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#How%20important%20is%20this%3F&quot; name=&quot;How%20important%20is%20this%3F&quot;&gt;
    How important is this?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;For most consumer applications, &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; will be enough to provide
reasonable durability with the benefit of performing much more quickly. However,
there are some situations where true ACID compliance is desired. Many (but not
all) of those situations involve server software.&lt;&#x2F;p&gt;
&lt;p&gt;With Apple no longer shipping server hardware and the performance of
&lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; &lt;a href=&quot;https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30371857&quot;&gt;on Apple&#x27;s drives&lt;&#x2F;a&gt;, it&#x27;s hard to fault Apple for
making the decision to use &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; in their version of SQLite. I wish
they would have opted to do it in a different way, such as a new pragma or
changing the default &lt;code&gt;fsync()&lt;&#x2F;code&gt; behavior instead of replacing &lt;code&gt;PRAMGA fullfsync&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s very confusing when a feature that&#x27;s &lt;a href=&quot;http:&#x2F;&#x2F;www3.sqlite.org&#x2F;pragma.html#pragma_fullfsync&quot;&gt;documented to be specific to
macOS&lt;&#x2F;a&gt; doesn&#x27;t behave as documented on macOS. As it stands, if
a developer wants the documented behavior, the easiest way probably is to build
SQLite from source.&lt;&#x2F;p&gt;
&lt;p&gt;I did not test any of these findings on iOS -- it&#x27;s been years since I have
tried doing any tracing on a device. I suspect Apple doesn&#x27;t maintain separate
versions of SQLite for iOS and macOS, but because their version of SQLite is
closed source, we cannot verify easily.&lt;&#x2F;p&gt;
&lt;p&gt;Regardless of whether Apple changes how SQLite synchronizes in the future, I
encourage Apple to publish their updates to SQLite alongside their other open
source repositories. I can&#x27;t imagine the changes made to SQLite would be
considered proprietary, and the ability to understand what differs between
SQLite&#x27;s source code and the shipping version in Apple&#x27;s operating systems is
important in understanding what guarantees SQLite provides on Apple&#x27;s hardware.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Optimizing BonsaiDb: Improving Transaction Batching (Part 1 of ?)</title>
		<published>2022-05-31T00:00:00+00:00</published>
		<updated>2022-05-31T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/optimizing-bonsaidb-p1/" type="text/html"/>
		<id>https://bonsaidb.io/blog/optimizing-bonsaidb-p1/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#First%2C%20a%20Thank%20You&quot; name=&quot;First%2C%20a%20Thank%20You&quot;&gt;
    First, a Thank You
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;My &lt;a href=&quot;&#x2F;blog&#x2F;durable-writes&#x2F;&quot;&gt;last post&lt;&#x2F;a&gt; covered how a couple of small but
impactful mistakes severely impacted &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;&#x27;s performance. The
numerous supportive messages and comments I received was truly humbling. Thank
you everyone who took the time to reach out in any way.&lt;&#x2F;p&gt;
&lt;p&gt;I had two goals with that post: to let anyone using or considering BonsaiDb
know of the issue, and to reassure any users or potential users that I was
confident in improving BonsaiDb&#x27;s performance.&lt;&#x2F;p&gt;
&lt;p&gt;At the time of writing that post, my ideas on how to improve performance were
half-formed, and I didn&#x27;t have any confidence in what sort of speedups I could
achieve. I was excited to experiment, but I wasn&#x27;t ready to share my thoughts.&lt;&#x2F;p&gt;
&lt;p&gt;There are two things that have helped me gain enough confidence to start sharing
what I&#x27;ve been working on: the numerous heartwarming messages and comments, and
making meaningful progress in testing my ideas.&lt;&#x2F;p&gt;
&lt;p&gt;Today&#x27;s post covers how I&#x27;ve improved &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;&#x27;s transaction batching to
be able to achieve a higher number of transactions per second. I will then
follow up by discussing how a new format for Nebari might be able to close the
remaining performance gap between BonsaiDb and PostgreSQL.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#The%20goal%20of%20improving%20batching&quot; name=&quot;The%20goal%20of%20improving%20batching&quot;&gt;
    The goal of improving batching
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Nebari&#x27;s current transaction batching supports multiple transactions against
different trees (a, b, and c):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;images&#x2F;parallel-tx-now-multi.svg&quot;&gt;&lt;img src=&quot;&amp;#x2F;images&amp;#x2F;parallel-tx-now-multi.svg&quot; class=&quot;block-image&quot; alt=&quot;Nebari v0.5 Multi-tree Transaction Batching&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In the above example, threads 1, 2 and 3 all begin a transaction against
different trees. Because they are all different trees, all threads are able to
operate simultaneously. Once a transaction is committed, it must be added to the
transaction log. The transaction manager automatically will batch multiple
transactions together.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s see what happens when these threads all try to modify the same tree instead (a):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;images&#x2F;parallel-tx-now-single.svg&quot;&gt;&lt;img src=&quot;&amp;#x2F;images&amp;#x2F;parallel-tx-now-single.svg&quot; class=&quot;block-image&quot; alt=&quot;Nebari v0.5 Single-tree Transaction Batching&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Thread 1 acquires the lock on tree a, causing the other two threads to wait
until its released. This doesn&#x27;t happen in Nebari today until the transaction is
fully confirmed. This is the problem that the Commerce Benchmark is BonsaiDb was
exhibiting: transactions would cause backlogs to form until most workers were
blocked waiting on other transactions to complete -- often serially.&lt;&#x2F;p&gt;
&lt;p&gt;What if we could enable this sort of pipelining:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;images&#x2F;parallel-tx-new.svg&quot;&gt;&lt;img src=&quot;&amp;#x2F;images&amp;#x2F;parallel-tx-new.svg&quot; class=&quot;block-image&quot; alt=&quot;New Transaction Batching Strategy&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The above sequence of operations still only allows each thread access to the
tree for writing one at a time, perserving Nebari&#x27;s current single-writer
philosophy. The transactional guarantees are still the same: no transaction is
confirmed complete until it is fully synchronized. The change is in how Nebari is keeping track of the tree states.&lt;&#x2F;p&gt;
&lt;p&gt;Currently, Nebari has a Read and a Write state. The Write state is what is
mutated during a transaction. Once a transaction is confirmed, the Write state
is published to the Read state. The new method introduces a third state: the
committed state. Before unlocking the tree, a clone of the tree&#x27;s Write state is
stored. When the transaction is confirmed, instead of publishing the Write
state, the cloned state is published.&lt;&#x2F;p&gt;
&lt;p&gt;This subtle change allows a second transaction to acquire the lock and update
the Write state. It too will clone the state and release the tree. If this
happened more quickly than the first transaction took to synchronize, even more
transactions may proceed and be batched. The only edge case to worry about is
that an older state being published should not be able to overwrite a newer
state.&lt;&#x2F;p&gt;
&lt;p&gt;This change would enable true &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multiversion_concurrency_control&quot;&gt;Multi-Version Concurrency Control (MVCC)&lt;&#x2F;a&gt;
in Nebari&#x27;s transactions.&lt;&#x2F;p&gt;
&lt;p&gt;I set out on Sunday afternoon to try this new approach.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Initial%20Results%3A%20Pretty%20good&quot; name=&quot;Initial%20Results%3A%20Pretty%20good&quot;&gt;
    Initial Results: Pretty good
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Yesterday just in time for my lunch break, I reached a point where I could
benchmark my &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;pull&#x2F;56&quot;&gt;new transaction batching code&lt;&#x2F;a&gt;. I am running a specific
profile of the Commerce Benchmark using 16 worker agents, which means the
database is trying to process requests from 16 threads simultaneously.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Backend&lt;&#x2F;th&gt;&lt;th&gt;Total Time&lt;&#x2F;th&gt;&lt;th&gt;Wall Time&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;bonsaidb v0.4.1&lt;&#x2F;td&gt;&lt;td&gt;377.2s&lt;&#x2F;td&gt;&lt;td&gt;23.57s&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;refactor from last blog post&lt;&#x2F;td&gt;&lt;td&gt;320.1s&lt;&#x2F;td&gt;&lt;td&gt;20.01s&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;new approach&lt;&#x2F;td&gt;&lt;td&gt;44.91s&lt;&#x2F;td&gt;&lt;td&gt;2.807s&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;postgresql&lt;&#x2F;td&gt;&lt;td&gt;18.25s&lt;&#x2F;td&gt;&lt;td&gt;1.141s&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;&lt;strong&gt;BonsaiDb v0.4.1 takes ~23 seconds to complete the benchmark. After these
changes, it now takes just shy of 3 seconds.&lt;&#x2F;strong&gt; This is clearly a &lt;em&gt;huge win&lt;&#x2F;em&gt;.
However, PostgreSQL still takes less than half the time, so there&#x27;s still room
for improvement. For those who are curious to dive in beyond the summary, I&#x27;ve
uploaded the &lt;a href=&quot;&#x2F;parallel-tx-p1-commerce-bench&#x2F;&quot;&gt;benchmark report&lt;&#x2F;a&gt; of the new
approach.&lt;&#x2F;p&gt;
&lt;p&gt;I compared the timeline view of profiling data in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;KDAB&#x2F;hotspot&quot;&gt;Hotspot&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;images&#x2F;parallel-writes-before-after.png&quot;&gt;&lt;img src=&quot;&amp;#x2F;images&amp;#x2F;parallel-writes-before-after.png&quot; class=&quot;block-image&quot; alt=&quot;Commerce Benchmark Overview from Scaleway&quot;  width=&quot;75%&quot;  &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I recognize this image might be hard to read on mobile devices. Clicking the
image will allow you to see it at full resolution. Both timelines are zoomed in
to show 100ms of activity. On the left is BonsaiDb v0.4.1 and on the right is
the updated code.&lt;&#x2F;p&gt;
&lt;p&gt;The orange colored parts are where CPU samples were taken. The green areas are
where those threads have no samples -- they were blocked. It&#x27;s clear at a quick
glance that there are a lot more orange areas on the right side, which is what
we&#x27;d expect given the higher throughput.&lt;&#x2F;p&gt;
&lt;p&gt;The other observation to note is the transaction log&#x27;s thread activity.
Theoretically, that thread&#x27;s activity should be roughtly the same, as it was and
still will be bottlenecked by file flushing. The thread that is highlighted blue
is the transaction log thread. While there is variation between the graphs, the
amount of activity and the space between them is roughly the same on average.
That is exactly what I was expecting to see.&lt;&#x2F;p&gt;
&lt;p&gt;Now, Nebari is bottlenecked by the frequency of flushes, and the Commerce
Benchmark is reaching its performance limit due to only having a fixed number of
worker agents. Once all agents are waiting on a transaction, everything is
blocked until the next batch is committed.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#Where%27s%20the%20remaining%20performance&quot; name=&quot;Where%27s%20the%20remaining%20performance&quot;&gt;
    Where&amp;#x27;s the remaining performance
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;One challenge that Nebari faces with the current format is that the transaction
log does not record which trees were involved in the transaction. If we can&#x27;t
guarantee that all trees are fully flushed before we write the transaction log,
we need some way to know upon loading the transaction log that the latest log
entry is valid. Because we don&#x27;t currently have a list of trees in the log
itself, there&#x27;s no way for the log to validate the latest log.&lt;&#x2F;p&gt;
&lt;p&gt;This is why the transaction log requires that all trees are fully flushed before
being written itself. If the log format were changed, we could add a list of
committed trees to each entry. This would be all that&#x27;s needed to allow Nebari
to perform a single flush per transaction batch, which should nearly double the
throughput with the current design. However, I suspect that wouldn&#x27;t be enough
to fully match PostgreSQL&#x27;s performance.&lt;&#x2F;p&gt;
&lt;p&gt;PostgreSQL, as well as many other database engines, pre-allocate chunks of
storage rather than appending to the end of the file for each write.
Additionally, when data is freed, other engines will often reuse that space. In
the previous &lt;a href=&quot;&#x2F;blog&#x2F;durable-writes&quot;&gt;blog post&lt;&#x2F;a&gt;, I included some results from a
benchmark showing how preallocating can drastically reduce overall write times.&lt;&#x2F;p&gt;
&lt;p&gt;Converting Nebari into a format that can preallocate yet still remain
append-only is one approach, but I&#x27;m not certain it&#x27;s worth the effort. Because
the above changes necessitate a format upgrade, I feel like my best option would
be to re-evaluate my options for how Nebari works to try to achieve the best
results.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#Designing%20a%20new%20storage%20format%20for%20Nebari&quot; name=&quot;Designing%20a%20new%20storage%20format%20for%20Nebari&quot;&gt;
    Designing a new storage format for Nebari
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Despite Nebari being designed as an append-only format, the majority of the code
base interacts with the file using two functions: &lt;code&gt;read_chunk()&lt;&#x2F;code&gt; and
&lt;code&gt;write_chunk()&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;write_chunk()&lt;&#x2F;code&gt; writes a series of bytes to the file and returns a &lt;code&gt;u64&lt;&#x2F;code&gt;. This
number is the offset into the file that the data is stored at, but all of the
B+Tree code treats this number as an opaque ID. The only way it is used is with
subsequent calls to &lt;code&gt;read_chunk()&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Because this abstraction exists, it seems pretty clear that the new format&#x27;s API
should look something like:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Ability to write one or more chunks of data in parallel with other writers,
with each chunk of data being assigned a unique id.&lt;&#x2F;li&gt;
&lt;li&gt;Ability to ask for all the pending writes to be flushed.&lt;&#x2F;li&gt;
&lt;li&gt;Ability to store a &amp;quot;root&amp;quot; header.&lt;&#x2F;li&gt;
&lt;li&gt;Ability to retrieve a previously stored chunk of data using its id.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;My overall goals of this new underlying storage layer would be:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Uses a single flush operation for integrity.&lt;&#x2F;li&gt;
&lt;li&gt;Supports multiple concurrent readers and writers.&lt;&#x2F;li&gt;
&lt;li&gt;Preallocates disk space in chunks.&lt;&#x2F;li&gt;
&lt;li&gt;Disk space can be released and reused.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If such a format could be built, it would be a fairly trivial operation to port
Nebari to it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#Sediment%3A%20A%20new%20foundation%20for%20Nebari&quot; name=&quot;Sediment%3A%20A%20new%20foundation%20for%20Nebari&quot;&gt;
    Sediment: A new foundation for Nebari
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;sediment&quot;&gt;Sediment&lt;&#x2F;a&gt; is my mostly-still-theorized file format that is aimed at
providing the API and goals outlined above.&lt;&#x2F;p&gt;
&lt;p&gt;Sediment will use a single-file architecture that allows storing blobs of data
with ACID compliance. Each stored blob is assigned a &lt;code&gt;GrainId&lt;&#x2F;code&gt; which can be used
to retrieve the data or release the data in the future.&lt;&#x2F;p&gt;
&lt;p&gt;Sediment follows my neverending need to name things in cute but meaningful ways.
The file is organized into a hierarchy of Basins, Stratum, and Grains. A
&lt;code&gt;GrainId&lt;&#x2F;code&gt; identifies which grain inside of which stratum inside of which basin
the data is stored.&lt;&#x2F;p&gt;
&lt;p&gt;Nearly ever header stored in the file will be stored twice. When a new set of
updates are being written, new data only touches the outdated copy. There are
multiple validations that can happen to ensure that each page touched by a
commit was flushed. This design is how Sediment will be able to use a single
flush operation to persist a batch of writes to disk.&lt;&#x2F;p&gt;
&lt;p&gt;Once a &lt;code&gt;GrainId&lt;&#x2F;code&gt; has data, it is immutable. The only two operations that can be
performed on a &lt;code&gt;GrainId&lt;&#x2F;code&gt; are: &lt;code&gt;get&lt;&#x2F;code&gt; and &lt;code&gt;archive&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Archiving a grain will prepare the grain to be reused. Grains are not
immediately freed, however. Instead, Sediment assigns each commit a
&lt;code&gt;SequenceId&lt;&#x2F;code&gt;. Sediment supports a built-in log that could be used to power a
replication log. To ensure data is not overwritten before a consumer of the log,
Sediment will allow the user to checkpoint the log to a specific &lt;code&gt;SequenceId&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Once the &lt;code&gt;SequenceId&lt;&#x2F;code&gt; of a given archive operation has been checkpointed, the
&lt;code&gt;GrainId&lt;&#x2F;code&gt;s will be marked as free and be able to be reused.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#Will%20this%20make%20BonsaiDb%20fast%20again%3F&quot; name=&quot;Will%20this%20make%20BonsaiDb%20fast%20again%3F&quot;&gt;
    Will this make BonsaiDb fast again?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I don&#x27;t expect the result of this to be that BonsaiDb is suddenly &lt;em&gt;faster&lt;&#x2F;em&gt; than
PostgreSQL. The reason is simple: when I first saw the benchmark results in
January, it took me a very long time convincing myself the PostgreSQL numbers
were correct before I allowed myself to get excited.&lt;&#x2F;p&gt;
&lt;p&gt;I had never heard that PostgreSQL is slow -- quite the contrary. From my
understanding, if your workload is properly optimized, it&#x27;s actually a very
performant database. I know there are ideas on how to improve its performance,
but overall, performance is not usually not people&#x27;s main complaint with
PostgreSQL and performance is continually improving with each release.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m not going to be able to achieve anywhere near the numbers reported in
January -- they were based on truly flawed processes and methodology. But, I&#x27;m
hopeful I can get close enough to other ACID-compliant database engines that
most users shouldn&#x27;t have issues with BonsaiDb&#x27;s performance moving forward.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-7&quot;&gt;


&lt;a href=&quot;#Why%20not%20ship%20what%20I%20have%3F&quot; name=&quot;Why%20not%20ship%20what%20I%20have%3F&quot;&gt;
    Why not ship what I have?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The problem with the current set of changes is that they are a complete format
change from v0.4.1. While Nebari&#x27;s format hasn&#x27;t changed today, BonsaiDb is now
using Nebari completely differently. This means that I need to design an upgrade
process to migrate the file format.&lt;&#x2F;p&gt;
&lt;p&gt;With my confidence level gaining in my new format&#x27;s design, I am not very
tempted to subject users to two format upgrades. While this decision means that
these performance updates will take longer to get into users hands, it means
less legacy code for me to support during the alpha period. At this stage in
BonsaiDb&#x27;s development, I have to prioritize maintainability.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-8&quot;&gt;


&lt;a href=&quot;#I%27m%20excited&quot; name=&quot;I%27m%20excited&quot;&gt;
    I&amp;#x27;m excited
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;While it would have been nice to have been able to work on other features the
last few weeks, I&#x27;m truly excited by the prospect of this new format. One of the
biggest gotchas of benchmarking BonsaiDb (before the recent issues) was the
caveat that a &amp;quot;compact&amp;quot; operation had to be performed to reclaim disk space.&lt;&#x2F;p&gt;
&lt;p&gt;With this new format, there will no longer be that caveat. There will still be
maintenance tasks that can be performed to try to optimize storage, but they
will be optional and shouldn&#x27;t be necessary in normal operation.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m simultaneously relieved and excited that I should be able to stick with
Nebari. The changes in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;250&quot;&gt;my pending view and document storage
rewrite&lt;&#x2F;a&gt; enable some really cool features:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;When indexing a view, Nebari&#x27;s B+Tree now embeds a the view&#x27;s reduced value at
each B+Tree node. This will enable reduce queries across ranges to be
optimized and not require accessing as much data when computing the results.&lt;&#x2F;li&gt;
&lt;li&gt;Document revisions are now based on Nebari&#x27;s SequenceId. This change opens the
path for exposing full document revision history, including a list of every
change across the entire collection -- a feature Nebari supported but BonsaiDb
did not have a way to expose efficiently.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Nebari&#x27;s ability to embed your own indexes in the B+Tree nodes and create custom
tree roots make it a unique low-level database offering.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-9&quot;&gt;


&lt;a href=&quot;#What%27s%20next%3F&quot; name=&quot;What%27s%20next%3F&quot;&gt;
    What&amp;#x27;s next?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This post and the last post provide a good summary of what I&#x27;ve been up to for
the past month, so I&#x27;m skipping a &amp;quot;This Month in BonsaiDb&amp;quot; post for May. If I
don&#x27;t have any news before the end of June rolls around, I&#x27;ll make sure to write
an update covering my progress.&lt;&#x2F;p&gt;
&lt;p&gt;In the meantime, if you&#x27;d like to try the current alpha of BonsaiDb, the
&lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of
examples. I have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;&quot;&gt;user&#x27;s
guide&lt;&#x2F;a&gt;, but I think &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;the
documentation&lt;&#x2F;a&gt; is in a better state at the moment.&lt;&#x2F;p&gt;
&lt;p&gt;Thank you again to everyone who took the time to write a note after last week&#x27;s
blog post. I&#x27;m thankful to be part of such a supportive community.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>BonsaiDb performance update: A deep-dive on file synchronization</title>
		<published>2022-05-22T00:00:00+00:00</published>
		<updated>2022-05-23T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/durable-writes/" type="text/html"/>
		<id>https://bonsaidb.io/blog/durable-writes/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#tl%3Bdr%3A%20BonsaiDb%20is%20slower%20than%20previously%20reported&quot; name=&quot;tl%3Bdr%3A%20BonsaiDb%20is%20slower%20than%20previously%20reported&quot;&gt;
    tl;dr: BonsaiDb is slower than previously reported
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The day after &lt;a href=&quot;&#x2F;blog&#x2F;april-2022-update&quot;&gt;the last post&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;justinj&quot;&gt;@justinj&lt;&#x2F;a&gt;
reported that they traced one of the Nebari examples and did not see any &lt;code&gt;fsync&lt;&#x2F;code&gt;
syscalls being executed. This was an honest mistake of misunderstanding the term
&amp;quot;true sink&amp;quot; in &lt;a href=&quot;https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;std&#x2F;io&#x2F;trait.Write.html&quot;&gt;&lt;code&gt;std::io::Write&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. It turns out &lt;code&gt;Write::flush()&lt;&#x2F;code&gt;&#x27;s
implementation for &lt;code&gt;std::io::File&lt;&#x2F;code&gt; is a no-op, as the &amp;quot;true sink&amp;quot; is the kernel,
not the disk.&lt;&#x2F;p&gt;
&lt;p&gt;I released &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;releases&#x2F;tag&#x2F;v0.5.3&quot;&gt;Nebari v0.5.3&lt;&#x2F;a&gt; the same day. I ran the Nebari
benchmark suite and... nothing changed. I ran the suite on GitHub Actions, no
change. I ran the suite on my dedicated VPS I use for a more stable benchmarking
environment than GitHub Actions... no change. I ran the suite on my Mac... huge
slowdown. I&#x27;ll cover why further in this post, but my initial impression was
that I dodged a bullet somehow.&lt;&#x2F;p&gt;
&lt;p&gt;A few days later, I noticed the BonsaiDb Commerce Benchmark was running slowly.
I quickly realized it was due to the synchronization changes and &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;250&quot;&gt;began an
entire rewrite of the view indexer and document storage&lt;&#x2F;a&gt;. A
few days ago, I reached a stage where I could run the suite with my changes.
Excited to see if it was enough to catch back up to PostgreSQL, I ran it and...
it was a little faster but was still very slow.&lt;&#x2F;p&gt;
&lt;p&gt;The rest of this post explores everything I&#x27;ve learned since then. Since this a
summary, let me end with a &lt;strong&gt;tl;dr: Reading data from BonsaiDb is still very
efficient, but due to mistakes in benchmarking, writes are quite slow for
workflows that insert or update a lot of data in a single collection. I am still
excited and motivated to build BonsaiDb, but I am currently uncertain whether I
will still write my own low-level database layer. All assumptions about
BonsaiDb&#x27;s performance must be reset.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#Why%20didn%27t%20my%20refactor%20help%3F&quot; name=&quot;Why%20didn%27t%20my%20refactor%20help%3F&quot;&gt;
    Why didn&amp;#x27;t my refactor help?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The rest of that day and the next two days were spent profiling and trying to
understand the results. I would then try to test my assumptions in an isolated
test, and I wasn&#x27;t able to make significant progress.&lt;&#x2F;p&gt;
&lt;p&gt;The following day as I was sipping coffee, I ran &lt;code&gt;df&lt;&#x2F;code&gt; to check my disk&#x27;s free
space. I realized that each time I ran that command, &lt;code&gt;&#x2F;tmp&lt;&#x2F;code&gt; was always listed as
a separate mountpoint. I use Manjaro Linux (based on Arch), and while I can
generally solve any problems I have with my computer, I never considered the
implications of &lt;code&gt;&#x2F;tmp&lt;&#x2F;code&gt; listed.&lt;&#x2F;p&gt;
&lt;p&gt;Given that it&#x27;s a separate mountpoint than my main filesystem, the next logical
question is: what filesystem does it use? The answer: &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Tmpfs&quot;&gt;tmpfs&lt;&#x2F;a&gt;, a
filesystem that acts like a RAM-disk and never persists your files except
through memory paging. &lt;code&gt;fsync&lt;&#x2F;code&gt; is essentially a no-op on such a filesystem. Many
of my benchmarks and tests used the excellent &lt;a href=&quot;https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;tempfile&quot;&gt;&lt;code&gt;tempfile&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; crate.&lt;&#x2F;p&gt;
&lt;p&gt;Despite having worked with Linux off and on since the early 2000s, I never
noticed this detail. The temporary directory is not a different filesystem on
the Mac, which is one factor in why Nebari&#x27;s benchmarks exhibited a major change
on the Mac while no change on Linux.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Should%20I%20continue%20Nebari%3F&quot; name=&quot;Should%20I%20continue%20Nebari%3F&quot;&gt;
    Should I continue Nebari?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The realization that all of my testing of other database&#x27;s performance was
severely flawed meant that I needed to recheck everything. There is a real
question: should I just ditch Nebari and use another database to back BonsaiDb?
Thinking about my motivations, I&#x27;ve never wanted to create &amp;quot;the fastest
database.&amp;quot;&lt;&#x2F;p&gt;
&lt;p&gt;My pitch for BonsaiDb has always been about developer experience and being &amp;quot;good
enough&amp;quot; for &amp;quot;most people.&amp;quot; I believe there are a lot of developers who waste a
lot of development cycles building apps that are more concerned about
high-availability than being able to scale to be the next Google. At that scale,
a one-size-fits-all solution is almost never the solution. If I can simplify
scaling from a test project to a highly-available solution and do it with
&amp;quot;reasonable&amp;quot; performance, I&#x27;ve met my goals for BonsaiDb.&lt;&#x2F;p&gt;
&lt;p&gt;The benefits of Nebari come down to it being tailor-fit to BonsaiDb&#x27;s needs.
With my new approach to document and view storage that leverages Nebari&#x27;s
ability to embed custom stats within its B+Tree, it meant I could build and
query map-reduce views in a very efficient manner. I have yet to see another
low-level database written in Rust that enables embedding custom information
inside of the B+Tree structure to enable these capabilities.&lt;&#x2F;p&gt;
&lt;p&gt;Because a tailor-fit solution is so attractive, I wanted to explore what it
might take to make Nebari faster.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#Why%20is%20Nebari%20slow%3F&quot; name=&quot;Why%20is%20Nebari%20slow%3F&quot;&gt;
    Why is Nebari slow?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;To answer that question, I needed to fix my usage of &lt;code&gt;tempfile&lt;&#x2F;code&gt; in the Nebari
benchmark. After doing that, Nebari still competed with
&lt;a href=&quot;https:&#x2F;&#x2F;sqlite.org&#x2F;&quot;&gt;SQLite&lt;&#x2F;a&gt; on many benchmarks, but &lt;a href=&quot;https:&#x2F;&#x2F;sled.rs&quot;&gt;Sled&lt;&#x2F;a&gt;
was reporting astonishing numbers. This led me to question whether Sled&#x27;s
transactions are actually ACID-compliant when explicitly asking for them to be
flushed.&lt;&#x2F;p&gt;
&lt;p&gt;See, in my testing, I was able to determine that &lt;code&gt;fdatasync()&lt;&#x2F;code&gt; was unable to
return in less than 1 millisecond on my machine. Here&#x27;s the output if the
transactional insert benchmark measuring the time it takes to do an
ACID-complaint insert of 1KB of data to a new key:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;blobs-insert&#x2F;nebari-versioned&#x2F;1KiB
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;time:   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;2.6591 ms 2.6920 ms 2.7348 ms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;thrpt:  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;365.66 KiB&#x2F;s 371.47 KiB&#x2F;s 376.07 KiB&#x2F;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;blobs-insert&#x2F;nebari&#x2F;1KiB
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;time:   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;2.6415 ms 2.6671 ms 2.7023 ms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;thrpt:  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;370.05 KiB&#x2F;s 374.93 KiB&#x2F;s 378.57 KiB&#x2F;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;blobs-insert&#x2F;sled&#x2F;1KiB&lt;&#x2F;span&gt;&lt;span&gt;  time:   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;38.438 us 38.894 us 39.376 us&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]                                    
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;thrpt:  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;24.801 MiB&#x2F;s 25.108 MiB&#x2F;s 25.406 MiB&#x2F;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;blobs-insert&#x2F;sqlite&#x2F;1KiB
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;time:   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;4.0630 ms 4.1192 ms 4.1913 ms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;thrpt:  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;238.59 KiB&#x2F;s 242.76 KiB&#x2F;s 246.12 KiB&#x2F;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;As you can see, Nebari sits in the middle with 2.6ms, SQLite is the slowest with
4.11ms, and Sled reports an incredibly short 38.9s. After a quick skim of
Sled&#x27;s source, Sled isn&#x27;t calling &lt;code&gt;fdatasync()&lt;&#x2F;code&gt; most of the time when you ask it
to flush its buffers. It&#x27;s using a different API: &lt;code&gt;sync_file_range()&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;From the output above, you might be tempted to infer that Sled never calls
&lt;code&gt;fsdatasync&lt;&#x2F;code&gt;, since the max single iteration time for Sled is 39.4s, and I
claim that &lt;code&gt;fdatasync&lt;&#x2F;code&gt; never completes in under 1ms on my machine. However,
Criterion uses sampling-based statistics, which means that it doesn&#x27;t look at
individual iteration times but rather iteration times for a set number of
iterations.&lt;&#x2F;p&gt;
&lt;p&gt;By logging out each individual iteration time, I can see that there are
individual iterations that &lt;em&gt;do&lt;&#x2F;em&gt; take as long as an &lt;code&gt;fdatasync&lt;&#x2F;code&gt; call. To
simplify, I created three tests to benchmark:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;append&lt;&#x2F;code&gt;: Write to the end of the file and call &lt;code&gt;fdatasync&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;preappend&lt;&#x2F;code&gt;: When a write needs more space in the file, extend the file using
&lt;code&gt;ftruncate&lt;&#x2F;code&gt; before writing. Call &lt;code&gt;fdatasync&lt;&#x2F;code&gt; after each write.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;syncrange&lt;&#x2F;code&gt;: When a write needs more space, extend the file using &lt;code&gt;ftruncate&lt;&#x2F;code&gt;
and calling &lt;code&gt;fdatasync&lt;&#x2F;code&gt; after the write. When a write does not need more
space, call &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; to persist the newly written data.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The output of &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ecton&#x2F;sync-tests&#x2F;blob&#x2F;main&#x2F;benches&#x2F;durable-writes.rs&quot;&gt;this benchmark&lt;&#x2F;a&gt; is:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;writes&#x2F;append&lt;&#x2F;span&gt;&lt;span&gt;           time:   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;2.6211 ms 2.6585 ms 2.7058 ms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;writes&#x2F;preallocate&lt;&#x2F;span&gt;&lt;span&gt;      time:   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;1.2467 ms 1.2675 ms 1.2923 ms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;writes&#x2F;syncrange&lt;&#x2F;span&gt;&lt;span&gt;        time:   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;190.59 us 193.03 us 195.83 us&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Our &lt;code&gt;syncrange&lt;&#x2F;code&gt; benchmark appears to not ever take longer than 195.8s. But, we
know that it calls &lt;code&gt;fdatasync&lt;&#x2F;code&gt;, so what&#x27;s happening? Let&#x27;s open up Criterion&#x27;s raw.csv report for &lt;code&gt;syncrange&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;group&lt;&#x2F;th&gt;&lt;th&gt;function&lt;&#x2F;th&gt;&lt;th&gt;value&lt;&#x2F;th&gt;&lt;th&gt;throughput_num&lt;&#x2F;th&gt;&lt;th&gt;throughput_type&lt;&#x2F;th&gt;&lt;th&gt;sample_measured_value&lt;&#x2F;th&gt;&lt;th&gt;unit&lt;&#x2F;th&gt;&lt;th&gt;iteration_count&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;writes&lt;&#x2F;td&gt;&lt;td&gt;syncrange&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;2,929,333.0&lt;&#x2F;td&gt;&lt;td&gt;ns&lt;&#x2F;td&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;writes&lt;&#x2F;td&gt;&lt;td&gt;syncrange&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;2,932,484.0&lt;&#x2F;td&gt;&lt;td&gt;ns&lt;&#x2F;td&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;writes&lt;&#x2F;td&gt;&lt;td&gt;syncrange&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;5,701,286.0&lt;&#x2F;td&gt;&lt;td&gt;ns&lt;&#x2F;td&gt;&lt;td&gt;18&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;writes&lt;&#x2F;td&gt;&lt;td&gt;syncrange&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;5,788,786.0&lt;&#x2F;td&gt;&lt;td&gt;ns&lt;&#x2F;td&gt;&lt;td&gt;24&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Criterion isn&#x27;t keeping track of every iteration. It keeps track of batches.
Because of this, the final statistics tallied aren&#x27;t able to see the true
maximum iteration time. Let&#x27;s run the same benchmark in my own benchmarking
harness:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Label&lt;&#x2F;th&gt;&lt;th&gt;avg&lt;&#x2F;th&gt;&lt;th&gt;min&lt;&#x2F;th&gt;&lt;th&gt;max&lt;&#x2F;th&gt;&lt;th&gt;stddev&lt;&#x2F;th&gt;&lt;th&gt;out%&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;append&lt;&#x2F;td&gt;&lt;td&gt;2.628ms&lt;&#x2F;td&gt;&lt;td&gt;2.095ms&lt;&#x2F;td&gt;&lt;td&gt;5.702ms&lt;&#x2F;td&gt;&lt;td&gt;147.0us&lt;&#x2F;td&gt;&lt;td&gt;0.004%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;preallocate&lt;&#x2F;td&gt;&lt;td&gt;1.243ms&lt;&#x2F;td&gt;&lt;td&gt;612.3us&lt;&#x2F;td&gt;&lt;td&gt;4.042ms&lt;&#x2F;td&gt;&lt;td&gt;859.3us&lt;&#x2F;td&gt;&lt;td&gt;0.004%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;syncrange&lt;&#x2F;td&gt;&lt;td&gt;189.1us&lt;&#x2F;td&gt;&lt;td&gt;12.83us&lt;&#x2F;td&gt;&lt;td&gt;2.847ms&lt;&#x2F;td&gt;&lt;td&gt;653.6us&lt;&#x2F;td&gt;&lt;td&gt;0.063%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Using this harness, I can now see results that make sense. The averages
match what we see from Criterion, but now our min and max show a wider range. We
can now see that even for the &lt;code&gt;syncrange&lt;&#x2F;code&gt; benchmark, some writes will take
2.8ms.&lt;&#x2F;p&gt;
&lt;p&gt;The takeaway is very important: using &lt;code&gt;sync_file_range&lt;&#x2F;code&gt;, Sled is able to make
the average write&#x27;s time take 38.9s, even though occasionally there will be
write operations that are longer due to the need for an &lt;code&gt;fdatasync&lt;&#x2F;code&gt;
when the file&#x27;s size changes.&lt;&#x2F;p&gt;
&lt;p&gt;Given how much faster &lt;code&gt;sync_file_range()&lt;&#x2F;code&gt; is, is it safe to use to achieve
durable writes?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#What%20does%20sync_file_range%20do%3F&quot; name=&quot;What%20does%20sync_file_range%20do%3F&quot;&gt;
    What does sync_file_range do?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;code&gt;sync_file_range()&lt;&#x2F;code&gt; has many modes of operation. At its core, it offers the
ability to ask the kernel to commit any dirty cached data to the filesystem. For
the purposes of this post, we are most interested in the mode of operation when
&lt;code&gt;SYNC_FILE_RANGE_WAIT_BEFORE&lt;&#x2F;code&gt;,  &lt;code&gt;SYNC_FILE_RANGE_WRITE&lt;&#x2F;code&gt;, and
&lt;code&gt;SYNC_FILE_RANGE_WAIT_AFTER&lt;&#x2F;code&gt; are passed.&lt;&#x2F;p&gt;
&lt;p&gt;With these flags, &lt;code&gt;sync_file_range()&lt;&#x2F;code&gt; is documented to wait for all dirty pages
within the range provided to be flushed to disk. However, the documentation for
this function advises that it is &amp;quot;extremely dangerous.&amp;quot;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#What%20are%20%27durable%20writes%27%3F&quot; name=&quot;What%20are%20%27durable%20writes%27%3F&quot;&gt;
    What are &amp;#x27;durable writes&amp;#x27;?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;When writing data to a file, the operating system does not immediately write the
bits to the phsysical disk. This would be incredibly slow, even with modern
SSDs. Instead, operating systems will typically manage a cache of the underlying
storage and occasionally flush the updated information to the storage. This
allows writes to be fast and enables the operating system to attempt to schedule
and reorder operations for efficiency.&lt;&#x2F;p&gt;
&lt;p&gt;The problem with this approach occurs when the power suddenly is cut to the
machine. Imagine a user hits &amp;quot;Save&amp;quot; in their program, the program confirmed it
was saved, and suddenly the building&#x27;s power dies. The program claimed it saved
the file, but upon rebooting, the file is missing or corrupt. How does that
happen? The file may have only been saved to the kernel&#x27;s cache and never
written to the physical disk.&lt;&#x2F;p&gt;
&lt;p&gt;The solution is called flushing or syncing. Each operating system exposes one or
more functions to ensure all writes to a file have successfully been persisted
to the physical media:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;On Linux, it&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;man.archlinux.org&#x2F;man&#x2F;fsync.2&quot;&gt;&lt;code&gt;fsync()&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;man.archlinux.org&#x2F;man&#x2F;fdatasync.2&quot;&gt;&lt;code&gt;fdatasync()&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, and
&lt;a href=&quot;https:&#x2F;&#x2F;man.archlinux.org&#x2F;man&#x2F;sync_file_range.2&quot;&gt;&lt;code&gt;sync_file_range()&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;On Windows, it&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;docs.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;win32&#x2F;api&#x2F;fileapi&#x2F;nf-fileapi-flushfilebuffers&quot;&gt;&lt;code&gt;FlushFileBuffers&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;On Mac&#x2F;iOS, &lt;code&gt;fsync()&lt;&#x2F;code&gt; is available but does not provide the same guarantees as
Linux. Instead, a call to &lt;code&gt;fcntl&lt;&#x2F;code&gt; with the &lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; option must be used
to trigger a write to physical media.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Rust uses the correct APIs for each platform when calling &lt;code&gt;File::sync_all&lt;&#x2F;code&gt; or
&lt;code&gt;File::sync_data&lt;&#x2F;code&gt; to provide durable writes. The standard library does not
provide APIs to invoke the underlying APIs mentioned above. Thankfully, the
&lt;code&gt;libc&lt;&#x2F;code&gt; crate makes it easy to call the APIs we are interested in for this post.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#Linux%3A%20Is%20sync_file_range%20viable%20for%20durable%20writes%3F&quot; name=&quot;Linux%3A%20Is%20sync_file_range%20viable%20for%20durable%20writes%3F&quot;&gt;
    Linux: Is sync_file_range viable for durable writes?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The &lt;a href=&quot;https:&#x2F;&#x2F;man.archlinux.org&#x2F;man&#x2F;sync_file_range.2&quot;&gt;man page for &lt;code&gt;sync_file_range()&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; includes this warning
(emphasis mine):&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;This system call is &lt;strong&gt;extremely dangerous and should not be used in portable
programs&lt;&#x2F;strong&gt;. None of these operations writes out the file&#x27;s metadata.
Therefore, unless the application is strictly performing overwrites of
already-instantiated disk blocks, there are no guarantees that the data will
be available after a crash. There is no user interface to know if a write is
purely an overwrite. On filesystems using copy-on-write semantics (e.g.,
&lt;em&gt;btrfs&lt;&#x2F;em&gt;) an overwrite of existing allocated blocks is impossible. &lt;strong&gt;When
writing into preallocated space, many filesystems also require calls into the
block allocator, which this system call does not sync out to disk&lt;&#x2F;strong&gt;. This
system call does not flush disk write caches and thus does not provide any
data integrity on systems with volatile disk write caches.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Sled is using it to achieve it&#x27;s incredible speed, and &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;spacejam&#x2F;sled&#x2F;issues&#x2F;1351&quot;&gt;the author is
aware&lt;&#x2F;a&gt; of this warning. One of the commentors on the linked page
points out that RocksDB &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&#x2F;blob&#x2F;bed40e7266b55349ce9d2dce27aeb2055813a5fe&#x2F;env&#x2F;io_posix.cc#L160-L166&quot;&gt;has special code to disable using the API on
&lt;code&gt;zfs&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. Pebble, which is a Go port&#x2F;spinoff of RocksDB, takes the
approach of &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;cockroachdb&#x2F;pebble&#x2F;blob&#x2F;3355a02e7cec7a4cbf775421a89dfbe833818266&#x2F;vfs&#x2F;syncing_file_linux.go#L34-L36&quot;&gt;opting-in &lt;code&gt;ext4&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. Both RocksDB and Pebble seem to
still use &lt;code&gt;fsync&lt;&#x2F;code&gt;&#x2F;&lt;code&gt;fdatasync&lt;&#x2F;code&gt; at various locations to ensure durability.&lt;&#x2F;p&gt;
&lt;p&gt;I decided to look at PostgreSQL&#x27;s source as well. They use &lt;code&gt;sync_file_range()&lt;&#x2F;code&gt;&#x27;s
asynchronous mode to &lt;a href=&quot;https:&#x2F;&#x2F;git.postgresql.org&#x2F;gitweb&#x2F;?p=postgresql.git;a=blob;f=src&#x2F;backend&#x2F;storage&#x2F;file&#x2F;fd.c;hb=e19272ef603bdb11a09e7f8500dc4e0fb4ec73de#l493&quot;&gt;hint to the OS&lt;&#x2F;a&gt; that the writes
need to be flushed, but they still issue &lt;code&gt;fsync&lt;&#x2F;code&gt; or &lt;code&gt;fdatasync&lt;&#x2F;code&gt; as needed.&lt;&#x2F;p&gt;
&lt;p&gt;I also looked to SQLite&#x27;s source: no references. I could not find any relevant
discussion threads either.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m not an expert on any of these databases, so my skim of their codebases
should be taken with a grain of salt.&lt;&#x2F;p&gt;
&lt;p&gt;I tried finding any information about the reliability of &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; for
durable overwrites on various filesystems, and I couldn&#x27;t find anything except
these little bits already linked.&lt;&#x2F;p&gt;
&lt;p&gt;Lacking any definitive answer regarding whether it&#x27;s able to provide durability
on any filesystems, I set out to test this myself.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-7&quot;&gt;


&lt;a href=&quot;#Testing%20sync_file_range%27s%20durability&quot; name=&quot;Testing%20sync_file_range%27s%20durability&quot;&gt;
    Testing sync_file_range&amp;#x27;s durability
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I set up an Ubuntu 20.04 Server virtual machine running kernel
5.4.0-110-generic. While pondering how to best shut the machine down after the
call to &lt;code&gt;sync_file_range&lt;&#x2F;code&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;justinj&quot;&gt;@justinj&lt;&#x2F;a&gt; came to the rescue again by
pointing out that &lt;code&gt;&#x2F;proc&#x2F;sysrq-trigger&lt;&#x2F;code&gt; exists. He also shared &lt;a href=&quot;http:&#x2F;&#x2F;justinjaffray.com&#x2F;durability-and-redo-logging&#x2F;&quot;&gt;his blog
post&lt;&#x2F;a&gt; where he performed similar tests against &lt;code&gt;fsync&lt;&#x2F;code&gt; while
exploring how to build a durable database log.&lt;&#x2F;p&gt;
&lt;p&gt;It turns out if you write &lt;code&gt;o&lt;&#x2F;code&gt; to &lt;code&gt;&#x2F;proc&#x2F;sysrq-trigger&lt;&#x2F;code&gt; on a Linux machine
(requires permissions), it will immediately power off. This greatly simplified
my testing setup.&lt;&#x2F;p&gt;
&lt;p&gt;I executed a VM using this command:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;qemu-system-x86_64 &lt;&#x2F;span&gt;&lt;span&gt;\
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;    -drive&lt;&#x2F;span&gt;&lt;span&gt; file=ubuntu-server,format=qcow2 \
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;    -drive&lt;&#x2F;span&gt;&lt;span&gt; file=extra,format=qcow2 \
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;    -enable-kvm &lt;&#x2F;span&gt;&lt;span&gt;\
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;    -m&lt;&#x2F;span&gt;&lt;span&gt; 2G \
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;    -smp&lt;&#x2F;span&gt;&lt;span&gt; 1 \
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;    -device&lt;&#x2F;span&gt;&lt;span&gt; e1000,netdev=net0 \
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;    -netdev&lt;&#x2F;span&gt;&lt;span&gt; user,id=net0,hostfwd=tcp::2222-:22
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In another terminal, I executed the various examples from &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ecton&#x2F;sync-tests&quot;&gt;the repository&lt;&#x2F;a&gt; over
&lt;code&gt;ssh&lt;&#x2F;code&gt;. After each example executed, the virtual machine would automatically
reboot. By executing examples in a loop, I was able to run these commands for
extended periods of time. My results are:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Filesystem&lt;&#x2F;th&gt;&lt;th&gt;is &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; durable?&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;btrfs&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ext4&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;xfs&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;zfs&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;-8&quot;&gt;


&lt;a href=&quot;#Safely%20extending%20a%20file%27s%20length%20while%20using%20sync_file_range&quot; name=&quot;Safely%20extending%20a%20file%27s%20length%20while%20using%20sync_file_range&quot;&gt;
    Safely extending a file&amp;#x27;s length while using sync_file_range
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;My original testing of &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; showed some failures, but after some
additional testing, I noticed it was only happening with either the first or
second test, but never on subsequent tests for filesystems I&#x27;ve labeled durable
above.&lt;&#x2F;p&gt;
&lt;p&gt;There are two examples that test &lt;code&gt;sync_file_range&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ecton&#x2F;sync-tests&#x2F;blob&#x2F;main&#x2F;examples&#x2F;sync_file_range.rs&quot;&gt;&lt;code&gt;sync_file_range.rs&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;: When initializing the data file, zeroes are manually
written to the file.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ecton&#x2F;sync-tests&#x2F;blob&#x2F;main&#x2F;examples&#x2F;sync_file_range_set_len.rs&quot;&gt;&lt;code&gt;sync_file_range_set_len.rs&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;: When
initializing the data file, &lt;code&gt;File::set_len()&lt;&#x2F;code&gt; is called to extend the file,
which is documented currently as:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;If it is greater than the current files size, then the file will be
extended to size and have all of the intermediate data filled in with 0s.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Both examples use &lt;code&gt;File::sync_all()&lt;&#x2F;code&gt;, and both examples call &lt;code&gt;File::sync_all&lt;&#x2F;code&gt; on
the containing directories to sync the file length change.&lt;&#x2F;p&gt;
&lt;p&gt;On ext4 and xfs, my testing showed that I could reliably reproduce data loss on
the initial run in the &lt;code&gt;sync_file_range_set_len&lt;&#x2F;code&gt; example but not the
&lt;code&gt;sync_file_range&lt;&#x2F;code&gt; example. Subsequent runs were durable. Why is that?&lt;&#x2F;p&gt;
&lt;p&gt;Despite what the Rust documentation states, under the hood, &lt;code&gt;File::set_len&lt;&#x2F;code&gt; uses
&lt;code&gt;ftruncate&lt;&#x2F;code&gt;, which is documented as:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;If the file size is increased, the extended area shall appear as if it were
zero-filled.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The distinction between &amp;quot;shall appear as if it were zero-filled&amp;quot; and &amp;quot;will be
extended to size and have all of the intermediate data filled in with 0s&amp;quot; is
subtle, but very important when considering the safety of &lt;code&gt;sync_file_range&lt;&#x2F;code&gt;. In
my earlier quote of &lt;code&gt;sync_file_range&lt;&#x2F;code&gt;&#x27;s warning, the second emphasis also seems
to relate to these findings.&lt;&#x2F;p&gt;
&lt;p&gt;From my testing, using &lt;code&gt;ftruncate&lt;&#x2F;code&gt; to fill pages with 0 will conflict with
&lt;code&gt;sync_file_range&lt;&#x2F;code&gt; on the first operation, but will likely succeed on future
tests on ext4 and xfs.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-9&quot;&gt;


&lt;a href=&quot;#Volatile%20Write%20Caches&quot; name=&quot;Volatile%20Write%20Caches&quot;&gt;
    Volatile Write Caches
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Update 2022-05-23&lt;&#x2F;strong&gt;: A &lt;a href=&quot;https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;rust&#x2F;comments&#x2F;uvlu5y&#x2F;bonsaidb_performance_update_a_deepdive_on_file&#x2F;i9nvftm&#x2F;&quot;&gt;comment on Reddit&lt;&#x2F;a&gt; correctly pointed
out I skipped discussing this portion of the &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; warning:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;This system call does not flush disk write caches and thus does not provide
any data integrity on systems with volatile disk write caches.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Even with all of the aforementioned preconditions being true, we can&#x27;t guarantee
that &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; if write caching is enabled. This is because the device
itself may have a write cache that is volatile. Unless write caching is
explicitly disabled, the only way for &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; to be safe on ext4 and
xfs is for the user to verify that the devices being used do not have volatile
write caches.&lt;&#x2F;p&gt;
&lt;p&gt;For my NVME boot drive, I&#x27;m able to see that it has a volatile write cache:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;$&lt;&#x2F;span&gt;&lt;span&gt; sudo nvme get-feature&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt; -f&lt;&#x2F;span&gt;&lt;span&gt; 6 &#x2F;dev&#x2F;nvme0n1
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;get-feature:0x06&lt;&#x2F;span&gt;&lt;span&gt; (Volatile Write Cache)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; Current value:0x00000001
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s turn it off and run our benchmark again:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;$&lt;&#x2F;span&gt;&lt;span&gt; sudo nvme set-feature&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt; -f&lt;&#x2F;span&gt;&lt;span&gt; 6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt; -v&lt;&#x2F;span&gt;&lt;span&gt; 0 &#x2F;dev&#x2F;nvme0n1
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;set-feature:0x06&lt;&#x2F;span&gt;&lt;span&gt; (Volatile Write Cache)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; value:00000000, cdw12:00000000, save:0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Label&lt;&#x2F;th&gt;&lt;th&gt;avg&lt;&#x2F;th&gt;&lt;th&gt;min&lt;&#x2F;th&gt;&lt;th&gt;max&lt;&#x2F;th&gt;&lt;th&gt;stddev&lt;&#x2F;th&gt;&lt;th&gt;out%&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;append&lt;&#x2F;td&gt;&lt;td&gt;5.492ms&lt;&#x2F;td&gt;&lt;td&gt;5.123ms&lt;&#x2F;td&gt;&lt;td&gt;12.75ms&lt;&#x2F;td&gt;&lt;td&gt;564.1us&lt;&#x2F;td&gt;&lt;td&gt;0.016%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;preallocate&lt;&#x2F;td&gt;&lt;td&gt;2.763ms&lt;&#x2F;td&gt;&lt;td&gt;1.665ms&lt;&#x2F;td&gt;&lt;td&gt;11.75ms&lt;&#x2F;td&gt;&lt;td&gt;1.685ms&lt;&#x2F;td&gt;&lt;td&gt;0.006%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;syncrange&lt;&#x2F;td&gt;&lt;td&gt;2.025ms&lt;&#x2F;td&gt;&lt;td&gt;1.592ms&lt;&#x2F;td&gt;&lt;td&gt;5.723ms&lt;&#x2F;td&gt;&lt;td&gt;910.1us&lt;&#x2F;td&gt;&lt;td&gt;0.064%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Our sub-millisecond times have vanished. The only reason &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; was
faster was because it was only writing to the volatile write cache. By disabling
the volatile write cache, the benefits of &lt;code&gt;sync_file_range&lt;&#x2F;code&gt; compared to the
preallocation strategy diminish.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-10&quot;&gt;


&lt;a href=&quot;#Conclusions%20about%20sync_file_range&quot; name=&quot;Conclusions%20about%20sync_file_range&quot;&gt;
    Conclusions about sync_file_range
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sync_file_range&lt;&#x2F;code&gt; is only safe to use on specific filesystems. Of the four I tested, &lt;code&gt;xfs&lt;&#x2F;code&gt;
and &lt;code&gt;ext4&lt;&#x2F;code&gt; appear to be completely reliable in their implementations, and
&lt;code&gt;zfs&lt;&#x2F;code&gt; and &lt;code&gt;btrfs&lt;&#x2F;code&gt; both are completely unreliable in their implementations.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;sync_file_range&lt;&#x2F;code&gt; is only safe to use on fully initialized pages.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;ftruncate&lt;&#x2F;code&gt; to extend a file does not fully initialize newly allocated pages
with zeroes and may take shortcuts instead. This makes using &lt;code&gt;sync_file_range&lt;&#x2F;code&gt;
on space allocated with &lt;code&gt;ftruncate&lt;&#x2F;code&gt; or similar operations unsafe to use.&lt;&#x2F;li&gt;
&lt;li&gt;Even with all of these conditions being met, volatile write caches on the disk
must be disabled to ensure full durability.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;-11&quot;&gt;


&lt;a href=&quot;#Mac%20OS&amp;#x2F;iOS%3A%20Does%20F_BARRIERFSYNC%20provide%20durable%20writes%3F&quot; name=&quot;Mac%20OS&amp;#x2F;iOS%3A%20Does%20F_BARRIERFSYNC%20provide%20durable%20writes%3F&quot;&gt;
    Mac OS&amp;#x2F;iOS: Does F_BARRIERFSYNC provide durable writes?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;No. Apple&#x27;s own &lt;a href=&quot;https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;xcode&#x2F;reducing-disk-writes&quot;&gt;documentation makes this clear&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some apps require a write barrier to ensure data persistence before subsequent
operations can proceed. Most apps can use the fcntl(&lt;em&gt;:&lt;&#x2F;em&gt;:) F_BARRIERFSYNC for
this.&lt;&#x2F;p&gt;
&lt;p&gt;Only use F_FULLFSYNC when your app requires a strong expectation of data
persistence.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Great, so &lt;code&gt;fnctl&lt;&#x2F;code&gt; with &lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt; is what is used to instead of &lt;code&gt;fsync&lt;&#x2F;code&gt;.
Let&#x27;s keep reading.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that F_FULLFSYNC represents a best-effort guarantee that iOS writes data
to the disk, but data can still be lost in the case of sudden power loss.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Apple really dropped the ball here. According to all available documentation I
can find: there is no way to run a truly ACID-compliant database on Mac OS. You
can get close, but a power loss could still result in a write being reported as
successfully persisted being gone after a power outage. A &lt;a href=&quot;https:&#x2F;&#x2F;mjtsai.com&#x2F;blog&#x2F;2022&#x2F;02&#x2F;17&#x2F;apple-ssd-benchmarks-and-f_fullsync&#x2F;&quot;&gt;post on Michael
Tsai&#x27;s blog&lt;&#x2F;a&gt; covers the investigation into this in more detail.&lt;&#x2F;p&gt;
&lt;p&gt;One interesting note is that SQLite uses &lt;code&gt;F_BARRIERFSYNC&lt;&#x2F;code&gt; by default for all of
its file synchronization on Mac&#x2F;iOS. Optionally, you can use a &lt;code&gt;#pragma&lt;&#x2F;code&gt; to
enable usage of &lt;code&gt;F_FULLFSYNC&lt;&#x2F;code&gt;. Given the relative overhead of the two APIs in my
limited testing, I can understand their decision, but I&#x27;m not sure it&#x27;s the best
default.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-12&quot;&gt;


&lt;a href=&quot;#Windows%3A%20Are%20there%20any%20APIs%20for%20partially%20syncing%20files%3F&quot; name=&quot;Windows%3A%20Are%20there%20any%20APIs%20for%20partially%20syncing%20files%3F&quot;&gt;
    Windows: Are there any APIs for partially syncing files?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;No. Unless you are utilizing memory mapped files, the only API avialable on
Windows is &lt;a href=&quot;https:&#x2F;&#x2F;docs.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;win32&#x2F;api&#x2F;fileapi&#x2F;nf-fileapi-flushfilebuffers&quot;&gt;&lt;code&gt;FlushFileBuffers&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-13&quot;&gt;


&lt;a href=&quot;#What%20does%20all%20of%20this%20mean%20for%20BonsaiDb%3F&quot; name=&quot;What%20does%20all%20of%20this%20mean%20for%20BonsaiDb%3F&quot;&gt;
    What does all of this mean for BonsaiDb?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Astute readers may have noticed that the Nebari benchmarks claimed to be similar
in performance to SQLite even post-sync changes. This is true on many metrics,
but it&#x27;s not an apples-to-apples comparison, and the difference is the primary
reason BonsaiDb slowed down.&lt;&#x2F;p&gt;
&lt;p&gt;SQLite has many approaches to persistence, but let&#x27;s look at the journaled
version as it&#x27;s fairly straightforward. When using a journal, SQLite creates a
file that contains information needed to undo the changes its about to make to
the database file. To ensure a consistent state that can be recovered after a
power outage at any point in this process, it must make at least two &lt;code&gt;fsync&lt;&#x2F;code&gt;
calls.&lt;&#x2F;p&gt;
&lt;p&gt;Nebari gets away with one &lt;code&gt;fsync&lt;&#x2F;code&gt; operation due to its append-only nature.
However, the moment you use the &lt;code&gt;Roots&lt;&#x2F;code&gt; type, there&#x27;s one more &lt;code&gt;fsync&lt;&#x2F;code&gt; operation:
the transaction log. Thus, Nebari isn&#x27;t actually faster than SQLite when a
transaction log is used, which is a requirement for multi-tree transactions.&lt;&#x2F;p&gt;
&lt;p&gt;This is further exacerbated by Nebari&#x27;s Multi-Reader, Single-Writer model of
transactions. If two threads are trying to write to the same tree, one will
begin its transaction and finish it while the other has to wait patiently for
the lock on the tree to be released.&lt;&#x2F;p&gt;
&lt;p&gt;This two-step sync method combined with contention over a few collections is
what caused the Commerce Benchmark to grind to a halt after &lt;code&gt;fsync&lt;&#x2F;code&gt; was actually
doing real work. Individual worker threads would back up waiting for their turn
to modify a collection.&lt;&#x2F;p&gt;
&lt;p&gt;Nebari&#x27;s architecture was designed in October, and I spent countless hours
profiling and testing its performance. Due to the aforementioned issues with my
methodology, so many of my performance assumptions were flat out wrong.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-14&quot;&gt;


&lt;a href=&quot;#What%27s%20next%3F&quot; name=&quot;What%27s%20next%3F&quot;&gt;
    What&amp;#x27;s next?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;It&#x27;s clear from these results that whatever solution is picked for BonsaiDb, it
needs to support a way to allow multiple transactions to proceed at the same
time. This is a much tougher problem to solve, and I&#x27;m uncertain I want to
tackle this problem myself.&lt;&#x2F;p&gt;
&lt;p&gt;For the longest time, I developed BonsaiDb with minimal &amp;quot;advertising.&amp;quot; Imposter
syndrome prevented me from sharing it for most of 2021. Over the alpha period, I
finally started feeling confidence in its reliability. Now, I&#x27;m back to
questioning whether I should attempt a new version of Nebari.&lt;&#x2F;p&gt;
&lt;p&gt;On one hand, seeing that Nebari is still pretty fast after fixing this bug
should prove to me that I &lt;em&gt;can&lt;&#x2F;em&gt; write a fast database. On the other hand, I&#x27;m so
embarrassed I didn&#x27;t notice these issues earlier, and it&#x27;s demoralizing to think
of all the time spent building upon mistaken assumptions. Nebari will also need
to transition to a more complex architecture, which makes it lose some of the
appeal I had for it.&lt;&#x2F;p&gt;
&lt;p&gt;The only thing I can say with confidence right now is that I still firmly
believe in my vision of BonsaiDb, regardless of what storage layer powers it. I
will figure out my plans soon so that existing users aren&#x27;t left in a lurch for
too long.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, I just want to say thank you to everyone who has supported me through
this journey. Despite the recent stress, BonsaiDb and Nebari have been fun and
rewarding projects to build.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>BonsaiDb April Update: Dogfooding and Fixing Bugs</title>
		<published>2022-05-02T00:00:00+00:00</published>
		<updated>2022-05-02T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/april-2022-update/" type="text/html"/>
		<id>https://bonsaidb.io/blog/april-2022-update/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;This month has been a journey of interesting projects that I have been itching
to write about. The primary focus of the month was to work on a new project to
use and test &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;. Before I dive into that topic, I want to
focus on a bug fix for Nebari that arose from testing this new project.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Nebari%200.5.2%20Update&quot; name=&quot;Nebari%200.5.2%20Update&quot;&gt;
    Nebari 0.5.2 Update
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Yesterday, I published an update to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt; which contains bug fixes
that prevent potential corruption of databases. For users of BonsaiDb, the view
indexer is the only way that this bug could arise, from my understanding of the
bugs discovered.&lt;&#x2F;p&gt;
&lt;p&gt;The problem with this bug is that it&#x27;s a silent bug until a compact operation
occurs. There are various &lt;code&gt;debug_assert!()&lt;&#x2F;code&gt; statements littered in the code that
assert the internal &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;B%2B_tree&quot;&gt;B+ tree&lt;&#x2F;a&gt; order is correct. These are expensive to
perform, so they are only enabled in debug builds. Users who are running
BonsaiDb or Nebari with release builds may not notice the bug, and the database
may return inconsistent results.&lt;&#x2F;p&gt;
&lt;p&gt;The only way I was able to reproduce the bug involved many multi-key
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;nebari&#x2F;latest&#x2F;nebari&#x2F;tree&#x2F;struct.Modification.html&quot;&gt;&lt;code&gt;Modification&lt;&#x2F;code&gt;s&lt;&#x2F;a&gt; that operated in such a way that a bug in the
tree balancing algorithm was found. The edge case only occurred when Nebari
attempted to avoid splitting a node by moving entries both to the previous and
next nodes during the same operation. This is impossible on small trees and is
impossible to trigger by performing operations with only a few keys per
operation or with a workflow that rarely deletes data.&lt;&#x2F;p&gt;
&lt;p&gt;Despite me feeling fairly confident users haven&#x27;t encountered this yet, I would
rest better knowing users have run &lt;code&gt;cargo update&lt;&#x2F;code&gt; to the latest version.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#Fuzzing%20Nebari&quot; name=&quot;Fuzzing%20Nebari&quot;&gt;
    Fuzzing Nebari
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;After I fixed the bug in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;, I proceeded to try to get a
&lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fuzzing&quot;&gt;fuzzing&lt;&#x2F;a&gt; suite set up. For those unfamiliar, fuzzing is an algorithmic
approach to automated testing where a &amp;quot;fuzzer&amp;quot; repeatedly generates parameters
for a test in an attempt to discover bugs. The &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;blob&#x2F;a6894b7c04938c35d4c69b161ae6985cf9de3ea5&#x2F;nebari&#x2F;src&#x2F;tree&#x2F;mod.rs#L2607-L2679&quot;&gt;test I wrote&lt;&#x2F;a&gt;
essentialy was a dumb fuzzer -- the outer loop could be changed to loop forever.&lt;&#x2F;p&gt;
&lt;p&gt;The advantage of my hand-written test is that it was able to narrow down the
failure case quickly, as I knew some of the constraints of the inputs that
produced the bug I encountered. The disadvantage is that it only would find a
specific pattern of failure, as my inputs were not completely random.&lt;&#x2F;p&gt;
&lt;p&gt;I ported the same approach to a fuzzer. The fuzzer is written to take an
arbitrary &lt;code&gt;Vec&amp;lt;BTreeSet&amp;lt;u16&amp;gt;&amp;gt;&amp;gt;&lt;&#x2F;code&gt;. For each &lt;code&gt;BTreeSet&lt;&#x2F;code&gt;, a &lt;code&gt;TreeFile&lt;&#x2F;code&gt; is modified
using a &lt;code&gt;CompareSwap&lt;&#x2F;code&gt; that inserts the key if a value isn&#x27;t already stored or
removes the key if a value is already stored. The fuzzer...&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;xkcd.com&#x2F;1745&#x2F;&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;imgs.xkcd.com&#x2F;comics&#x2F;record_scratch.png&quot; alt=&quot;xkcd #1745 &amp;quot;Record Scratch&amp;quot;&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Hey, who would have guessed it? Fuzzing works! As I was writing the last
paragraph, the fuzzer found that my fix in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;releases&#x2F;tag&#x2F;v0.5.1&quot;&gt;v0.5.1&lt;&#x2F;a&gt; was incomplete. I
couldn&#x27;t bring myself to continue writing the post and shifted to working on
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;commit&#x2F;70ac91e32835117d594441e103c239ddef7a6ff6&quot;&gt;a fix&lt;&#x2F;a&gt;. I held off on releasing &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;releases&#x2F;tag&#x2F;v0.5.2&quot;&gt;v0.5.2&lt;&#x2F;a&gt; until last
night, which allowed me to continue running fuzzing for 6 more hours after
fixing the bug. It&#x27;s been running continuously overnight and up to the point
of publishing this post without further incident.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The fuzzer tries over and over to break my test by sending in different values
for the &lt;code&gt;Vec&amp;lt;BTreeSet&amp;lt;u16&amp;gt;&amp;gt;&lt;&#x2F;code&gt;. Once it discovers a failure, it saves the
problematic input so that it can be repeated on-demand.&lt;&#x2F;p&gt;
&lt;p&gt;But that&#x27;s only part of what fuzzing can do for us. By running an &amp;quot;input
minimization&amp;quot; operation on the crashing input, the fuzzer will try to find the
smallest input that can reproduce the failure. It saves each of the discovered
failure cases so they can be tested independently if desired. Once it can no
longer find a smaller input, it reports the minimized test case.&lt;&#x2F;p&gt;
&lt;p&gt;I was feeling pretty confident in Nebari&#x27;s stability before Friday. My
confidence has been shaken, but as I write more fuzzing tests and run them for
extended periods of time, I&#x27;m hopeful I can weed out any remaining bugs and
restore my confidence.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Dogfooding%3A%20Shrinking%20Git%20Repository%20Sizes&quot; name=&quot;Dogfooding%3A%20Shrinking%20Git%20Repository%20Sizes&quot;&gt;
    Dogfooding: Shrinking Git Repository Sizes
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;At the start of the month, I really wanted to build something with BonsaiDb. One
problem BonsaiDb contributors have is related to my choice to use &lt;a href=&quot;https:&#x2F;&#x2F;pages.github.com&#x2F;&quot;&gt;GitHub
Pages&lt;&#x2F;a&gt; to build a deployed version of BonsaiDb&#x27;s
&lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;main&#x2F;docs&#x2F;bonsaidb&#x2F;&quot;&gt;documentation&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;main&#x2F;benchmarks&#x2F;suite&#x2F;report&#x2F;&quot;&gt;benchmark&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;main&#x2F;benchmarks&#x2F;commerce&#x2F;&quot;&gt;results&lt;&#x2F;a&gt;, and &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;main&#x2F;guide&#x2F;&quot;&gt;user&#x27;s guide&lt;&#x2F;a&gt; on every
commit. The way this works is that a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;cfe80d2b9d399c34b06cfab3dd1b34b148cf4f19&#x2F;.github&#x2F;workflows&#x2F;docs.yml&quot;&gt;workflow&lt;&#x2F;a&gt;
generates the static websites into a &lt;code&gt;gh-pages&lt;&#x2F;code&gt; branch, and GitHub deploys the
latest version of that branch automatically.&lt;&#x2F;p&gt;
&lt;p&gt;This comes at a cost, however. If you have cloned BonsaiDb&#x27;s repository, you
probably noticed that the repository is quite large. Using Git, we can inspect
how much disk space the &lt;code&gt;main&lt;&#x2F;code&gt; branch takes up compared to the &lt;code&gt;gh-pages&lt;&#x2F;code&gt;
branch:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;# Total size of commits reachable from `main`
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;git&lt;&#x2F;span&gt;&lt;span&gt; rev-list&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt; --disk-usage --objects&lt;&#x2F;span&gt;&lt;span&gt;  refs&#x2F;heads&#x2F;main
&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;,302,211&lt;&#x2F;span&gt;&lt;span&gt; (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;~&lt;&#x2F;span&gt;&lt;span&gt;4.3mb)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;# Total size of commits not reachable from `main`.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;git&lt;&#x2F;span&gt;&lt;span&gt; rev-list&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt; --disk-usage --objects --all --not&lt;&#x2F;span&gt;&lt;span&gt; refs&#x2F;heads&#x2F;main
&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;,215,042,793&lt;&#x2F;span&gt;&lt;span&gt; (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;~&lt;&#x2F;span&gt;&lt;span&gt;3.2gb)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;By moving &lt;code&gt;gh-pages&lt;&#x2F;code&gt; hosting somewhere else, I can reduce BonsaiDb&#x27;s repository
size down to a few megabytes instead of a few gigabytes. I&#x27;m aware there are
other approaches that can be taken to alleviate this problem, but when paired
with a request to add support for &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;222&quot;&gt;file storage&lt;&#x2F;a&gt;, I saw it as an
opportunity to &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eating_your_own_dog_food&quot;&gt;dogfood&lt;&#x2F;a&gt; BonsaiDb.&lt;&#x2F;p&gt;
&lt;p&gt;On Friday, I deployed &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;dossier&quot;&gt;Dossier&lt;&#x2F;a&gt; to &lt;a href=&quot;https:&#x2F;&#x2F;khonsu.dev&#x2F;dossier&#x2F;main&#x2F;docs&#x2F;dossier&#x2F;&quot;&gt;khonsu.dev&lt;&#x2F;a&gt;. I loaded
all of the BonsaiDb &lt;code&gt;gh-pages&lt;&#x2F;code&gt; files, but then encountered a slower sync time
than I had hoped for. I had an idea for &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;commit&#x2F;cfe80d2b9d399c34b06cfab3dd1b34b148cf4f19&quot;&gt;an optimization&lt;&#x2F;a&gt; for
listing files in &lt;code&gt;bonsaidb-files&lt;&#x2F;code&gt; and proceeded to update the View used for that
operation and incremented its version number. This informs BonsaiDb it should
discard all of the previously indexed data and reindex that view. While testing
this locally against a database that had been used extensively for days and had
over 100,000 files in it, I encountered the Nebari bug covered above. This
caused me to pause progress on Dossier, as I will always prioritize any bugs
relating to data integrity.&lt;&#x2F;p&gt;
&lt;p&gt;I was hoping this blog post would announce that all of BonsaiDb&#x27;s static pages,
including this blog, were hosted using Dossier, but I haven&#x27;t quite reached that
milestone. I&#x27;m hopeful that will be true within the next week, however!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#Easier%20Administration%20of%20BonsaiDb&quot; name=&quot;Easier%20Administration%20of%20BonsaiDb&quot;&gt;
    Easier Administration of BonsaiDb
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Another challenge I faced in building Dossier is that I needed a way to
administer permissions. One morning I had the thought: &amp;quot;It&#x27;d be great if I had a
tool like &lt;code&gt;psql&lt;&#x2F;code&gt;.&amp;quot; For those unfamiliar, &lt;code&gt;psql&lt;&#x2F;code&gt; is a simple command line
interface that allows you to execute commands on a PostgreSQL database. Of
course, most databases have SQL, and BonsaiDb has no language.&lt;&#x2F;p&gt;
&lt;p&gt;Previously, I had considered the extensible &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;main&#x2F;examples&#x2F;basic-server&#x2F;examples&#x2F;cli.rs&quot;&gt;command line
interface&lt;&#x2F;a&gt; to be a good approach, but as I added authentication
support, I realized that every operation was going to re-authenticate. One
advantage a tool like &lt;code&gt;pqsl&lt;&#x2F;code&gt; has is that it authenticates once upon launch and
then reuses the same connection for subsequent commands.&lt;&#x2F;p&gt;
&lt;p&gt;Because developing programming languages is a hobby of mine, one day I whipped
up a demo exploring the idea of a language that worked both as a proc macro and
as a string:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; results = bql!( &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; my_new_user = user &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;ecton&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; create with password &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;hunter2&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; )
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;execute_on&lt;&#x2F;span&gt;&lt;span&gt;(&amp;amp;storage)
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span&gt;println!(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;Created user id: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{:?}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, results.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;get&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;my_new_user&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;());
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; results = Program::parse(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;r&lt;&#x2F;span&gt;&lt;span&gt;#&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;let jon = user &amp;quot;jon&amp;quot; create;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;#)
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;execute_on&lt;&#x2F;span&gt;&lt;span&gt;(&amp;amp;storage)
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span&gt;println!(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;Created user id: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{:?}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, results.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;get&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;jon&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;());
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I was envisioning allowing this language to be extended. For example, the
&lt;code&gt;bonsaidb-files&lt;&#x2F;code&gt; crate should be able to define its own commands. This creates
an interesting problem that I decided wasn&#x27;t worth tackling: proc-macros cannot
invoke code in types that are outside of its own list of dependencies.&lt;&#x2F;p&gt;
&lt;p&gt;In the end, I decided that the use case for a language for use within Rust would
be unlikely to surpass a well designed API&#x27;s fluidity. I put aside this
experiment for now, and for Dossier, I&#x27;ve continued &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;commit&#x2F;48893ef033b1cbe40392021ce92130077291f680&quot;&gt;to
expand&lt;&#x2F;a&gt; BonsaiDb&#x27;s built-in command line interface.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#Open%20File%20Limits&quot; name=&quot;Open%20File%20Limits&quot;&gt;
    Open File Limits
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Earlier in the month, I was testing something on my Mac laptop, and I decided to
invoke the entire test suite with all features enabled on that machine. I ran
into an error due to having too many open files. I &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;issues&#x2F;8&quot;&gt;expected&lt;&#x2F;a&gt;
to eventually run into this, and I was proud that my test suite was the first I
heard of it from a user. One might ask, why does the test suite using so many
files that it&#x27;s a problem on that machine?&lt;&#x2F;p&gt;
&lt;p&gt;Most of the unit tests in BonsaiDb are written against the core traits like
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.Connection.html&quot;&gt;&lt;code&gt;Connection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.StorageConnection.html&quot;&gt;&lt;code&gt;StorageConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. The
&lt;code&gt;core-suite&lt;&#x2F;code&gt; integration test uses &lt;code&gt;bonsaidb-client&lt;&#x2F;code&gt; to run the test suite
against a shared &lt;code&gt;bonsaidb-server&lt;&#x2F;code&gt; instance. Each unit test for each connection
type gets its own database. The schema most tests utilize has multiple views,
each which uses a few files to track their state.&lt;&#x2F;p&gt;
&lt;p&gt;But still, with roughly 100 unit tests in the suite and two connection types, that
is only a few hundred files. It turns out that Mac OS has a default per-process
limit of 256 open files.&lt;&#x2F;p&gt;
&lt;p&gt;In production, this is easily worked around by configuring the server&#x27;s
environment, so this issue wasn&#x27;t a huge priority to me. But my mind started
exploring what sort of data structure I might need to solve this problem. I
wanted to use an &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cache_replacement_policies#Least_recently_used_%28LRU%29&quot;&gt;LRU&lt;&#x2F;a&gt; thought an LRU cache that worked with a &lt;code&gt;BTreeMap&lt;&#x2F;code&gt; instead of a &lt;code&gt;HashMap&lt;&#x2F;code&gt;. Alas,
none of the existing crates seemed to offer this functionality. I whipped up &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;lrumap&quot;&gt;my
own LRU crate&lt;&#x2F;a&gt; (awaiting publication).&lt;&#x2F;p&gt;
&lt;p&gt;Despite being 100% safe, it is very similar in performance compared to the
unsafe crate Nebari currently uses.&lt;&#x2F;p&gt;
&lt;p&gt;I attempted a few approaches in Nebari, but it&#x27;s a tough problem to solve. As
this work was dragging on, I really wanted to stop chasing this squirrel and
make progress on my original goal of the month. I will be finishing up this
problem in the next month or two. Until then, if you run into this issue, you
can configure your system to increase the maximum open files for your process,
user, or system.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#RFC%3A%20Token%20Authentication&quot; name=&quot;RFC%3A%20Token%20Authentication&quot;&gt;
    RFC: Token Authentication
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve written up &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;discussions&#x2F;248&quot;&gt;a request for comments discussion&lt;&#x2F;a&gt; about
how I&#x27;ve implemented token authentication for BonsaiDb. I&#x27;m dogfooding token
authentication within Dossier by using it to power API Tokens that each GitHub
Repository can have to upload files. The Dossier server uses BonsaiDb&#x27;s built-in
Role-Based Access Control to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;dossier&#x2F;blob&#x2F;7b5d1d393cab65cef633ec36b0ab7a7a37f2aacf&#x2F;src&#x2F;api.rs#L233-L236&quot;&gt;verify permissions&lt;&#x2F;a&gt; when
synchronizing files.&lt;&#x2F;p&gt;
&lt;p&gt;Since I&#x27;ve created a new approach and am using a newer algorithm
(&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;BLAKE3-team&#x2F;BLAKE3&quot;&gt;BLAKE3&lt;&#x2F;a&gt;), I wanted to see if there was any inherent problems with the
approach I&#x27;m taking. Unless I hear from someone who is a cryptographer, I&#x27;m
likely to document this feature as &amp;quot;experimental&amp;quot; for the foreseeable future.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#What%27s%20next%3F&quot; name=&quot;What%27s%20next%3F&quot;&gt;
    What&amp;#x27;s next?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;m going to be continuing to expand the fuzzing suite for Nebari to try to
ensure it&#x27;s rock-solid. After that, I want to finish moving most of Khonsu Labs&#x27;
self-hosted static pages onto Dossier. Once that is all done, I expect I will
feel ready to release the next update to BonsaiDb.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-7&quot;&gt;


&lt;a href=&quot;#Getting%20Started&quot; name=&quot;Getting%20Started&quot;&gt;
    Getting Started
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of
examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;&quot;&gt;user&#x27;s
guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good
documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We would love to hear from you if you have questions or feedback. We have &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an issue&lt;&#x2F;a&gt; with any questions or feedback.&lt;&#x2F;p&gt;
&lt;p&gt;We dream big with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, and we believe that it can simplify writing and deploying complex, data-driven applications in Rust. We would love &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;labels&#x2F;good%20first%20issue&quot;&gt;additional contributors&lt;&#x2F;a&gt; who have similar passions and ambitions.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear about it. Nothing makes us happier than hearing people are building things with our crates!&lt;&#x2F;p&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;1&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;1&lt;&#x2F;sup&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;xkcd.com&#x2F;1745&#x2F;&quot;&gt;&amp;quot;Record Scratch&amp;quot;&lt;&#x2F;a&gt; by &lt;a href=&quot;https:&#x2F;&#x2F;xkcd.com&quot;&gt;xkcd&lt;&#x2F;a&gt; is licensed under &lt;a href=&quot;https:&#x2F;&#x2F;creativecommons.org&#x2F;licenses&#x2F;by-nc&#x2F;2.5&#x2F;&quot;&gt;CC BY-NC 2.5&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>BonsaiDb v0.4.0: Now available without async</title>
		<published>2022-03-29T00:00:00+00:00</published>
		<updated>2022-03-29T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/bonsaidb-v0-4-0/" type="text/html"/>
		<id>https://bonsaidb.io/blog/bonsaidb-v0-4-0/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;BonsaiDb &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.4.0&quot;&gt;v0.4.0&lt;&#x2F;a&gt; has been released with a new blocking (non-async)
API, better identity&#x2F;authentication session management, and many other
improvements. The full list of changes can be viewed &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.4.0&quot;&gt;on the GitHub Release
page&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;For readers who enjoy the &amp;quot;This month in BonsaiDb&amp;quot; updates, this release
announcement takes the place of that post this month.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Updating%20existing%20projects&quot; name=&quot;Updating%20existing%20projects&quot;&gt;
    Updating existing projects
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This update contains no changes to how data is stored. However, there are a
large number of types that have been renamed to distinguish async types from
blocking types.&lt;&#x2F;p&gt;
&lt;p&gt;If you have an existing project and want to continue using async, here are the
types that you need to find and replace with their async counterparts in your
project:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Crate&lt;&#x2F;th&gt;&lt;th&gt;Blocking Type&lt;&#x2F;th&gt;&lt;th&gt;Async Type&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Core&lt;&#x2F;td&gt;&lt;td&gt;Connection&lt;&#x2F;td&gt;&lt;td&gt;AsyncConnection&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Core&lt;&#x2F;td&gt;&lt;td&gt;StorageConnection&lt;&#x2F;td&gt;&lt;td&gt;AsyncStorageConnection&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Core&lt;&#x2F;td&gt;&lt;td&gt;PubSub&lt;&#x2F;td&gt;&lt;td&gt;AsyncPubSub&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Core&lt;&#x2F;td&gt;&lt;td&gt;Subscriber&lt;&#x2F;td&gt;&lt;td&gt;AsyncSubscriber&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Core&lt;&#x2F;td&gt;&lt;td&gt;KeyValue&lt;&#x2F;td&gt;&lt;td&gt;AsyncKeyValue&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Core&lt;&#x2F;td&gt;&lt;td&gt;LowLevelConnection&lt;&#x2F;td&gt;&lt;td&gt;AsyncLowLevelConnection&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Local&lt;&#x2F;td&gt;&lt;td&gt;Storage&lt;&#x2F;td&gt;&lt;td&gt;AsyncStorage&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Local&lt;&#x2F;td&gt;&lt;td&gt;Database&lt;&#x2F;td&gt;&lt;td&gt;AsyncDatabase&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;After changing these types, you might encounter errors with functions like
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;schema&#x2F;trait.SerializedCollection.html#method.push_into&quot;&gt;&lt;code&gt;SerializedCollection::push_into&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; or &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;schema&#x2F;trait.SerializedCollection.html#method.list&quot;&gt;&lt;code&gt;SerializedCollection::list&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. In every
situation that this happens, you should be able to simply add &lt;code&gt;_async&lt;&#x2F;code&gt; to the
function name. For example, &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;schema&#x2F;trait.SerializedCollection.html#method.push_into_async&quot;&gt;&lt;code&gt;SerializedCollection::push_into_async&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; is the async
version of &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;schema&#x2F;trait.SerializedCollection.html#method.push_into&quot;&gt;&lt;code&gt;SerializedCollection::push_into&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;If you are using the local version of BonsaiDb, Tokio has become an optional
dependency. To enable async, enable the appropriate feature for whichever crate
you&#x27;re importing:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;toml&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-toml &quot;&gt;&lt;code class=&quot;language-toml&quot; data-lang=&quot;toml&quot;&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;bonsaidb &lt;&#x2F;span&gt;&lt;span&gt;= { &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;version &lt;&#x2F;span&gt;&lt;span&gt;= &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;0.4.0&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;features &lt;&#x2F;span&gt;&lt;span&gt;= [&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;local&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;local-async&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;] }
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;bonsaidb-local &lt;&#x2F;span&gt;&lt;span&gt;= { &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;version &lt;&#x2F;span&gt;&lt;span&gt;= &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;0.4.0&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;default-features &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;false&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;features &lt;&#x2F;span&gt;&lt;span&gt;= [&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;async&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;] }
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There are other small breaking changes which are unlikely to affect most users.
The full list can be &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;v0.4.0&#x2F;CHANGELOG.md&quot;&gt;viewed in the CHANGELOG&lt;&#x2F;a&gt; or &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.4.0&quot;&gt;release
page&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;For those it might help, I&#x27;ve updated an &lt;a href=&quot;https:&#x2F;&#x2F;minority-game.gooey.rs&#x2F;&quot;&gt;example project&lt;&#x2F;a&gt; of
mine from v0.2 to this new release. The changes can be viewed in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;minority-game&#x2F;commit&#x2F;c7ae081a0c37dc359f4c6e0bc2f821dc5a73f8aa&quot;&gt;this
commit&lt;&#x2F;a&gt;. This project features a client&#x2F;server workspace
and uses BonsaiDb&#x27;s custom API functionality.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#Why%20introduce%20a%20blocking%20API%3F&quot; name=&quot;Why%20introduce%20a%20blocking%20API%3F&quot;&gt;
    Why introduce a blocking API?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;BonsaiDb has always wrapped &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;&#x27;s blocking API with
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;tokio&#x2F;latest&#x2F;tokio&#x2F;task&#x2F;fn.spawn_blocking.html&quot;&gt;tokio::task::spawn_blocking&lt;&#x2F;a&gt; and exposed an async API. The
reason behind this initial design was that the local-only version of BonsaiDb
was a convenience feature, and the &amp;quot;real&amp;quot; benefit of projects adopting BonsaiDb
was to use its server mode (and eventually clustering mode). I believe strongly
that async is the best way to design a server that has long-lasting connections,
and the types of applications I envision being developed with BonsaiDb fit that
access model: Remote PubSub, Server-Push Apis, and more.&lt;&#x2F;p&gt;
&lt;p&gt;One day I was joking that a friend wouldn&#x27;t want to use BonsaiDb for his project
because it would force him to use async. While he proceeded to say that async
wouldn&#x27;t stop him, the exchange made me take stock of the current users of
BonsaiDb. Of all the people who I know are using BonsaiDb, most were only using
the local version and many weren&#x27;t building networked applications.&lt;&#x2F;p&gt;
&lt;p&gt;Case in point, one user was painstakingly porting their
project to async from sync to be able to use BonsaiDb. Thankfully, this set of
changes were already underway, and they were able to be the first external
tester of the new blocking API.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#How%20does%20BonsaiDb%20enable%20both%20blocking%20and%20async%20access%3F&quot; name=&quot;How%20does%20BonsaiDb%20enable%20both%20blocking%20and%20async%20access%3F&quot;&gt;
    How does BonsaiDb enable both blocking and async access?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;For the the local implementation, the most direct implementation is the blocking
implementation. This means that there will be less overhead on local
&amp;quot;connections&amp;quot; using the non-async interface. The overhead of the async types is
very low, but it is
present, and it is the same overhead that has always been present on BonsaiDb&#x27;s
interface.&lt;&#x2F;p&gt;
&lt;p&gt;The local implementation exposes &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;local&#x2F;struct.Storage.html&quot;&gt;&lt;code&gt;Storage&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; and
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;local&#x2F;struct.Database.html&quot;&gt;&lt;code&gt;Database&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, which implement
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.StorageConnection.html&quot;&gt;&lt;code&gt;StorageConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.Connection.html&quot;&gt;&lt;code&gt;Connection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;
respectively. These types are the blocking types.
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;local&#x2F;struct.AsyncStorage.html&quot;&gt;&lt;code&gt;AsyncStorage&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;local&#x2F;struct.AsyncDatabase.html&quot;&gt;&lt;code&gt;AsyncDatabase&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; implement
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.AsyncStorageConnection.html&quot;&gt;&lt;code&gt;AsyncStorageConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; and
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.AsyncConnection.html&quot;&gt;&lt;code&gt;AsyncConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. These types are the async types that
internally store a handle to the Tokio runtime they belong to. This grants these
types the ability to spawn blocking tasks as needed, even when used in
async code that is using another async runtime.&lt;&#x2F;p&gt;
&lt;p&gt;All local types offer several convenient functions that allow seamless
conversion to and from each other, enabling applications that use both blocking
and async to use BonsaiDb as their database with ease.&lt;&#x2F;p&gt;
&lt;p&gt;The server implementation currently only exposes an initialization API that is
compatible with async code. Once the server is initialized, however, a
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;server&#x2F;type.Server.html&quot;&gt;&lt;code&gt;Server&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;&#x2F;&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;server&#x2F;struct.CustomServer.html&quot;&gt;&lt;code&gt;CustomServer&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; instance can be converted to
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;local&#x2F;struct.Storage.html&quot;&gt;&lt;code&gt;Storage&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, enabling blocking access to the underlying storage layer.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;client&#x2F;struct.Client.html&quot;&gt;&lt;code&gt;Client&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; type used for accessing a server implements both
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.StorageConnection.html&quot;&gt;&lt;code&gt;StorageConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.AsyncStorageConnection.html&quot;&gt;&lt;code&gt;AsyncStorageConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, except for on WASM where no
blocking implementation is provided. Currently, all networking implementations
require Tokio to drive their networking behind the scenes, but in the future
alternative implementations are possible to remove the reliance on Tokio for the
network client.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#Improved%20authenticated%20session%20handling&quot; name=&quot;Improved%20authenticated%20session%20handling&quot;&gt;
    Improved authenticated session handling
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Prior to this release, there was no way to enforce database permissions in the
offline version of BonsaiDb. If you are building a multi-user app but are using
your own web server, how are you supposed to be able to leverage BonsaiDb&#x27;s
permissions and authentication?&lt;&#x2F;p&gt;
&lt;p&gt;This release addresses this problem by moving all permission checking into the
offline version and adding &lt;code&gt;assume_identity()&lt;&#x2F;code&gt; to
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.StorageConnection.html&quot;&gt;&lt;code&gt;StorageConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;&#x2F;&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.AsyncStorageConnection.html&quot;&gt;&lt;code&gt;AsyncStorageConnection&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. Assuming an identity requires the
permission to be allowed by the current connection. These changes allow all
applications that need multi-user support to use the built-in role-based access
control and, optionally, the built-in password authentication.&lt;&#x2F;p&gt;
&lt;p&gt;Each connection object now has an associated &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;struct.Session.html&quot;&gt;&lt;code&gt;Session&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, available
through the new &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.HasSession.html&quot;&gt;&lt;code&gt;HasSession&lt;&#x2F;code&gt; trait&lt;&#x2F;a&gt;. The functions
&lt;code&gt;assume_identity()&lt;&#x2F;code&gt; and &lt;code&gt;authenticate()&lt;&#x2F;code&gt; return a new instance that is
authenticated with the updated identity and associated permissions. The original
connection retains its original authentication. This allows multiple
authentication sessions to coexist on the same underlying connection, regardless
of if it&#x27;s a local or remote connection.&lt;&#x2F;p&gt;
&lt;p&gt;When opening a local storage&#x2F;database&#x2F;server, the connection begins with no
identity and restricted permissions. When connecting to a remote server, the
client&#x27;s unauthenticated session is limited to the permissions from
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;server&#x2F;struct.ServerConfiguration.html#structfield.default_permissions&quot;&gt;&lt;code&gt;ServerConfiguration::default_permissions&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. After
authenticating a user or assuming an identity,
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;local&#x2F;config&#x2F;struct.StorageConfiguration.html#structfield.authenticated_permissions&quot;&gt;&lt;code&gt;StorageConfiguration::authenticated_permissions&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;
will be merged with the identity&#x27;s granted permissions.&lt;&#x2F;p&gt;
&lt;p&gt;I hope to see users trying out BonsaiDb&#x27;s built-in user management and
role-based access control!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#Primary%20Key&amp;#x2F;View%20Key%20improvements&quot; name=&quot;Primary%20Key&amp;#x2F;View%20Key%20improvements&quot;&gt;
    Primary Key&amp;#x2F;View Key improvements
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;traits&#x2F;key.html&quot;&gt;&lt;code&gt;Key&lt;&#x2F;code&gt; trait&lt;&#x2F;a&gt; has been split into two traits: &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;trait.Key.html&quot;&gt;&lt;code&gt;Key&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;
and &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;trait.KeyEncoding.html&quot;&gt;&lt;code&gt;KeyEncoding&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. This split has enabled various queries to
take borrowed representations of the keys. For example, a &lt;code&gt;String&lt;&#x2F;code&gt; key type can
now be queried using an &lt;code&gt;&amp;amp;str&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; doc = MyCollection::get(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;my key&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, &amp;amp;db)?;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; mappings = db.view::&amp;lt;MyView&amp;gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;with_key&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;my key&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;query&lt;&#x2F;span&gt;&lt;span&gt;()?;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Additional types now have &lt;code&gt;Key&lt;&#x2F;code&gt; implementations provided for them: &lt;code&gt;[u8; N]&lt;&#x2F;code&gt;,
&lt;code&gt;SystemTime&lt;&#x2F;code&gt; and &lt;code&gt;Duration&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#Using%20Time%20as%20a%20Key&quot; name=&quot;Using%20Time%20as%20a%20Key&quot;&gt;
    Using Time as a Key
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Both &lt;code&gt;SystemTime&lt;&#x2F;code&gt; and &lt;code&gt;Duration&lt;&#x2F;code&gt; are stored with the full range that &lt;code&gt;Duration&lt;&#x2F;code&gt;
supports. In memory, these types require 12 bytes to represent, and even with
variable integer encoding, the values encoded as keys can still be fairly long.&lt;&#x2F;p&gt;
&lt;p&gt;Most users don&#x27;t need nanosecond precision with the range of 42 times the age of
the universe. After a fun coworking session &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&#x2F;&quot;&gt;on our Discord server&lt;&#x2F;a&gt;, I
introduced &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;time&#x2F;limited&#x2F;struct.LimitedResolutionDuration.html&quot;&gt;&lt;code&gt;LimitedResolutionDuration&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; and
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;time&#x2F;limited&#x2F;struct.LimitedResolutionTimestamp.html&quot;&gt;&lt;code&gt;LimitedResolutionTimestamp&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. These types allow representing durations and
timestamps using smaller in-memory representations, and offer additional
compression using variable integer encoding and customizable epochs.&lt;&#x2F;p&gt;
&lt;p&gt;To see an example of the benefits, compare timestamps encoded over the span of
the next 40 years and their &lt;code&gt;Key&lt;&#x2F;code&gt;-encoded lengths:&lt;&#x2F;p&gt;
&lt;img src=&quot;&amp;#x2F;images&amp;#x2F;timestamp-key-encoding-chart.png&quot; class=&quot;block-image&quot; alt=&quot;Timestamp Encoding Lengths (bytes)&quot;   &#x2F;&gt;
&lt;p&gt;As you can see, &lt;code&gt;SystemTime&lt;&#x2F;code&gt; encodes as 9 bytes (and will continue to do so
until year 10,680). &lt;code&gt;SystemTime&lt;&#x2F;code&gt;&#x27;s &lt;code&gt;Key&lt;&#x2F;code&gt; implementation encodes the duration
from the unix epoch, so this means that Durations that of 52 years with
nanosecond precision take 9 bytes to encode. The smaller the Duration being
encoded, the smaller the output will be.&lt;&#x2F;p&gt;
&lt;p&gt;By using one millisecond resolution, we can reduce the number of bytes down to 6
and also use only 8 bytes in memory to represent the value. By using one second
resolution, we can further reduce the key size by another byte.&lt;&#x2F;p&gt;
&lt;p&gt;How can we compress timestamps further? By leveraging an Epoch that is closer to
the median of the expected timestamp&#x2F;duration range, we can allow the variable
integer encoding to shine. The closer to the Epoch the timestamp is, the smaller
the value is.&lt;&#x2F;p&gt;
&lt;p&gt;In the above chart, the two values that dip in the middle use
&lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;time&#x2F;limited&#x2F;struct.BonsaiEpoch.html&quot;&gt;&lt;code&gt;BonsaiEpoch&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;. After considering that databases either live long
enough to be used for almost an eternity (PostgreSQL was first released in the
1980s) or live a short life. I thought the 10-year anniversary of the first
commit of BonsaiDb seemed like a good idea.&lt;&#x2F;p&gt;
&lt;p&gt;Despite it being an arbitrary decision, further consideration showed that epoch
is good for one reason: Nanosecond-resolution timestamps encoded today will
encode as 8 bytes, and will continue doing so until year 2,049. If an epoch
slightly farther out were chosen, today&#x27;s timestamps would encode with a full 9
bytes, offering no benefit over the unix epoch version. This extra byte of
savings isn&#x27;t much, but it does make the key size word size or less on most
modern machines.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve added a &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;time&#x2F;index.html#types&quot;&gt;full set of types&lt;&#x2F;a&gt; that support resolutions ranging
from nanoseconds to weeks, and timestamp types that support both the BonsaiDb
and unix epochs. You can also implement your &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;time&#x2F;limited&#x2F;trait.TimeResolution.html&quot;&gt;own resolution&lt;&#x2F;a&gt;
and&#x2F;or &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;time&#x2F;limited&#x2F;trait.TimeEpoch.html&quot;&gt;epoch&lt;&#x2F;a&gt; to tailor fit how these new types work.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#Other%20contributors%20this%20release&quot; name=&quot;Other%20contributors%20this%20release&quot;&gt;
    Other contributors this release
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I wanted to make sure to include a mention for &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;vbmade2000&quot;&gt;@vbmade2000&lt;&#x2F;a&gt;&#x27;s
contributions to this release. They added the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;215&quot;&gt;ability to delete users&lt;&#x2F;a&gt;,
and the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;221&quot;&gt;ability to retrieve&lt;&#x2F;a&gt; a list of document headers without fetching
the document&#x27;s contents.&lt;&#x2F;p&gt;
&lt;p&gt;If you are interested in contributing to BonsaiDb, I am trying to keep a healthy
list of &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues?q=is%3Aopen+label%3A%22good+first+issue%22+sort%3Aupdated-desc&quot;&gt;good first issues&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-7&quot;&gt;


&lt;a href=&quot;#Getting%20Started&quot; name=&quot;Getting%20Started&quot;&gt;
    Getting Started
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of
examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;&quot;&gt;user&#x27;s
guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good
documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We would love to hear from you if you have questions or feedback. We have &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an issue&lt;&#x2F;a&gt; with any questions or feedback.&lt;&#x2F;p&gt;
&lt;p&gt;We dream big with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, and we believe that it can simplify writing and deploying complex, data-driven applications in Rust. We would love &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;labels&#x2F;good%20first%20issue&quot;&gt;additional contributors&lt;&#x2F;a&gt; who have similar passions and ambitions.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear about it. Nothing makes us happier than hearing people are building things with our crates!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>A Year of BonsaiDb: A retrospective and looking to the future</title>
		<published>2022-03-19T00:00:00+00:00</published>
		<updated>2022-03-19T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/one-year-anniversary/" type="text/html"/>
		<id>https://bonsaidb.io/blog/one-year-anniversary/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Today marks the one year anniversary of the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;commit&#x2F;43bd3a25b61fc7841c9554422d7bb46ad4362e59&quot;&gt;initial commit&lt;&#x2F;a&gt; to
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;. This project is largely a written by &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ecton&quot;&gt;me (@Ecton)&lt;&#x2F;a&gt;,
although I&#x27;m very thankful to have been accompanied along the way by
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;daxpedda&quot;&gt;@daxpedda&lt;&#x2F;a&gt;. He is responsible for the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;fabruic&quot;&gt;QUIC networking
layer&lt;&#x2F;a&gt;, our currently removed OPAQUE-KE support, and above all,
countless long discussions and debates that not only led to starting this
project, but also to the easy-to-use API designs we have today.&lt;&#x2F;p&gt;
&lt;p&gt;In the last few months, we&#x27;ve also had two additional contributors:
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;modprog&quot;&gt;@ModProg&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;vbmade2000&quot;&gt;@vbmade2000&lt;&#x2F;a&gt;. I&#x27;m grateful for their
additions to the project, and I hope this year we see even more contributors as
BonsaiDb gains momentum.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Beginning%20a%20real%20bonsai%20cluster&quot; name=&quot;Beginning%20a%20real%20bonsai%20cluster&quot;&gt;
    Beginning a real bonsai cluster
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The name BonsaiDb was suggested by &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;daxpedda&quot;&gt;@daxpedda&lt;&#x2F;a&gt; after countless times
where we stumbled trying to pronounce the project&#x27;s former name: PliantDb. When
I &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;short-update-pliantdb-is-now-known-as-bonsaidb&#x2F;74&quot;&gt;announced the new name&lt;&#x2F;a&gt;, I researched trying to grow a bonsai
tree in the Las Vegas climate. My initial findings were that many of the iconic
types are difficult to grow here, and I ended up shelving the idea.&lt;&#x2F;p&gt;
&lt;p&gt;As March was approaching, I knew I wanted to commemorate the anniversary of the
first commit. I researched again and found two species that were great
candidates for being able to be grown mostly indoors. After some deliberation, I
decided to start with a Golden Gate (Tiger Bark) Ficus:&lt;&#x2F;p&gt;
&lt;img src=&quot;&amp;#x2F;images&amp;#x2F;one-year-anniversary-ficus.jpg&quot; class=&quot;block-image&quot; alt=&quot;One Year Anniversary Golden Gate Ficus&quot;  width=&quot;400&quot;  &#x2F;&gt;
&lt;p&gt;After giving the tree another week or two of adjustment to its new home, I&#x27;ll be
pruning it for the first time and attempting to grow a second tree from its
cuttings. Within the limits of being able to provide good living conditions for
each tree, I hope to expand the &amp;quot;cluster&amp;quot; each year and will share progress
pictures along the way!&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#The%20first%20year%3A%20Inspiration&quot; name=&quot;The%20first%20year%3A%20Inspiration&quot;&gt;
    The first year: Inspiration
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The inspiration for &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; came from weeks and weeks of
discussions between &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ecton&quot;&gt;myself&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;daxpedda&quot;&gt;@daxpedda&lt;&#x2F;a&gt;. We were wanting
to build an cluster architecture for an MMO that:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Was easy to deploy and maintain&lt;&#x2F;li&gt;
&lt;li&gt;Was affodable&lt;&#x2F;li&gt;
&lt;li&gt;Could scale horizontally&lt;&#x2F;li&gt;
&lt;li&gt;Could subdivide players by &amp;quot;location&amp;quot; within the cluster:
&lt;ul&gt;
&lt;li&gt;Each location could have database&#x2F;cache&#x2F;cpu resources allocated dynamically&lt;&#x2F;li&gt;
&lt;li&gt;Each location would be Highly Available&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Used as many Rust-written solutions as possible&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;At the start of our discussions, I was dead-set on using PostgreSQL and Redis
for our database&#x2F;caching needs. However both PostgreSQL and Redis aren&#x27;t easy to
deploy and maintain in a highly available cluster, and if I chose managed
solutions, they definitely weren&#x27;t affordable.&lt;&#x2F;p&gt;
&lt;p&gt;When starting my last company, I chose &lt;a href=&quot;https:&#x2F;&#x2F;couchdb.apache.org&#x2F;&quot;&gt;CouchDB&lt;&#x2F;a&gt; for various reasons
that aren&#x27;t worth diving into. Through the ten years of growing that company, I
became intimately familiar with the simple yet powerful design of CouchDB.
Several times I pondered rewriting CouchDB atop PostgreSQL using JSONB columns
to see how it would perform. I never ended up taking any of those prototypes
very far, but the thought process helped me appreciate and understand CouchDB&#x27;s
inner workings better.&lt;&#x2F;p&gt;
&lt;p&gt;One day while working on &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;cosmicverge&quot;&gt;Cosmic Verge&lt;&#x2F;a&gt;, it dawned on me: I had
built a small wrapper around &lt;a href=&quot;https:&#x2F;&#x2F;sled.rs&quot;&gt;Sled&lt;&#x2F;a&gt; that made it feel like a higher-level
database. What if I went further and built a CouchDB-like API atop Sled?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#The%20first%20year%3A%20Initial%20Goals&quot; name=&quot;The%20first%20year%3A%20Initial%20Goals&quot;&gt;
    The first year: Initial Goals
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Initially, I was hesitant to tell anyone I was building a database. Despite
relying on &lt;a href=&quot;https:&#x2F;&#x2F;sled.rs&quot;&gt;Sled&lt;&#x2F;a&gt; for the low-level transactional storage, it still felt
like one of those projects that is usually ill-advised. I finally felt confident
enough in my reasoning of &lt;em&gt;why&lt;&#x2F;em&gt; I was building &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; that I &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;did-i-say-i-was-refocusing&#x2F;58#pliantdb-the-core-of-cosmic-verge-2&quot;&gt;wrote a detailed
devlog&lt;&#x2F;a&gt; describing the motivation for tackling this new
project.&lt;&#x2F;p&gt;
&lt;p&gt;Early on, I never had high ambitions for BonsaiDb. After all, this was the first
database I had ever written. The likelihood of me building a high-performance
database seemed low. Instead, I wanted to focus primarily on developer
experience, reliability, and ease of deployment. I tried to make intelligent
choices based on my expectations of what would be efficient, but I intentionally
avoided comparing BonsaiDb against other databases until &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;blog&#x2F;commerce-benchmark&#x2F;&quot;&gt;January of this
year&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To me the selling points of BonsaiDb were (and still are):&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Straightforward to use and deploy.&lt;&#x2F;li&gt;
&lt;li&gt;Can write generic code that works regardless of how BonsaiDb is deployed.&lt;&#x2F;li&gt;
&lt;li&gt;&amp;quot;Battery-powered&amp;quot; with common needs for developers, reducing complexity of deployments:
&lt;ul&gt;
&lt;li&gt;ACID-compliant transactional document storage&lt;&#x2F;li&gt;
&lt;li&gt;Lightweight Atomic Key-Value store (a-la Redis)&lt;&#x2F;li&gt;
&lt;li&gt;PubSub&lt;&#x2F;li&gt;
&lt;li&gt;Unified and extensible authentication and permissions model&lt;&#x2F;li&gt;
&lt;li&gt;Persistent Job Queue System (a la SQS&#x2F;Resque&#x2F;Sidekiq). Work on this has been
started but is not shipping yet, but it has been a goal from the start of
the project.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Built in Rust, for Rust.&lt;&#x2F;li&gt;
&lt;li&gt;Vision includes a &amp;quot;grows with you&amp;quot; pitch:
&lt;ul&gt;
&lt;li&gt;Bi-directional replication&lt;&#x2F;li&gt;
&lt;li&gt;Quorum-based clustering&lt;&#x2F;li&gt;
&lt;li&gt;Modular plugin system to support &amp;quot;resuable&amp;quot; collections and services&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#BonsaiDb%20isn%27t%20just%20a%20toy&quot; name=&quot;BonsaiDb%20isn%27t%20just%20a%20toy&quot;&gt;
    BonsaiDb isn&amp;#x27;t just a toy
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In October a series of events made me look at porting the append-only B-Tree
implementation from CouchDB to Rust as a potential replacement for &lt;a href=&quot;https:&#x2F;&#x2F;sled.rs&quot;&gt;Sled&lt;&#x2F;a&gt;
in our architecture. In the end, the bug that I was experiencing on Mac &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;introducing-nebari-a-key-value-data-store-written-using-an-append-only-file-format&#x2F;81#what-caused-the-uncontrolled-memory-usage-5&quot;&gt;was
unrelated&lt;&#x2F;a&gt; to Sled, but its symptoms manifested in such a way that
resembled issues others have had with Sled in the past.&lt;&#x2F;p&gt;
&lt;p&gt;After getting some basic unit tests working, I did something really stupid: I
benchmarked it against Sled and SQLite. Why was it stupid? It kicked off a
viscious few weeks, nearly reaffirming the common wisdom: why was I trying to
write my own database? I would stare &lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;nebari-scaleway-gp1-xs&#x2F;index.html&quot;&gt;at microbenchmarks&lt;&#x2F;a&gt; and
profiling results, tweak something, and try again. Over and over. For some
reason it just wasn&#x27;t occurring to me: the fact &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt; was even in the
same realm as these highly optimized databases was exciting enough. Eventually,
I &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;introducing-nebari-a-key-value-data-store-written-using-an-append-only-file-format&#x2F;81#why-still-develop-nebari-6&quot;&gt;convinced myself&lt;&#x2F;a&gt; it was better for the future of BonsaiDb to
adopt Nebari.&lt;&#x2F;p&gt;
&lt;p&gt;In January I was preparing to release our first alpha. I knew if I didn&#x27;t have
benchmarks available, it would be one of the first questions asked. I wrote a
benchmark suite aimed at comparing BonsaiDb against PostgreSQL in a simple
eCommerce setup. I wasn&#x27;t prepared for the results: BonsaiDb is significantly
faster than PostgreSQL &lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;bonsaidb-scaleway-gp1-xs&#x2F;commerce&#x2F;index.html&quot;&gt;in this benchmark suite&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Last month I set my focus &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;blog&#x2F;february-2022-update&#x2F;#Testing%20BonsaiDb%20with%20a%20large%20dataset&quot;&gt;testing large datasets&lt;&#x2F;a&gt; and was able
to achieve performance again that exceeded my expectations. I no longer am
viewing BonsaiDb as an easy-to-use database that will be &amp;quot;good enough&amp;quot; for many
people. I now view BonsaiDb as having the potential to be a serious database
option for most people.&lt;&#x2F;p&gt;
&lt;p&gt;For being just one year old, I&#x27;m extremely happy with where BonsaiDb is today.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#Year%20Two%3A%20My%20Goals&quot; name=&quot;Year%20Two%3A%20My%20Goals&quot;&gt;
    Year Two: My Goals
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;One thing that surprised me was how much interest &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt; garnered.
Despite not heavily advertising it, it&#x27;s accumulated a fair number of stars on
GitHub. It&#x27;s popular enough that I&#x27;ve been rethinking how I was planning on
implementing certain features. Outside of bug fixes and minor features, I
haven&#x27;t spent much time working on Nebari as it has been meeting BonsaiDb&#x27;s
needs. I would like to spend more time in this coming year making Nebari a
better crate for developers to consume directly. Ultimately, the more users
Nebari has, the more confidence everyone can have in BonsaiDb&#x27;s ACID compliance.&lt;&#x2F;p&gt;
&lt;p&gt;For &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, there are several goals I have:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Features, Features, Features&lt;&#x2F;strong&gt;: I&#x27;m excited at exploring a wide variety of
features aiming to make BonsaiDb more powerful and easier to use:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;220&quot;&gt;Ability to use BonsaiDb in non-async
code&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;78&quot;&gt;Persistent Job Queue&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;162&quot;&gt;Supporting Metrics&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;136&quot;&gt;Document Dependencies&#x2F;Foreign Keys&#x2F;Graph Relationships&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;69&quot;&gt;Collection Lifecycles&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;90&quot;&gt;Replication&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;There are countless other features I hope to explore as well, but those are
some of the higher priority items as I view them.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stability&lt;&#x2F;strong&gt;: I would like to be able to declare a stable version within the
next year. I&#x27;m currently treating BonsaiDb as if it&#x27;s stable: trying to
preserve backwards compatibility for the storage format, prioritizing bug
fixes, and keeping a thorough changelog.&lt;&#x2F;p&gt;
&lt;p&gt;By this time next year, I would like to be at a point where the core of
BonsaiDb is changing infrequently, and focus is on building higher level
abstractions.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tooling&lt;&#x2F;strong&gt;: A good, graphical database administration tool not only makes
exploring a database more approachable, it also can aide in quickly diagnosing
and fixing data issues. Because BonsaiDb is architected to not know about the
contents of the documents being stored, creating a good, generic
administration tool will be a fun challenge.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community&lt;&#x2F;strong&gt;: Fostering an active developer community around BonsaiDb is
important for many reasons. As I mentioned before, the more users BonsaiDb
has, the more confident we can all feel about its reliability. Beyond that,
however, is something more fundamental for me: I thrive on success stories and
helping solve interesting data problems.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#Help%20grow%20BonsaiDb&quot; name=&quot;Help%20grow%20BonsaiDb&quot;&gt;
    Help grow BonsaiDb
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Building BonsaiDb has reawakened many of the passions I felt in my first
full-time computer-related job: working on REALbasic (now
&lt;a href=&quot;https:&#x2F;&#x2F;www.xojo.com&#x2F;&quot;&gt;Xojo&lt;&#x2F;a&gt;). As I started getting schema design questions and
feature requests from potential users, I reflected on the joy those interactions
provided me. It is clear to me that one of the reasons I look back so fondly at
my first programming job is the wide array of problems I helped users solve.&lt;&#x2F;p&gt;
&lt;p&gt;For the longest time, I held off on allowing any means of sponsoring development
of BonsaiDb. Until recently, I was still hoping to get back to writing an MMO
once BonsaiDb was mature enough. My dreams have changed in the past months: I
want to focus on bringing safe, high-level data-oriented application development
to Rust. Initially, that means getting BonsaiDb to a maturity level where it
meets my original vision.&lt;&#x2F;p&gt;
&lt;p&gt;Many readers may not be aware that the early days of BonsaiDb development were
overlapped with my development of &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;gooey&#x2F;&quot;&gt;Gooey&lt;&#x2F;a&gt;. To this day, it is the only
framework that meets my personal desires of application development. However,
it&#x27;s lacking many critical features&#x2F;widgets. I paused development because I want
to focus on BonsaiDb. Yet, I also want to begin writing GUI administration tools
for BonsaiDb soon.&lt;&#x2F;p&gt;
&lt;p&gt;I would love to continue dedicating my focus to these areas of the Rust ecosystem:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;BonsaiDb: Easy-to-use database that grows with you.&lt;&#x2F;li&gt;
&lt;li&gt;Building cross-platform applications in Rust (with or without Gooey)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If you would like to ensure my ability to continue working on these projects
full-time, I am now &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;sponsors&#x2F;ecton&quot;&gt;accepting sponsorship through GitHub
Sponsors&lt;&#x2F;a&gt;. I have been working on open-source Rust projects since
November 2019 all funded by the sale of my previous startup. I would love the
opportunity to continue working on open-source full time without needing to
focus on building another startup. As I am not in dire need of finances, please
only sponsor me if you have truly disposable income.&lt;&#x2F;p&gt;
&lt;p&gt;Another important way to help grow BonsaiDb is to contribute. I&#x27;ve been working
on adding &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;&amp;quot;Good First Issue&amp;quot;&lt;&#x2F;a&gt; tasks. If you&#x27;re looking to
contribute to an open-source Rust project, I would be honored to have you as
part of our team.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#Trying%20out%20BonsaiDb&quot; name=&quot;Trying%20out%20BonsaiDb&quot;&gt;
    Trying out BonsaiDb
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of
examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;&quot;&gt;user&#x27;s
guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good
documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;I would love to hear from you if you have questions or feedback. We have
&lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord
server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an
issue&lt;&#x2F;a&gt; with any questions or
feedback.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear
about it. Nothing makes us happier than hearing people are building things with
our crates!&lt;&#x2F;p&gt;
&lt;p&gt;Thank you to all of the wonderful people in the Khonsu Labs community and the
Rust ecosystem.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>BonsaiDb v0.3.0: Optimized views, bug fixes, and more</title>
		<published>2022-03-03T00:00:00+00:00</published>
		<updated>2022-03-03T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/bonsaidb-v0-3-0/" type="text/html"/>
		<id>https://bonsaidb.io/blog/bonsaidb-v0-3-0/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;BonsaiDb &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.3.0&quot;&gt;v0.3.0&lt;&#x2F;a&gt; has been released featuring optimized view
processing, bug fixes, and a few new features -- including one from a new
contributor!&lt;&#x2F;p&gt;
&lt;p&gt;Optimization of the view processing system has been covered in the &lt;a href=&quot;&#x2F;blog&#x2F;february-2022-update#Optimizing%20View%20Mapping&quot;&gt;previous
blog post&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Deleting%20users%20via%20StorageConnection&quot; name=&quot;Deleting%20users%20via%20StorageConnection&quot;&gt;
    Deleting users via StorageConnection
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;A new function, &lt;code&gt;delete_user()&lt;&#x2F;code&gt; has been added to &lt;code&gt;StorageConnection&lt;&#x2F;code&gt;, allowing
for users to not only be created but also deleted without direct database
access. This feature was added by &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;vbmade2000&quot;&gt;@vbmade2000&lt;&#x2F;a&gt; -- now the third
contributor to the BonsaiDb repository!&lt;&#x2F;p&gt;
&lt;p&gt;BonsaiDb provides open-access its &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;admin&#x2F;index.html&quot;&gt;admin schema types&lt;&#x2F;a&gt;. This meant
that users could still be deleted, but it required doing so directly through
&lt;code&gt;Storage::admin()&lt;&#x2F;code&gt;, and couldn&#x27;t be done over a networked client, for example.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#Querying%20Data%20by%20Prefix&quot; name=&quot;Querying%20Data%20by%20Prefix&quot;&gt;
    Querying Data by Prefix
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;A user on Discord requested an easier way to be able to query data using a
prefix. For example, if the key type is a &lt;code&gt;String&lt;&#x2F;code&gt; or &lt;code&gt;Vec&amp;lt;u8&amp;gt;&lt;&#x2F;code&gt;, a query should
be able to be made for any matches that start with a given prefix. Collections
now support listing documents with a given prefix:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span&gt;(Debug, Serialize, Deserialize, Default, Collection)]
&lt;&#x2F;span&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;collection&lt;&#x2F;span&gt;&lt;span&gt;(name = &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;MyCollection&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, primary_key = String)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;pub struct &lt;&#x2F;span&gt;&lt;span&gt;MyCollection;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; results = MyCollection::list_with_prefix(String::from(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;), db).await?;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;results&lt;&#x2F;code&gt; will contain all documents whose id&#x27;s start with &lt;code&gt;a&lt;&#x2F;code&gt;. The same
capability is also available for views:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span&gt;(View, Debug, Clone)]
&lt;&#x2F;span&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;view&lt;&#x2F;span&gt;&lt;span&gt;(name = &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;by-name&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, key = String, collection = MyCollection)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;struct &lt;&#x2F;span&gt;&lt;span&gt;ByName;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span&gt; mapping in db
&lt;&#x2F;span&gt;&lt;span&gt;    .view::&amp;lt;ByName&amp;gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;with_key_prefix&lt;&#x2F;span&gt;&lt;span&gt;(String::from(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;))
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;query&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    .await?
&lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    assert!(mapping.key.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;starts_with&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;));
&lt;&#x2F;span&gt;&lt;span&gt;    println!(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt; in document &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{:?}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, mapping.key, mapping.source);
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This functionality is powered by a new trait,
&lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;docs&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;trait.IntoPrefixRange.html&quot;&gt;&lt;code&gt;IntoPrefixRange&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;,
which allows any custom key type to provide this functionality if possible.
BonsaiDb currently only provides implementations for &lt;code&gt;String&lt;&#x2F;code&gt; and the &lt;code&gt;Vec&amp;lt;u8&amp;gt;&lt;&#x2F;code&gt;
types.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Nebari%20Bug%20Fixes&quot; name=&quot;Nebari%20Bug%20Fixes&quot;&gt;
    Nebari Bug Fixes
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This release updates &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt; to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;releases&#x2F;tag&#x2F;v0.4.0&quot;&gt;v0.4.0&lt;&#x2F;a&gt;. It contains
an important fix to ensure the transaction log is always written sequentially.
This edge case was discovered while running unit tests &amp;quot;one more time&amp;quot; before
preparing a release. A unit test that never failed suddenly failed. Here&#x27;s the
code in question that failed:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span&gt; handles = Vec::new();
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;_ in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;10 &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; manager = manager.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span&gt;    handles.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span&gt;(std::thread::spawn(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;move &lt;&#x2F;span&gt;&lt;span&gt;|| {
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span&gt; id in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;0_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u32&lt;&#x2F;span&gt;&lt;span&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;1_000 &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; tx = manager.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;new_transaction&lt;&#x2F;span&gt;&lt;span&gt;([&amp;amp;id.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;to_be_bytes&lt;&#x2F;span&gt;&lt;span&gt;()[..]]);
&lt;&#x2F;span&gt;&lt;span&gt;            tx.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;commit&lt;&#x2F;span&gt;&lt;span&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span&gt;        }
&lt;&#x2F;span&gt;&lt;span&gt;    }));
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The test then proceeds to wait for all the threads to finish and verify the
results. One thing it does is attempt to load each transaction one by one. At
first glance, this seems innocuous, but due to how Nebari is designed,
&lt;code&gt;new_transaction()&lt;&#x2F;code&gt; allocates a transaction id, but it isn&#x27;t written to the
transaction log until &lt;code&gt;commit()&lt;&#x2F;code&gt; occurs. I was &lt;em&gt;lucky&lt;&#x2F;em&gt; to have this test fail,
as it was a legitimate edge case that I hadn&#x27;t tested against. Needless to say,
there&#x27;s now a dedicated unit test to out-of-order writing.&lt;&#x2F;p&gt;
&lt;p&gt;While working on this fix, however, I now have enough of a reason to implement
the next version of the transaction log to address a few design issues I&#x27;ve
thought of since writing it originally. The good news is that even if the
transaction log your database has currently is out of order, I plan to have the
upgrade process to the new format fix any ordering issues.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#Getting%20Started&quot; name=&quot;Getting%20Started&quot;&gt;
    Getting Started
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of
examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;&quot;&gt;user&#x27;s
guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good
documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We would love to hear from you if you have questions or feedback. We have &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an issue&lt;&#x2F;a&gt; with any questions or feedback.&lt;&#x2F;p&gt;
&lt;p&gt;We dream big with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, and we believe that it can simplify writing and deploying complex, data-driven applications in Rust. We would love &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;labels&#x2F;good%20first%20issue&quot;&gt;additional contributors&lt;&#x2F;a&gt; who have similar passions and ambitions.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear about it. Nothing makes us happier than hearing people are building things with our crates!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>BonsaiDb February update: Supporting and Optimizing BonsaiDb</title>
		<published>2022-02-26T00:00:00+00:00</published>
		<updated>2022-02-26T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/february-2022-update/" type="text/html"/>
		<id>https://bonsaidb.io/blog/february-2022-update/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;It&#x27;s the last weekend of the month, and I want to make it a habit at the end of
each month to reflect on the previous month as well as talk about what I&#x27;m
currently focused on or what is coming next.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Status%20of%20the%20Alpha&quot; name=&quot;Status%20of%20the%20Alpha&quot;&gt;
    Status of the Alpha
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I released our initial alpha on February 5. I am ecstatic with the warm
reception. &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&#x2F;&quot;&gt;Our Discord server&lt;&#x2F;a&gt; has grown with several active users who
have been running BonsaiDb through its paces. Several bugs have been discovered
and fixed, with these releases happening in February:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.1.0&quot;&gt;v0.1.0&lt;&#x2F;a&gt;: Initial alpha release.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.1.1&quot;&gt;v0.1.1&lt;&#x2F;a&gt;: &lt;a href=&quot;&#x2F;blog&#x2F;february-2022-update#Derive%20Macros&quot;&gt;Derive macros&lt;&#x2F;a&gt; for common traits, many documentation
improvements, and memory-only databases for testing purposes.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.1.2&quot;&gt;v0.1.2&lt;&#x2F;a&gt;: Worked
around an undocumented panic within tokio::sleep on Windows that a user
experienced. Upstream patch merged as well.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.2.0&quot;&gt;v0.2.0&lt;&#x2F;a&gt;: &lt;a href=&quot;.&#x2F;bonsaidb-v0-2-0#Custom%20Primary%20Keys&quot;&gt;Custom
primary keys&lt;&#x2F;a&gt;, &lt;a href=&quot;&#x2F;blog&#x2F;bonsaidb-v0-2-0#LZ4%20Compression&quot;&gt;LZ4
compression&lt;&#x2F;a&gt;, and other minor improvements.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In addition to these releases, the underlying storage layer (&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;)
has been updated as well with several new features, bug fixes, and performance
improvements. The primary performance improvements that have been released have
centered around better usage of Rust&#x27;s type system to avoid extra allocations.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#Derive%20Macros&quot; name=&quot;Derive%20Macros&quot;&gt;
    Derive Macros
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ModProg&#x2F;&quot;&gt;@ModProg&lt;&#x2F;a&gt; noticed that I had open issues for adding derive macros for
some common traits. He became the second contributor to the BonsaiDb repository
when these macros were added in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.1.1&quot;&gt;v0.1.1&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;I have an important design philosophy with BonsaiDb: Everything should be as
ergonomic as possible without macros. Macros should be used to improve an
already-well-designed API. I was comfortable writing
schemas by hand, but I recognized how often I copy and pasted boilerplate code
and then tweaked small portions -- a good sign that a macro would help!&lt;&#x2F;p&gt;
&lt;p&gt;The way these macros are written is using a new crate &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ModProg&#x2F;&quot;&gt;@ModProg&lt;&#x2F;a&gt; wrote:
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ModProg&#x2F;attribute-derive&quot;&gt;attribute-derive&lt;&#x2F;a&gt;. I&#x27;ve made a few
updates to the macros since his pull request, and it&#x27;s been a wonderful
experience. If you are looking into writing an attribute proc macro, you might
want to consider using this crate to make simplify parsing and support more
Rust-y syntaxes.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Supporting%20BonsaiDb&quot; name=&quot;Supporting%20BonsaiDb&quot;&gt;
    Supporting BonsaiDb
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve tried to be very responsive to all questions, feedback, and issues. It&#x27;s
fun seeing BonsaiDb evaluated in new ways. As I recalled in &lt;a href=&quot;&#x2F;blog&#x2F;bonsaidb-v0-2-0#LZ4%20Compression&quot;&gt;the LZ4 Compression
feature introduction&lt;&#x2F;a&gt;, I introduced that feature the day after a
potential user ran into space issues when evaluating BonsaiDb against their
current Sled code.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ModProg&#x2F;&quot;&gt;@ModProg&lt;&#x2F;a&gt; one morning tried running BonsaiDb on a Raspberry Pi, which
led to us diagnosing &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;GuillaumeGomez&#x2F;sysinfo&#x2F;issues&#x2F;700&quot;&gt;a bug&#x2F;edge case that&#x27;s been fixed in the &lt;code&gt;sysinfo&lt;&#x2F;code&gt;
crate&lt;&#x2F;a&gt;. It was a fun
morning debugging BonsaiDb, an async codebase, over a Discord screenshare
showing an ssh session into the Raspberry Pi running GDB!&lt;&#x2F;p&gt;
&lt;p&gt;A few days ago, the same user that was testing a somewhat large dataset reported
an issue. It turned out to be &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;commit&#x2F;125234cb3928ad4cd0c087d1afdd1d25ecd5710e&quot;&gt;a long-standing
bug&lt;&#x2F;a&gt;
in Nebari&#x27;s transaction log reading code. The unit tests were inadequate, and
the only reason it was noticed was due to the LZ4 decompression failing once a
transaction log entry was large enough to be stored compressed across multiple
pages. The associated data isn&#x27;t currently being used outside of the
&lt;code&gt;list_executed_transactions()&lt;&#x2F;code&gt; function, but eventually is planned to be a core
part of replication. A &lt;code&gt;cargo update&lt;&#x2F;code&gt; is all that is needed for BonsaiDb to
adopt the updated version of Nebari.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#Testing%20BonsaiDb%20with%20a%20large%20dataset&quot; name=&quot;Testing%20BonsaiDb%20with%20a%20large%20dataset&quot;&gt;
    Testing BonsaiDb with a large dataset
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Despite the bug that was just found, I still felt fairly confident in BonsaiDb&#x27;s
stability. Yet, it showed I hadn&#x27;t really tested a big dataset. With a user
trying to use BonsaiDb with a large dataset, I felt like I should do my own
testing to feel as confident in BonsaiDb&#x27;s abilities as I can be.&lt;&#x2F;p&gt;
&lt;p&gt;After looking through a few locations, I stumbled upon the &lt;a href=&quot;https:&#x2F;&#x2F;openlibrary.org&#x2F;developers&#x2F;dumps&quot;&gt;Open Library Data
Dumps&lt;&#x2F;a&gt;. The data set I&#x27;m going to talk
about in this post contains these entities:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Collection&lt;&#x2F;th&gt;&lt;th&gt;Records&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Authors&lt;&#x2F;td&gt;&lt;td&gt;9,145,605&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Editions&lt;&#x2F;td&gt;&lt;td&gt;33,102,390&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Works&lt;&#x2F;td&gt;&lt;td&gt;24,010,896&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The uncompressed file that this data is loaded from is 51,447mb. It contains
additional data from the types listed, so direct size comparisons aren&#x27;t able to
be done post-import.&lt;&#x2F;p&gt;
&lt;p&gt;Writing an importer was a fun project. Since the data being imported is too
large to read into RAM before processing, I started with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;5216db14f621233c0a5f7f8ba2ac62c7508b3bd5&#x2F;examples&#x2F;open-library&#x2F;examples&#x2F;open-library.rs#L440-L481&quot;&gt;a streaming TSV
parser&lt;&#x2F;a&gt;
that uses &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;zesterer&#x2F;flume&quot;&gt;flume&lt;&#x2F;a&gt; to send parsed records to another task. This uses a
bounded channel which will park the TSV parser thread if it outpaces the code
that saves the records to the disk.&lt;&#x2F;p&gt;
&lt;p&gt;The data importing logic is fairly straightforward. I have one method that
receives records from the TSV parser and &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;5216db14f621233c0a5f7f8ba2ac62c7508b3bd5&#x2F;examples&#x2F;open-library&#x2F;examples&#x2F;open-library.rs#L545-L565&quot;&gt;creates batched
transactions&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Importing the data with LZ4 compression enabled took 60m56s, and with
compression disabled it took 64m12s. The resulting file sizes are interesting:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Collection&lt;&#x2F;th&gt;&lt;th&gt;LZ4 Uncompacted&lt;&#x2F;th&gt;&lt;th&gt;LZ4 Compacted&lt;&#x2F;th&gt;&lt;th&gt;Uncompacted&lt;&#x2F;th&gt;&lt;th&gt;Compacted&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Authors&lt;&#x2F;td&gt;&lt;td&gt;13,316mb&lt;&#x2F;td&gt;&lt;td&gt;3,675mb&lt;&#x2F;td&gt;&lt;td&gt;23,508mb&lt;&#x2F;td&gt;&lt;td&gt;4,159mb&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Editions&lt;&#x2F;td&gt;&lt;td&gt;74,459mb&lt;&#x2F;td&gt;&lt;td&gt;33,155mb&lt;&#x2F;td&gt;&lt;td&gt;118,330mb&lt;&#x2F;td&gt;&lt;td&gt;38,372mb&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Works&lt;&#x2F;td&gt;&lt;td&gt;40,443mb&lt;&#x2F;td&gt;&lt;td&gt;12,082mb&lt;&#x2F;td&gt;&lt;td&gt;71,601mb&lt;&#x2F;td&gt;&lt;td&gt;13,565mb&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;BonsaiDb uses &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;, which uses an append-only file format for its
database. As the internal trees are built, old nodes are left behind on disk.
The only way to reclaim the disk space currently is to compact the database -- a
process which rewrites all the active data to a fresh file and swaps the files
atomically once fully synchronized. Compacting the compressed database took
6m55s, and compacting the uncompressed database took 7m20s.&lt;&#x2F;p&gt;
&lt;p&gt;After testing importing and overwriting the database several times, my
confidence level in BonsaiDb grew. I built a simple CLI that allows querying
authors, editions, and works by their unique ID. Everything was working great.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#Querying%20large%20datasets%20with%20views&quot; name=&quot;Querying%20large%20datasets%20with%20views&quot;&gt;
    Querying large datasets with views
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I wanted to add support to show all the books (works) that an author wrote.
In the data coming from OpenLibrary, the &lt;code&gt;Work&lt;&#x2F;code&gt; type has a list of author roles.
This is the
&lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;about&#x2F;concepts&#x2F;view.html&quot;&gt;&lt;code&gt;View&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;
definition that enables this query:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span&gt;(View, Debug, Clone)]
&lt;&#x2F;span&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;view&lt;&#x2F;span&gt;&lt;span&gt;(name = &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;by-author&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, collection = Work, key = String, value = u32)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;struct &lt;&#x2F;span&gt;&lt;span&gt;WorksByAuthor;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span&gt;CollectionViewSchema &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;WorksByAuthor {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span&gt;View = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8fa1b3;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span&gt;(
&lt;&#x2F;span&gt;&lt;span&gt;        &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;document&lt;&#x2F;span&gt;&lt;span&gt;: CollectionDocument&amp;lt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self::&lt;&#x2F;span&gt;&lt;span&gt;View as View&amp;gt;::Collection&amp;gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    ) -&amp;gt; ViewMapResult&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self::&lt;&#x2F;span&gt;&lt;span&gt;View&amp;gt; {
&lt;&#x2F;span&gt;&lt;span&gt;        document
&lt;&#x2F;span&gt;&lt;span&gt;            .contents
&lt;&#x2F;span&gt;&lt;span&gt;            .authors
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;into_iter&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;filter_map&lt;&#x2F;span&gt;&lt;span&gt;(|&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;role&lt;&#x2F;span&gt;&lt;span&gt;| role.author)
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span&gt;(|&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;author&lt;&#x2F;span&gt;&lt;span&gt;| {
&lt;&#x2F;span&gt;&lt;span&gt;                document
&lt;&#x2F;span&gt;&lt;span&gt;                    .header
&lt;&#x2F;span&gt;&lt;span&gt;                    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;emit_key_and_value&lt;&#x2F;span&gt;&lt;span&gt;(author.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;into_key&lt;&#x2F;span&gt;&lt;span&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;replace&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;&#x2F;a&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;&#x2F;authors&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;            })
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;collect&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The data has some sanitization issues, including that keys sometimes are
shortened from &lt;code&gt;&#x2F;authors&#x2F;UNIQUEID&lt;&#x2F;code&gt; to &lt;code&gt;&#x2F;a&#x2F;UNIQUEID&lt;&#x2F;code&gt;. Because importing the data
takes so long, I chose to clean up the data post-import rather than force myself
to re-run the import with some code to clean it up while importing.&lt;&#x2F;p&gt;
&lt;p&gt;The entire function to display an author in the CLI is:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;async &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8fa1b3;&quot;&gt;summarize&lt;&#x2F;span&gt;&lt;span&gt;(&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;database&lt;&#x2F;span&gt;&lt;span&gt;: &amp;amp;Database) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt; {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span&gt;Some(name) = &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.name {
&lt;&#x2F;span&gt;&lt;span&gt;        println!(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;Name: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, name);
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span&gt;Some(bio) = &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.bio {
&lt;&#x2F;span&gt;&lt;span&gt;        println!(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;Biography:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, bio.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;())
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; works = database
&lt;&#x2F;span&gt;&lt;span&gt;        .view::&amp;lt;WorksByAuthor&amp;gt;()
&lt;&#x2F;span&gt;&lt;span&gt;        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;with_key&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.key.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span&gt;())
&lt;&#x2F;span&gt;&lt;span&gt;        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;query_with_collection_docs&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;        .await?;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;!works.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;is_empty&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;        println!(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;Works:&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;);
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span&gt; work in works.documents.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;values&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span&gt;Some(title) = &amp;amp;work.contents.title {
&lt;&#x2F;span&gt;&lt;span&gt;                println!(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, work.contents.key, title)
&lt;&#x2F;span&gt;&lt;span&gt;            }
&lt;&#x2F;span&gt;&lt;span&gt;        }
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Ok(())
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;While this was easy to implement, I knew that this was going to be a painful
process to run. BonsaiDb&#x27;s views are lazily evaluated (with the exception of
unique views). The first time the view is accessed, the view will need to have
it&#x27;s &lt;code&gt;map()&lt;&#x2F;code&gt; function invoked for each document stored in the collection. For
the Works collection, that means processing 24 million rows.&lt;&#x2F;p&gt;
&lt;p&gt;The initial query took over an hour. Subsequent queries are fast: the &lt;code&gt;time open-library author OL1394865A&lt;&#x2F;code&gt; command completes in less than 400ms once the
view is indexed.&lt;&#x2F;p&gt;
&lt;p&gt;I really wanted to be able to turn this example into a benchmarking suite, and
the only way to do that without driving myself crazy was to start optimizing.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#Optimizing%20View%20Mapping&quot; name=&quot;Optimizing%20View%20Mapping&quot;&gt;
    Optimizing View Mapping
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The initial implementation of the view mapper was aimed at being correct, not
necessarily fast. It was originally designed to operate within
&lt;a href=&quot;https:&#x2F;&#x2F;sled.rs&#x2F;&quot;&gt;Sled&lt;&#x2F;a&gt;, which allowed non-transactional reads and writes. The
thought was that I could simply parallelize some loops and magically the view
mapping system would be faster because Sled would handle ensuring the data was
written to disk correctly with multi-threaded writes.&lt;&#x2F;p&gt;
&lt;p&gt;After switching to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;, the view mapper always runs in a
transactional context. Each view mapping operation created its own transaction
-- the slowest way to write to the database. The biggest priority was going to
be to refactor the view mapper to batch the operations into larger transactions.&lt;&#x2F;p&gt;
&lt;p&gt;My &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;213&#x2F;commits&#x2F;b40607f753f1ab1808eb25a559e39fa0f6080541&quot;&gt;initial refactoring did just
that&lt;&#x2F;a&gt;
and lowered the execution time significantly (~70%), but it also introduced a subtle
bug. As a result, not all mappings were being saved, but I
didn&#x27;t notice the issue until much later in the refactoring process.&lt;&#x2F;p&gt;
&lt;p&gt;One issue I was running into is that Nebari&#x27;s &lt;code&gt;TransactionTree&lt;&#x2F;code&gt; required an
exclusive borrow on the &lt;code&gt;ExecutingTransaction&lt;&#x2F;code&gt;. This prevented being able to read
documents in one thread while writing view entries in another thread. I
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;commit&#x2F;c195be84412b532dd1802379676cada106f6c62f&quot;&gt;refactored Nebari&#x27;s transaction handling
code&lt;&#x2F;a&gt;
to allow this sort of parallelization.&lt;&#x2F;p&gt;
&lt;p&gt;After &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;213&#x2F;commits&#x2F;5216db14f621233c0a5f7f8ba2ac62c7508b3bd5&quot;&gt;adopting the new
functionality&lt;&#x2F;a&gt;,
here are the view mapping times on the compressed dataset:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Collection&lt;&#x2F;th&gt;&lt;th&gt;Size&lt;&#x2F;th&gt;&lt;th&gt;View&lt;&#x2F;th&gt;&lt;th&gt;Unoptimized (LZ4)&lt;&#x2F;th&gt;&lt;th&gt;Optimized (LZ4)&lt;&#x2F;th&gt;&lt;th&gt;Optimized&lt;&#x2F;th&gt;&lt;th&gt;Uncompacted&lt;&#x2F;th&gt;&lt;th&gt;Compacted&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Works&lt;&#x2F;td&gt;&lt;td&gt;12,082mb&lt;&#x2F;td&gt;&lt;td&gt;WorksByAuthor&lt;&#x2F;td&gt;&lt;td&gt;1h02m03s&lt;&#x2F;td&gt;&lt;td&gt;10m16s&lt;&#x2F;td&gt;&lt;td&gt;10m13s&lt;&#x2F;td&gt;&lt;td&gt;16,241mb&lt;&#x2F;td&gt;&lt;td&gt;1,863mb&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Editions&lt;&#x2F;td&gt;&lt;td&gt;33,155mb&lt;&#x2F;td&gt;&lt;td&gt;EditionsByWork&lt;&#x2F;td&gt;&lt;td&gt;26m52s&lt;&#x2F;td&gt;&lt;td&gt;12m23s&lt;&#x2F;td&gt;&lt;td&gt;12m48s&lt;&#x2F;td&gt;&lt;td&gt;17,507mb&lt;&#x2F;td&gt;&lt;td&gt;3,396mb&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;While preparing this post, I&#x27;ve re-run the pre-optimized code and optimized code
to verify my notes on timings. When initially starting work on optimization, I
hadn&#x27;t implemented &lt;code&gt;EditionsByWork&lt;&#x2F;code&gt;. I&#x27;ve verified the output of both
unoptimized versions and optimized versions match a random set of entries that
I&#x27;ve compared against their website. To clarify some of the numbers: all sizes
listed above are the compressed sizes. Adding all sizes made the table too large
to be easily understood.&lt;&#x2F;p&gt;
&lt;p&gt;At this stage, I no longer have any low hanging fruit from an algorithm
perspective. The next step in optimization would be to start profiling, but I&#x27;ve
reached my initial goals, so I&#x27;m going to be focusing on wrapping up this &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;213&quot;&gt;pull
request&lt;&#x2F;a&gt; by finishing this
example. I would enjoy turning this into a benchmark suite, but I&#x27;m also not
excited at the prospects of how long such a suite would take to debug fully.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#Next%20steps%20for%20BonsaiDb&quot; name=&quot;Next%20steps%20for%20BonsaiDb&quot;&gt;
    Next steps for BonsaiDb
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;m going to continue prioritizing any stability or performance issues as they
are reported. There have been a few repeated requests that I&#x27;m going to be
working on soon: &amp;quot;best practices&amp;quot; overview, internal archicture documentation,
and a high-level roadmap.&lt;&#x2F;p&gt;
&lt;p&gt;Several people have expressed interest in learning more about BonsaiDb&#x27;s
internals and roadmap with a goal of potentially contributing. I will be working
on those requests soon. In the meantime, I want to extend my offer to anyone
reading this: if you have any questions about any part of BonsaiDb, don&#x27;t
hesitate to ask! Regardless of your experience with Rust, if you&#x27;re excited at
the idea of helping build BonsaiDb, I will be happy to help you get up to speed.&lt;&#x2F;p&gt;
&lt;p&gt;Aside from improved documentation and project planning, I was working on
building a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;pull&#x2F;211&quot;&gt;persistent job
queue&lt;&#x2F;a&gt;, which I will be
returning to and hopefully ship in March. I&#x27;m building this feature as a
standalone crate that will allow users to create independent job queues with
different job distribution strategies, and the long-term goal is for this crate
to be the first &amp;quot;plugin&amp;quot; for BonsaiDb.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-7&quot;&gt;


&lt;a href=&quot;#Getting%20Started&quot; name=&quot;Getting%20Started&quot;&gt;
    Getting Started
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of
examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.2.0&#x2F;guide&#x2F;&quot;&gt;user&#x27;s
guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good
documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We would love to hear from you if you have questions or feedback. We have
&lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord
server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an
issue&lt;&#x2F;a&gt; with any questions or
feedback.&lt;&#x2F;p&gt;
&lt;p&gt;We dream big with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, and we believe that it can simplify
writing and deploying complex, data-driven applications in Rust. We would love
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;labels&#x2F;good%20first%20issue&quot;&gt;additional
contributors&lt;&#x2F;a&gt;
who have similar passions and ambitions.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear
about it. Nothing makes us happier than hearing people are building things with
our crates!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>BonsaiDb v0.2.0: Custom Primary Keys, LZ4 Compression</title>
		<published>2022-02-18T00:00:00+00:00</published>
		<updated>2022-02-18T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/bonsaidb-v0-2-0/" type="text/html"/>
		<id>https://bonsaidb.io/blog/bonsaidb-v0-2-0/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a new database aiming to be the most developer-friendly
Rust database. BonsaiDb has a unique feature set geared at solving many common
data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I&#x27;m excited to announce &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.2.0&quot;&gt;v0.2.0&lt;&#x2F;a&gt;, which brings two major features in
addition to many small improvements and fixes. There are API-level changes that
are potentially breaking changes. The &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;tree&#x2F;main&#x2F;CHANGELOG.md&quot;&gt;CHANGELOG&lt;&#x2F;a&gt; should help navigate through the changes
necessary to update your projects.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Custom%20Primary%20Keys&quot; name=&quot;Custom%20Primary%20Keys&quot;&gt;
    Custom Primary Keys
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;One design decision that was made early in BonsaiDb&#x27;s development was to use a
u64 as the unique id&#x2F;primary key for all documents. This was largely done to
simplify the API, and I felt like it was reasonable that users could create an
alternate key using a &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;schema&#x2F;trait.ViewSchema.html#method.unique&quot;&gt;unique view&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve attempted twice before to change it to allow customizing the primary key,
but each time I ended up reverting the changes as I didn&#x27;t like the impact to
the API. However, this time I&#x27;ve succeeded in adding this API with minimal
impact to the high-level API.&lt;&#x2F;p&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;v0.2.0&#x2F;examples&#x2F;basic-local&#x2F;examples&#x2F;primary-keys.rs&quot;&gt;new example&lt;&#x2F;a&gt; demonstrates how this feature can be
used with the &lt;code&gt;Collection&lt;&#x2F;code&gt; derive macro:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span&gt;(Debug, Serialize, Deserialize, Collection, Eq, PartialEq)]
&lt;&#x2F;span&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;collection&lt;&#x2F;span&gt;&lt;span&gt;(name = &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;multi-key&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, primary_key = (u32, u64))]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;struct &lt;&#x2F;span&gt;&lt;span&gt;MultiKey {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;: String,
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;async &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8fa1b3;&quot;&gt;test&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;C: Connection&amp;gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;db&lt;&#x2F;span&gt;&lt;span&gt;: C) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt; {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; inserted = MultiKey {
&lt;&#x2F;span&gt;&lt;span&gt;        value: String::from(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;hello&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;),
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;insert_into&lt;&#x2F;span&gt;&lt;span&gt;((&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;42&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span&gt;), &amp;amp;db)
&lt;&#x2F;span&gt;&lt;span&gt;    .await?;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; retrieved = MultiKey::get((&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;42&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span&gt;), &amp;amp;db)
&lt;&#x2F;span&gt;&lt;span&gt;        .await?
&lt;&#x2F;span&gt;&lt;span&gt;        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;expect&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;document not found&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;);
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The way this is implemented, any type that implements &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;latest&#x2F;bonsaidb&#x2F;core&#x2F;key&#x2F;trait.Key.html&quot;&gt;&lt;code&gt;Key&lt;&#x2F;code&gt; trait&lt;&#x2F;a&gt; and
isn&#x27;t too large can be used as a primary key. The current size limitation is
purely for optimization, and may be relaxed in the future.&lt;&#x2F;p&gt;
&lt;p&gt;More information about this feature can be found &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.2.0&#x2F;guide&#x2F;about&#x2F;concepts&#x2F;collection.html#primary-keys&quot;&gt;in our user guide&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#LZ4%20Compression&quot; name=&quot;LZ4%20Compression&quot;&gt;
    LZ4 Compression
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;A potential user was originally trying to diagnose why BonsaiDb was creating
larger databases than Sled. After some troubleshooting, it was discovered that
they had enabled the &lt;code&gt;zstd&lt;&#x2F;code&gt; feature in Sled, enabling automatic compression of
data being stored.&lt;&#x2F;p&gt;
&lt;p&gt;This was a feature I had already planned on supporting, although my research had
led me to believe that LZ4 was a better choice for an algorithm. Without doing
comparative benchmarking personally, I decided to start with LZ4 compression and
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;198&quot;&gt;add &lt;code&gt;zstd&lt;&#x2F;code&gt; once namespaced features are released in Rust&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Some benchmarks have been updated to include compression for comparision. The
results are interesting to look at, but no serious comparisons should be drawn
yet. If you are under tight space constraints, this feature can save a lot of
storage space while not impacting performance in a major way.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.2.0&#x2F;benchmarks&#x2F;commerce&#x2F;large-balanced&#x2F;4&#x2F;index.html#FindProduct&quot;&gt;&lt;img src=&quot;https:&amp;#x2F;&amp;#x2F;dev.bonsaidb.io&amp;#x2F;v0.2.0&amp;#x2F;benchmarks&amp;#x2F;commerce&amp;#x2F;large-balanced&amp;#x2F;4&amp;#x2F;FindProduct.png&quot; class=&quot;block-image&quot; alt=&quot;lookup product by id&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As you can see in this graph, the &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.2.0&#x2F;benchmarks&#x2F;commerce&#x2F;&quot;&gt;Commerce
Benchmark&lt;&#x2F;a&gt; sometimes shows
improved performance over the uncompressed version. As always, performance will
depend on your hardware and operating environment, which is why this is an
opt-in feature.&lt;&#x2F;p&gt;
&lt;p&gt;The user who originally asked about this feature had reported that BonsaiDb&#x27;s
uncompressed database size a staggering 30x larger than the equivalent data in
Sled. The next day they were able to test a branch with the feature enabled. The
BonsaiDb database compressed with LZ4 shrunk to under half the size of their
Sled database.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Getting%20Started&quot; name=&quot;Getting%20Started&quot;&gt;
    Getting Started
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.2.0&#x2F;guide&#x2F;&quot;&gt;user&#x27;s guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We would love to hear from you if you have questions or feedback. We have &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an issue&lt;&#x2F;a&gt; with any questions or feedback.&lt;&#x2F;p&gt;
&lt;p&gt;We dream big with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, and we believe that it can simplify writing and deploying complex, data-driven applications in Rust. We would love &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;labels&#x2F;good%20first%20issue&quot;&gt;additional contributors&lt;&#x2F;a&gt; who have similar passions and ambitions.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear about it. Nothing makes us happier than hearing people are building things with our crates!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Announcing BonsaiDb 0.1.0: A Rust NoSQL database that grows with you</title>
		<published>2022-02-05T00:00:00+00:00</published>
		<updated>2022-02-05T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/announcing-bonsaidb-alpha/" type="text/html"/>
		<id>https://bonsaidb.io/blog/announcing-bonsaidb-alpha/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What is BonsaiDb?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is new database aiming to be the most developer-friendly Rust database. BonsaiDb has a unique feature set geared at solving many common data problems. We have a page dedicated to answering the question: &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&quot;&gt;What is BonsaiDb?&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;It&#x27;s been over six months since &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;pliantdb-v0-1-0-dev-4-released-custom-apis-unique-views-webassembly&#x2F;69&quot;&gt;we announced PliantDb 0.1.0-dev.4&lt;&#x2F;a&gt;. Beyond the name change, a lot has changed. We&#x27;ve been focusing our efforts on stabilizing BonsaiDb&#x27;s core architecture so that we can start building applications with BonsaiDb. Today, we&#x27;re excited to announce our first alpha release: &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.1.0&quot;&gt;version 0.1.0&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Upgrading%20existing%20databases&quot; name=&quot;Upgrading%20existing%20databases&quot;&gt;
    Upgrading existing databases
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;There is no upgrade path from 0.1.0-dev.4. Prior to this release, we warned against using this project outside of experimentation, so we hope this is not an issue for anyone. However, if this is devastating news to anyone, please reach out to us, and we can try to help.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#What%20does%20alpha%20mean%3F&quot; name=&quot;What%20does%20alpha%20mean%3F&quot;&gt;
    What does alpha mean?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The biggest barrier to adoption was our own making: our README actively discouraged early-adopters except out of experimentation. We held off releasing this first alpha until after we were confident in the foundation we&#x27;ve laid. We expect bugs will be found, and we are certain that at least one user will lose data. It&#x27;s just a fact of life. The alpha phase enables adventurous users to begin using BonsaiDb to help us find these issues before we label BonsaiDb &amp;quot;stable.&amp;quot;&lt;&#x2F;p&gt;
&lt;p&gt;We encourage users that attempt to use this in an environment where data loss would be detrimental to utilize and test the &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;about&#x2F;#backup-restore&quot;&gt;backup and restore&lt;&#x2F;a&gt; functionality regularly.&lt;&#x2F;p&gt;
&lt;p&gt;We are still planning to add features during the alpha phase, but those features will focus on the higher-level database functionality. Changes to how data is stored will be minimal or non-existent unless bugs or performance issues are uncovered.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#What%27s%20new%3F&quot; name=&quot;What%27s%20new%3F&quot;&gt;
    What&amp;#x27;s new?
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;While the fundamental ideas powering BonsaiDb are still the same as previous releases, this release left almost no code untouched. The GitHub stats show &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;compare&#x2F;v0.1.0-dev.4...v0.1.0&quot;&gt;29,933 additions and 11,678 across 450 commits&lt;&#x2F;a&gt; -- and those stats don&#x27;t include the new crates introduced. The &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&#x2F;tag&#x2F;v0.1.0&quot;&gt;release on GitHub&lt;&#x2F;a&gt; covers all of the changes, but we wanted to draw attention to a few notable entries.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#New%20Storage%20Layer&quot; name=&quot;New%20Storage%20Layer&quot;&gt;
    New Storage Layer
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Under the hood, this alpha unveils a new storage engine: &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;. While the storage layer we were using before (&lt;a href=&quot;https:&#x2F;&#x2F;sled.rs&quot;&gt;Sled&lt;&#x2F;a&gt;) is incredibly fast, it also is quite complex and didn&#x27;t quite fit BonsaiDb&#x27;s needs perfectly.&lt;&#x2F;p&gt;
&lt;p&gt;Nebari maintains a fairly straightforward design while still achieving fairly competitive benchmarks for being such a new library. Additionally, its implementation can be custom-tailored to the workloads we need to support BonsaiDb. Nebari &lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;nebari-scaleway-gp1-xs&#x2F;index.html&quot;&gt;performs quite well&lt;&#x2F;a&gt; in our initial testing.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-4&quot;&gt;


&lt;a href=&quot;#New%20Serialization%20Ecosystem&quot; name=&quot;New%20Serialization%20Ecosystem&quot;&gt;
    New Serialization Ecosystem
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Part of our design from the beginning was to allow users to store arbitrary chunks of data as a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.1.0&#x2F;guide&#x2F;about&#x2F;concepts&#x2F;document.html&quot;&gt;document&lt;&#x2F;a&gt;. In other words, users shouldn&#x27;t be forced into a specific serialization or encoding format for their data. Unfortunately, a side effect of this was the API&#x27;s usability suffered.&lt;&#x2F;p&gt;
&lt;p&gt;This alpha addresses the API shortcomings with functionality designed around &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;transmog&quot;&gt;Transmog&lt;&#x2F;a&gt;. Transmog is a fairly simple concept: a universal set of traits to describe a basic serialization and deserialization interface. An example of Transmog in cation can be found in our new example: &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;main&#x2F;examples&#x2F;view-histogram&#x2F;examples&#x2F;view-histogram.rs&quot;&gt;view-histogram.rs&lt;&#x2F;a&gt;. It demonstrates storing a &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;hdrhistogram&#x2F;latest&#x2F;hdrhistogram&#x2F;struct.Histogram.html&quot;&gt;hdrhistogram::Histogram&lt;&#x2F;a&gt; as a view&#x27;s value by implementing the needed Transmog traits, allowing these types of fluid queries:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Retrieve an `hdrhistogram::Histogram` with all samples recorded.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; total_histogram = db.view::&amp;lt;AsHistogram&amp;gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;reduce&lt;&#x2F;span&gt;&lt;span&gt;().await?;
&lt;&#x2F;span&gt;&lt;span&gt;println!(
&lt;&#x2F;span&gt;&lt;span&gt;    &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;99th Percentile overall: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt; (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt; samples)&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;,
&lt;&#x2F;span&gt;&lt;span&gt;    total_histogram.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;value_at_quantile&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;0.99&lt;&#x2F;span&gt;&lt;span&gt;),
&lt;&#x2F;span&gt;&lt;span&gt;    total_histogram.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Using Transmog as a foundation, BonsaiDb provides streamlined APIs for interacting with &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;0.1.0&#x2F;bonsaidb&#x2F;core&#x2F;connection&#x2F;trait.Connection.html#using-connection-with-collectiondocumentt&quot;&gt;serialized collections&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-5&quot;&gt;


&lt;a href=&quot;#Extensible%20TCP%20Handling&quot; name=&quot;Extensible%20TCP%20Handling&quot;&gt;
    Extensible TCP Handling
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We have a vision that BonsaiDb will be a complete application development platform, including building web applications. Therefore, we have created a way to theoretically use any HTTP framework and mount the BonsaiDb WebSockets at a specific route.&lt;&#x2F;p&gt;
&lt;p&gt;We have a new example (&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;main&#x2F;examples&#x2F;axum&#x2F;examples&#x2F;axum.rs&quot;&gt;axum.rs&lt;&#x2F;a&gt;) which demonstrates how to accomplish this using the &lt;a href=&quot;https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;axum&quot;&gt;Axum&lt;&#x2F;a&gt; framework.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-6&quot;&gt;


&lt;a href=&quot;#At-Rest%20Encryption&quot; name=&quot;At-Rest%20Encryption&quot;&gt;
    At-Rest Encryption
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Not all hosting environments offer encrypted filesystems, and almost all applications store some Personally Identifiable Information (PII). Therefore, it is a good practice (and often a legal requirement) to ensure that all sensitive information is stored encrypted on the disk.&lt;&#x2F;p&gt;
&lt;p&gt;BonsaiDb now optionally supports transparent &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.1.0&#x2F;guide&#x2F;administration&#x2F;encryption.html&quot;&gt;at-rest encryption using externally stored keys&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-7&quot;&gt;


&lt;a href=&quot;#Improving%20the%20Ecosystem&quot; name=&quot;Improving%20the%20Ecosystem&quot;&gt;
    Improving the Ecosystem
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;New and updated crates in the Rust ecosystem power this release:&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-8&quot;&gt;


&lt;a href=&quot;#New%20Crates&quot; name=&quot;New%20Crates&quot;&gt;
    New Crates
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ModProg&#x2F;derive-where&#x2F;&quot;&gt;derive-where&lt;&#x2F;a&gt;: Derive standard traits more easily when generic type bounds are involved&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt;: Low-level, transactional key-value storage&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;ordered-varint&quot;&gt;ordered-varint&lt;&#x2F;a&gt;: Variable-length signed and unsigned integer encoding that retains sortable at the byte-level&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;pot&quot;&gt;Pot&lt;&#x2F;a&gt;: A concise, self-describing binary serialization format.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;rustme&quot;&gt;RustMe&lt;&#x2F;a&gt;: Tool and library for managing READMEs for Rust projects&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;transmog&quot;&gt;Transmog&lt;&#x2F;a&gt;: Universal serialization traits and utilities.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;-9&quot;&gt;


&lt;a href=&quot;#Updated%20Crates&quot; name=&quot;Updated%20Crates&quot;&gt;
    Updated Crates
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;These are existing crates we&#x27;ve worked on as part of developing this release of BonsaiDb:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;User65k&#x2F;async-acme&#x2F;&quot;&gt;async-acme&lt;&#x2F;a&gt;: Async ACME client for the TLS-ALPN-01 challenge type&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;RustCrypto&#x2F;password-hashes&quot;&gt;password-hashes&lt;&#x2F;a&gt;: Password hashing functions and KDFs.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;zeroize&quot;&gt;Zeroize&lt;&#x2F;a&gt;: Utilities for zeroing buffers before deallocating.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;-10&quot;&gt;


&lt;a href=&quot;#In-progress%20Crates&quot; name=&quot;In-progress%20Crates&quot;&gt;
    In-progress Crates
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We removed OPAQUE support for now, but will be reintroducing it &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;161&quot;&gt;in the
future&lt;&#x2F;a&gt;.
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;daxpedda&quot;&gt;@daxpedda&lt;&#x2F;a&gt; has been hard at work helping bring the
ecosystem up to the current draft spec:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;custodian&quot;&gt;custodian-password&lt;&#x2F;a&gt;: High-level implementation of the OPAQUE password-authenticated key exchange protocol.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;novifinancial&#x2F;opaque-ke&quot;&gt;opaque-ke&lt;&#x2F;a&gt;: An implementation of the OPAQUE password-authenticated key exchange protocol. Used by custodian-password.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;novifinancial&#x2F;voprf&#x2F;&quot;&gt;voprf&lt;&#x2F;a&gt;: An implementation of a verifiable oblivious pseudorandom function. Used by &lt;code&gt;opaque-ke&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;-11&quot;&gt;


&lt;a href=&quot;#Getting%20Started&quot; name=&quot;Getting%20Started&quot;&gt;
    Getting Started
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;&quot;&gt;new homepage&lt;&#x2F;a&gt; has basic setup instructions and a list of examples. We have started writing a &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;v0.1.0&#x2F;guide&#x2F;&quot;&gt;user&#x27;s guide&lt;&#x2F;a&gt;, and we have tried to write &lt;a href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;bonsaidb&#x2F;&quot;&gt;good documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We would love to hear from you if you have questions or feedback. We have &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;&quot;&gt;community Discourse forums&lt;&#x2F;a&gt; and a &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&quot;&gt;Discord server&lt;&#x2F;a&gt;, but also welcome anyone to &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;new&quot;&gt;open an issue&lt;&#x2F;a&gt; with any questions or feedback.&lt;&#x2F;p&gt;
&lt;p&gt;We dream big with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, and we believe that it can simplify writing and deploying complex, data-driven applications in Rust. We would love &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;labels&#x2F;good%20first%20issue&quot;&gt;additional contributors&lt;&#x2F;a&gt; who have similar passions and ambitions.&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, if you build something with one of our libraries, we would love to hear about it. Nothing makes us happier than hearing people are building things with our crates!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>BonsaiDb January update: Alpha Next Week</title>
		<published>2022-01-27T00:00:00+00:00</published>
		<updated>2022-05-24T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/january-2022-update/" type="text/html"/>
		<id>https://bonsaidb.io/blog/january-2022-update/</id>
		<content type="html">&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is a pure Rust database that grows with you. It already is &lt;a href=&quot;&#x2F;about&quot;&gt;feature-rich&lt;&#x2F;a&gt;, but we are still working towards our initial alpha. This month&#x27;s update will highlight the changes since the last update and cover what&#x27;s remaining before our alpha release.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;highlighted-updates&quot;&gt;Highlighted updates&lt;&#x2F;h2&gt;
&lt;p&gt;I have made &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;compare&#x2F;355c7904dd9b64874d99721941d2b0c0002f26b4...c1bc3ca6ce488fe8c26d265a3b1e9b8fb62d1347&quot;&gt;90 commits (+8,406&#x2F;-3,097)&lt;&#x2F;a&gt; since &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;bonsaidb-december-update-finishing-up-alpha-1&#x2F;88&quot;&gt;last month&#x27;s update&lt;&#x2F;a&gt;. Many improvements have been made based on feedback from early adopters. Thank you to everyone who has asked a question, reported an issue, or provided any feedback!&lt;&#x2F;p&gt;
&lt;h3 id=&quot;&quot;&gt;


&lt;a href=&quot;#Key-Value%20Store%20Optimization&quot; name=&quot;Key-Value%20Store%20Optimization&quot;&gt;
    Key-Value Store Optimization
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I firmly believe in under-promising and over-delivering. So when I was writing the &lt;a href=&quot;&#x2F;about&quot;&gt;about page&lt;&#x2F;a&gt; originally, I had to clarify in the &lt;a href=&quot;&#x2F;about&#x2F;#key-value&quot;&gt;Key-Value section&lt;&#x2F;a&gt; that the performance was limited by &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;120&quot;&gt;the current design&lt;&#x2F;a&gt;. As such, I decided I would prefer to go ahead and implement the design proposed in that issue.&lt;&#x2F;p&gt;
&lt;p&gt;The Key-Value store is meant to perform atomic operations with reasonable durability. This is the primary difference between the goals of the Key-Value store and &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;about&#x2F;concepts&#x2F;collection.html&quot;&gt;Collection&lt;&#x2F;a&gt; storage. The Key-Value store is meant to be a good alternative to running &lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;&quot;&gt;Redis&lt;&#x2F;a&gt;, and Redis uses a delayed persistence design.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve copied the general design of how persistence works, except that with BonsaiDb the database isn&#x27;t kept in memory constantly. This means that BonsaiDb&#x27;s Key-Value store can contain more data than the RAM on your machine, something that is possible but not recommended with Redis.&lt;&#x2F;p&gt;
&lt;p&gt;By default, BonsaiDb will persist each operation after it&#x27;s performed. This can cause a lot of extra disk IO if thousands of operations are being performed per second. Additionally, it can cause file bloat due to the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;append-only file format&lt;&#x2F;a&gt; utilized. Instead, it&#x27;s recommended to &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;administration&#x2F;configuration.html#key-value-persistence&quot;&gt;configure persistence&lt;&#x2F;a&gt; such that writes are delayed based on what you feel is a good balance for your use case. For example:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;Storage::open(
&lt;&#x2F;span&gt;&lt;span&gt;    StorageConfiguration::new(&amp;amp;database_directory)
&lt;&#x2F;span&gt;&lt;span&gt;        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;key_value_persistence&lt;&#x2F;span&gt;&lt;span&gt;(KeyValuePersistence::lazy([
&lt;&#x2F;span&gt;&lt;span&gt;            PersistenceThreshold::after_changes(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;and_duration&lt;&#x2F;span&gt;&lt;span&gt;(Duration::from_secs(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;)),
&lt;&#x2F;span&gt;&lt;span&gt;            PersistenceThreshold::after_changes(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;),
&lt;&#x2F;span&gt;&lt;span&gt;  ])
&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This configuration has two rules: persist after 5 seconds if there is at least 1 change, and perist if there are at least 200 changes. This means if BonsaiDb unexpectedly is killed, at most the most recent 5 seconds of changes would be lost. However, if a batch of changes is written, they will be persisted immediately.&lt;&#x2F;p&gt;
&lt;p&gt;So, how does the key-value store perform compared to Redis?&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;bonsaidb-scaleway-gp1-xs&#x2F;suite&#x2F;get-bytes&#x2F;1KiB&#x2F;report&#x2F;index.html&quot;&gt;&lt;img src=&quot;https:&amp;#x2F;&amp;#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&amp;#x2F;bonsaidb-scaleway-gp1-xs&amp;#x2F;suite&amp;#x2F;get-bytes&amp;#x2F;1KiB&amp;#x2F;report&amp;#x2F;violin.svg&quot; class=&quot;block-image&quot; alt=&quot;get-key 1kb&quot; style=&quot;background-color: #DDD&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Well, as you might expect, if you don&#x27;t have network access, things go very fast -- measured in nanoseconds on my personal computer. However, the networking performance leaves something to be desired. After doing a lot of profiling, I could see that the TLS for the QUIC connection accounts for roughly 30% of the time spent. However, that still is a little slower than the WebSocket implementation, which in turn is significantly slower than Redis.&lt;&#x2F;p&gt;
&lt;p&gt;My profiling has led me to believe that &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;129&quot;&gt;switching to Socketto&lt;&#x2F;a&gt; will bring the WebSocket implementation closer by reducing the number of allocations. Only time will tell if that will match Redis&#x27;s performance, but I&#x27;m hopeful it will be close enough to not care. For the QUIC connection, our &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;daxpedda&quot;&gt;other major contributor&lt;&#x2F;a&gt; has plans to dig in and see what can be done to reduce some of the allocations we saw in the profiling we did.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#Collection%20Benchmarks&quot; name=&quot;Collection%20Benchmarks&quot;&gt;
    Collection Benchmarks
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Edit 2022-05-25: These benchmarks were discovered to have serious problems.
Read more &lt;a href=&quot;&#x2F;blog&#x2F;durable-writes&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;My last &lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;blog&#x2F;commerce-benchmark&#x2F;&quot;&gt;blog post&lt;&#x2F;a&gt; goes into detail about a new benchmark I wrote to attempt to simulate a simple relational database workload. The results were staggering to me.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bonsaidb.io&#x2F;blog&#x2F;commerce-benchmark&#x2F;&quot;&gt;&lt;img src=&quot;https:&amp;#x2F;&amp;#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&amp;#x2F;bonsaidb-scaleway-gp1-xs&amp;#x2F;commerce&amp;#x2F;large-writeheavy&amp;#x2F;8&amp;#x2F;LookupProduct.png&quot; class=&quot;block-image&quot; alt=&quot;find prodcut by id graph&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Nearly every operation across every workload showed all ways of accessing BonsaiDb outperforming PostgreSQL significantly.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Rethinking%20Serialization&quot; name=&quot;Rethinking%20Serialization&quot;&gt;
    Rethinking Serialization
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;One problem that I had with BonsaiDb is that I didn&#x27;t want to force users into a specific serialization format. The base document type contains a buffer of bytes -- you can store whatever you&#x27;d like as a document. I believe our new serialization format, &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;pot&quot;&gt;Pot&lt;&#x2F;a&gt;, is a great choice, but there are very good reasons to make it easy to support any serialization format.&lt;&#x2F;p&gt;
&lt;p&gt;This month I spent time creating &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;transmog&quot;&gt;Transmog&lt;&#x2F;a&gt;, my take on an approach to universal serialization traits. This project aims at creating a common interface that most serialization formats could offer an implementation of. By leveraging transmog, BonsaiDb&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;docs&#x2F;bonsaidb&#x2F;core&#x2F;schema&#x2F;trait.SerializedCollection.html&quot;&gt;SerializedCollection&lt;&#x2F;a&gt; trait can be used to automatically serialize and deserialize your collection types for you regardless of the format you&#x27;re storing them as. Additionally, View value serialization is now powered by Transmog, as our &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;blob&#x2F;main&#x2F;examples&#x2F;view-histogram&#x2F;examples&#x2F;view-histogram.rs&quot;&gt;hdrhistogram view example&lt;&#x2F;a&gt; demonstrates:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F;&#x2F; A view for [`Samples`] which produces a histogram.
&lt;&#x2F;span&gt;&lt;span&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span&gt;(Debug, Clone)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;pub struct &lt;&#x2F;span&gt;&lt;span&gt;AsHistogram;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span&gt;View &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;AsHistogram {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span&gt;Collection = Samples;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span&gt;Key = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u64&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span&gt;Value = SyncHistogram&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u64&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8fa1b3;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;(&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;) -&amp;gt; Name {
&lt;&#x2F;span&gt;&lt;span&gt;        Name::new(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;as-histogram&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;)
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span&gt;SerializedView &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;AsHistogram {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span&gt;Format = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8fa1b3;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span&gt;() -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self::&lt;&#x2F;span&gt;&lt;span&gt;Format {
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span&gt;Format&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;&amp;#39;static&lt;&#x2F;span&gt;&lt;span&gt;, SyncHistogram&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u64&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;AsHistogram {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span&gt;Error = HistogramError;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8fa1b3;&quot;&gt;serialize_into&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;W: std::io::Write&amp;gt;(
&lt;&#x2F;span&gt;&lt;span&gt;        &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;: &amp;amp;SyncHistogram&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u64&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;writer&lt;&#x2F;span&gt;&lt;span&gt;: W,
&lt;&#x2F;span&gt;&lt;span&gt;    ) -&amp;gt; Result&amp;lt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self::&lt;&#x2F;span&gt;&lt;span&gt;Error&amp;gt; {
&lt;&#x2F;span&gt;&lt;span&gt;        V2Serializer::new()
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;serialize&lt;&#x2F;span&gt;&lt;span&gt;(value, &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;mut&lt;&#x2F;span&gt;&lt;span&gt; writer)
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;map_err&lt;&#x2F;span&gt;&lt;span&gt;(HistogramError::Serialization)?;
&lt;&#x2F;span&gt;&lt;span&gt;        Ok(())
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span&gt;OwnedDeserializer&amp;lt;SyncHistogram&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u64&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;AsHistogram {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8fa1b3;&quot;&gt;deserialize_from&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;R: std::io::Read&amp;gt;(
&lt;&#x2F;span&gt;&lt;span&gt;        &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;reader&lt;&#x2F;span&gt;&lt;span&gt;: R,
&lt;&#x2F;span&gt;&lt;span&gt;    ) -&amp;gt; Result&amp;lt;SyncHistogram&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u64&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;Self::&lt;&#x2F;span&gt;&lt;span&gt;Error&amp;gt; {
&lt;&#x2F;span&gt;&lt;span&gt;        hdrhistogram::serialization::Deserializer::new()
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;deserialize&lt;&#x2F;span&gt;&lt;span&gt;(&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;mut&lt;&#x2F;span&gt;&lt;span&gt; reader)
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span&gt;(SyncHistogram::from)
&lt;&#x2F;span&gt;&lt;span&gt;            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;map_err&lt;&#x2F;span&gt;&lt;span&gt;(HistogramError::Deserialization)
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This example shows how using &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;transmog&quot;&gt;Transmog&lt;&#x2F;a&gt; we&#x27;re able to use the custom serialization functions built into the &lt;a href=&quot;https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;hdrhistogram&quot;&gt;hdrhistogram&lt;&#x2F;a&gt; crate. Querying the view returns a &lt;code&gt;SyncHistogram&amp;lt;u64&amp;gt;&lt;&#x2F;code&gt; directly:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; total_histogram = db.view::&amp;lt;AsHistogram&amp;gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;reduce&lt;&#x2F;span&gt;&lt;span&gt;().await?;
&lt;&#x2F;span&gt;&lt;span&gt;println!(
&lt;&#x2F;span&gt;&lt;span&gt; &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;99th Percentile overall: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt; (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt; samples)&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;,
&lt;&#x2F;span&gt;&lt;span&gt; total_histogram.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;value_at_quantile&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;0.99&lt;&#x2F;span&gt;&lt;span&gt;),
&lt;&#x2F;span&gt;&lt;span&gt; total_histogram.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I think this example shows the inredible power of our &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;about&#x2F;concepts&#x2F;view.html&quot;&gt;map&#x2F;reduce views&lt;&#x2F;a&gt; and the Rust type system.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-3&quot;&gt;


&lt;a href=&quot;#Alpha%20coming%20next%20week&quot; name=&quot;Alpha%20coming%20next%20week&quot;&gt;
    Alpha coming next week
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I had a chat this morning with our other major contributor, (&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;daxpedda&quot;&gt;@daxpedda&lt;&#x2F;a&gt;), to try to understand the implications of the breaking changes coming in the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;novifinancial&#x2F;opaque-ke&quot;&gt;OPAQUE-KE&lt;&#x2F;a&gt; crate. The result of the conversation was &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;158&quot;&gt;this issue to implement traditional password hashing&lt;&#x2F;a&gt;. The first alpha will not contain OPAQUE as we&#x27;re going to wait until an update supporting  draft-irtf-cfrg-opaque-07 is released. This is one of the things he&#x27;s been working hard on for months, and he&#x27;s near the point where we can integrate the changes into BonsaiDb!&lt;&#x2F;p&gt;
&lt;p&gt;However, as OPAQUE is still a draft specification, there is too much possibility that things will break again in the future before being stabilized. We will help support these upgrades, but it&#x27;s not going to be the best administration experience, and it will potentially require utilizing multiple crate versions in the long run.&lt;&#x2F;p&gt;
&lt;p&gt;Once I swap out this implementation, I&#x27;m going to be creating a 0.1 branch &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;in the repository&lt;&#x2F;a&gt;. We will be using the 0.x version range for our alpha and beta phases and eventually &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;137&quot;&gt;release 1.0&lt;&#x2F;a&gt; as the first stable version.&lt;&#x2F;p&gt;
&lt;p&gt;I will ask early adopters to check it out ahead of releasing to crates.io at the end of next week. I&#x27;ll send out a message on &lt;a href=&quot;https:&#x2F;&#x2F;discord.khonsulabs.com&#x2F;&quot;&gt;our Discord&lt;&#x2F;a&gt; as well as update &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;issues&#x2F;87&quot;&gt;this issue&lt;&#x2F;a&gt; once the branch is available.&lt;&#x2F;p&gt;
&lt;p&gt;In the meantime, our &lt;a href=&quot;&#x2F;&quot;&gt;homepage&lt;&#x2F;a&gt; has basic getting started information including a full list of examples. I look forward to hearing what people build with BonsaiDb!&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Have questions or comments? Discuss this post on &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;bonsaidb-january-update-alpha-next-week&#x2F;93&quot;&gt;our forums&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Benchmarking relational data in BonsaiDb</title>
		<published>2022-01-18T00:00:00+00:00</published>
		<updated>2022-05-24T00:00:00+00:00</updated>
		<link href="https://bonsaidb.io/blog/commerce-benchmark/" type="text/html"/>
		<id>https://bonsaidb.io/blog/commerce-benchmark/</id>
		<content type="html">&lt;blockquote&gt;
&lt;p&gt;If you aren&#x27;t familiar with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt;, check out our &lt;a href=&quot;&#x2F;about&#x2F;&quot;&gt;What is
BonsaiDb?&lt;&#x2F;a&gt; page.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;While we&#x27;re working towards our first alpha, I&#x27;ve been trying to anticipate
questions potential users might have when looking at BonsaiDb for the first
time. Although we are keeping the alpha label, we are hoping to find some
adventurous users who are excited at our vision of a database designed for and
written in Rust.&lt;&#x2F;p&gt;
&lt;p&gt;One obvious question that almost everyone will ask at some point when hearing
about a new database: &lt;strong&gt;how does it perform?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;BonsaiDb is a unique database. It is best described as a &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;NoSQL&quot;&gt;NoSQL&lt;&#x2F;a&gt;
database, but traditionally NoSQL databases tend to favor eventual consistency
and speed over ACID compliance. When starting this project, I specifically
wanted to have all the guarantees PostgreSQL makes, but easier to use and deploy.&lt;&#x2F;p&gt;
&lt;p&gt;Until this past week, I knew from previous benchmarks that &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&quot;&gt;Nebari&lt;&#x2F;a&gt; (our
underlying storage layer) was &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;nebari&#x2F;tree&#x2F;192b6d34c6ad1350c2e469359f51423f69b1e2d4&#x2F;benchmarks&quot;&gt;pretty fast&lt;&#x2F;a&gt;, but I had no real
indicator of how BonsaiDb would perform relative to PostgreSQL.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;&quot;&gt;


&lt;a href=&quot;#Designing%20a%20Benchmark%20Suite&quot; name=&quot;Designing%20a%20Benchmark%20Suite&quot;&gt;
    Designing a Benchmark Suite
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I had three different parameters I wanted to measure:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Dataset size&lt;&#x2F;li&gt;
&lt;li&gt;The amount of concurrent workers&lt;&#x2F;li&gt;
&lt;li&gt;Read-heavy vs Write-heavy workloads&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I decided to create a benchmark suite inspired by the needs of an ecommerce
website. The basic idea is to generate an initial data set, a list of plans for
workers to execute, and test the dataset and plans using different quantities of
workers. The operations being benchmarked are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Lookup product by id&lt;&#x2F;li&gt;
&lt;li&gt;Find product by name (exact match)&lt;&#x2F;li&gt;
&lt;li&gt;Create shopping cart&lt;&#x2F;li&gt;
&lt;li&gt;Add product to cart&lt;&#x2F;li&gt;
&lt;li&gt;Checkout&lt;&#x2F;li&gt;
&lt;li&gt;Review a product&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;A single plan is generated by using probabilities to adjust how many plans
succeed in each step of the shopping process. Some plans will search for
products and never add them to a cart; Others will not only purchase the
product, but also write a review. By adjusting the probabilities of each action
occurring, we create a funnel that allows us to adjust the ratio of reads and
writes.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve written some more notes on the implementation of the benchmark itself, such
as how aggregation of ratings is handled, in the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;tree&#x2F;73aa1b1e8086c23bee10cd3024bf5fcaff8ea13e&#x2F;benchmarks&#x2F;benches&#x2F;commerce#user-content-benchmark-notes&quot;&gt;benchmark&#x27;s
README&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;-1&quot;&gt;


&lt;a href=&quot;#BonsaiDb%20is%20faster%20than%20I%20had%20hoped%21&quot; name=&quot;BonsaiDb%20is%20faster%20than%20I%20had%20hoped%21&quot;&gt;
    BonsaiDb is faster than I had hoped!
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Edit 2022-05-25: These benchmarks were discovered to have serious problems.
Read more &lt;a href=&quot;&#x2F;blog&#x2F;durable-writes&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;These graphs show the accumulated execution time of each operation:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;bonsaidb-scaleway-gp1-xs&#x2F;commerce&#x2F;large-writeheavy&#x2F;8&#x2F;index.html#LookupProduct&quot;&gt;&lt;img src=&quot;https:&amp;#x2F;&amp;#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&amp;#x2F;bonsaidb-scaleway-gp1-xs&amp;#x2F;commerce&amp;#x2F;large-writeheavy&amp;#x2F;8&amp;#x2F;LookupProduct.png&quot; class=&quot;block-image&quot; alt=&quot;lookup product by id&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;bonsaidb-scaleway-gp1-xs&#x2F;commerce&#x2F;large-writeheavy&#x2F;8&#x2F;index.html#FindProduct&quot;&gt;&lt;img src=&quot;https:&amp;#x2F;&amp;#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&amp;#x2F;bonsaidb-scaleway-gp1-xs&amp;#x2F;commerce&amp;#x2F;large-writeheavy&amp;#x2F;8&amp;#x2F;FindProduct.png&quot; class=&quot;block-image&quot; alt=&quot;find product by name&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;bonsaidb-scaleway-gp1-xs&#x2F;commerce&#x2F;large-writeheavy&#x2F;8&#x2F;index.html#AddProductToCart&quot;&gt;&lt;img src=&quot;https:&amp;#x2F;&amp;#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&amp;#x2F;bonsaidb-scaleway-gp1-xs&amp;#x2F;commerce&amp;#x2F;large-writeheavy&amp;#x2F;8&amp;#x2F;AddProductToCart.png&quot; class=&quot;block-image&quot; alt=&quot;add product to cart&quot;   &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This set of graphs is from the &amp;quot;large, write-heavy, 2 workers per core&amp;quot;
benchmark, run on a &lt;a href=&quot;https:&#x2F;&#x2F;scaleway.com&quot;&gt;Scaleway&lt;&#x2F;a&gt; GP1-XS instance running
Ubuntu 20.04 with 4 CPU cores, 16GB of RAM, and local NVME storage. The entire
suite&#x27;s results can be &lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs-storage.s3.us-west-000.backblazeb2.com&#x2F;bonsaidb-scaleway-gp1-xs&#x2F;commerce&#x2F;index.html&quot;&gt;viewed here&lt;&#x2F;a&gt; as well.&lt;&#x2F;p&gt;
&lt;p&gt;As I started seeing these results, I was simply blown away. I&#x27;ve tried to be &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;tree&#x2F;73aa1b1e8086c23bee10cd3024bf5fcaff8ea13e&#x2F;benchmarks&#x2F;benches&#x2F;commerce#user-content-benchmark-notes&quot;&gt;as
fair as possible&lt;&#x2F;a&gt; in writing this benchmark suite. Over time I
plan on adding additional database backends for comparison, as well as
additional operations as BonsaiDb gains more features.&lt;&#x2F;p&gt;
&lt;p&gt;One major difference between PostgreSQL and BonsaiDb is the lack of SQL. With
BonsaiDb, the query language is Rust itself. This is why I often refer to
BonsaiDb as a &lt;em&gt;programmable database&lt;&#x2F;em&gt;. The interface for accessing your data is
how &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;about&#x2F;concepts&#x2F;view.html&quot;&gt;you design it&lt;&#x2F;a&gt;. For example, here&#x27;s the implementation of find product:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; product = Product::load(&amp;amp;operation.name, &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.database)
&lt;&#x2F;span&gt;&lt;span&gt;    .await
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Result
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;(); &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Option&amp;lt;CollectionDocument&amp;lt;Product&amp;gt;&amp;gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; rating = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;self
&lt;&#x2F;span&gt;&lt;span&gt;    .database
&lt;&#x2F;span&gt;&lt;span&gt;    .view::&amp;lt;ProductReviewsByProduct&amp;gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;with_key&lt;&#x2F;span&gt;&lt;span&gt;(product.id as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;u32&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;with_access_policy&lt;&#x2F;span&gt;&lt;span&gt;(AccessPolicy::NoUpdate)
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;reduce&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    .await
&lt;&#x2F;span&gt;&lt;span&gt;    .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#96b5b4;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span&gt;(); &lt;&#x2F;span&gt;&lt;span style=&quot;color:#65737e;&quot;&gt;&#x2F;&#x2F; Returns a f32
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;-2&quot;&gt;


&lt;a href=&quot;#Trying%20out%20BonsaiDb&quot; name=&quot;Trying%20out%20BonsaiDb&quot;&gt;
    Trying out BonsaiDb
    &lt;i class=&quot;bi bi-link-45deg hoverable&quot;&gt;&lt;&#x2F;i&gt;
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&quot;&gt;BonsaiDb&lt;&#x2F;a&gt; is currently labeled experimental. We are working to stabilize
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;custodian&quot;&gt;custodian-password&lt;&#x2F;a&gt;, which has involved updating many existing
crates to improve the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;novifinancial&#x2F;opaque-ke&quot;&gt;OPAQUE-KE&lt;&#x2F;a&gt;
ecosystem in Rust. After that is done, we are going to label BonsaiDb as alpha.
It hasn&#x27;t been used by many people yet, so we expect that there will be bugs and
some of those bugs might even cause loss of data. That being said, we&#x27;ve been
using it ourselves with &lt;a href=&quot;https:&#x2F;&#x2F;khonsulabs.com&#x2F;&quot;&gt;KhonsuLabs.com&lt;&#x2F;a&gt; since early
November 2021 with no issues, in addition to a couple other small test projects.&lt;&#x2F;p&gt;
&lt;p&gt;We encourage most users to wait another week or two until we have the alpha on
Crates.io, but for those looking to play with something new, we&#x27;d love any
feedback from early users. We&#x27;ve already made countless improvements to the API,
&lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;docs&#x2F;bonsaidb&#x2F;&quot;&gt;documentation&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;dev.bonsaidb.io&#x2F;release&#x2F;guide&#x2F;&quot;&gt;user&#x27;s guide&lt;&#x2F;a&gt; and
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;tree&#x2F;73aa1b1e8086c23bee10cd3024bf5fcaff8ea13e&#x2F;examples&quot;&gt;examples&lt;&#x2F;a&gt; as a result of questions from early adopters.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re interested in BonsaiDb but want to wait until we&#x27;ve released a stable
version, we invite you to subscribe to &lt;a href=&quot;&#x2F;blog&#x2F;atom.xml&quot;&gt;this site&#x27;s feed&lt;&#x2F;a&gt; or
watch the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;khonsulabs&#x2F;bonsaidb&#x2F;releases&quot;&gt;repository&#x27;s releases section&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We are looking forward to seeing what types of applications BonsaiDb will power someday!&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Have questions or comments? Discuss this post on &lt;a href=&quot;https:&#x2F;&#x2F;community.khonsulabs.com&#x2F;t&#x2F;benchmarking-relational-data-in-bonsaidb&#x2F;91&quot;&gt;our forums&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
	</entry>
</feed>
